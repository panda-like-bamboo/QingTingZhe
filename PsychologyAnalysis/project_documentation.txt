项目文档生成时间: 2025-05-16 15:38:06

==================================================
第一部分：项目目录结构
==================================================

PsychologyAnalysis/
  .env
  .gitignore
  alembic.ini
  code2txt.py
  Combiner.py
  create_initial_user.py
  create_structure.bat
  find _token.py
  main.py
  project_documentation.txt
  project_text.txt
  psychology_analysis.db
  README.md
  requirements.txt
  run_celery_worker.py
  .git/
    COMMIT_EDITMSG
    config
    description
    HEAD
    index
    hooks/
      applypatch-msg.sample
      commit-msg.sample
      fsmonitor-watchman.sample
      post-update.sample
      pre-applypatch.sample
      pre-commit.sample
      pre-merge-commit.sample
      pre-push.sample
      pre-rebase.sample
      pre-receive.sample
      prepare-commit-msg.sample
      push-to-checkout.sample
      update.sample
    info/
      exclude
    logs/
      HEAD
      refs/
        heads/
          main
          feature/
            add-SQLite
        remotes/
          origin/
            main
            feature/
              add-SQLite
    objects/
      00/
        1ad4bb4a59cbdb0ac1ddf134f9503ee4eb941d
        f1f9a338f4ecda0461e2b1e919d3679f88b02c
      01/
        b512c1b975ae8ed48bec78d2f2a797aee04175
      03/
        5c221979f77795910253f051d9d03dff9c4085
        a64fae01884fb7377a512d0118eac53cfe9345
      04/
        eb1421784606bd918c72ce21ee7f0278bebf15
      0c/
        dec50e08b0ab11ba7d53f7b989a6c614c31abb
      0d/
        2b14ced5984d5fcbd7e0c3d4b8824f31c1fa86
        e0b6d0d203267db04218b4aa21ed5542ab10d7
      0f/
        8f77aaad74eba52509e01efc00b4bd4f2db5a0
        be2c23a32171ca42eed24e8b48eb6ae70e50bc
      10/
        59cc4ab69a96f51712a550fe291d6dd4fdb2b7
      12/
        170485ae2e5af27f6a5612b1af82aa1aca1d16
      13/
        14ce251df032572faa5b4b1c54741b762c2859
        50414ac811c9720f85d6c3236bbd79d7d0a0af
      14/
        755b8a01c4534431afaa63e9a9e69855fe9693
      15/
        530d58381cfaa592f432d06d0e3b4d75979fa1
      16/
        428857e8476be65ef739ea56109654e7d44ed0
      17/
        9554ed9b3425e2cedc13865bd9a73f5cbe290a
      18/
        0028419bd3d3c73905a966756a8c23bd13192b
      19/
        0edb5d839e6ba4394ea560eb1aacf1e0ff2ba7
        9a7229ad454705cff2987ca2c9aa8844d6e672
      1a/
        33b1dc7c13e7687022f838a0701e0beecc73ec
        62c857ba278139810ddc9efba3815ce8a475ac
        6f5248e38415868d72b959451bca579a80cc0a
      1b/
        19c678ab390732d2693b1bc7ddd31c48e9986c
        a8056d26b4e150beab2d9c4cb520985febb136
      1c/
        17b125e8414f73946d0502609d81a080166e1e
        21834c10f9c325ff3f5f8afaf7ac14706952be
        f1118dba58a12d8abdf85a1264f9302dc989ce
      1e/
        ae7d3c4e6aa8867a3921270d822ec7492a23d7
      1f/
        11b9b2ac4c648475275a6821b3e2de1a44bd37
      20/
        336ee91d617c0235f9d18d07e6a6c9f47aafad
      22/
        59b8381631ec194eeadb805f64296cfadb01fc
      23/
        1c188fbd04c8f681e6fa84ca53015c009785a7
      25/
        54701d69ac6a2ca0ad95cff5cfdc419216e794
        57f71c9ab0fca990b555b577551c4d8388a2b8
      26/
        27167eeb025b72af9103bbf8120620c63f1bb5
        ed667e9c00a416625d8a9f03dd8f6b26be218d
      27/
        c2f6dc9369ae95547ad0780469b687901bb84f
      2b/
        8466c8c4b97b5b581473c6ec56ebf403bb0b18
        8d7c06000aedc326c3645fc1dda5d8acc245b9
      2d/
        0cae5aa5df3e38bbc4c530a43b2e5eaf23f3b3
      2f/
        09c0b9da126a06729c3e25128161354403c93f
      34/
        06ce50c42fef949edef25cdf5ac8becec27af3
        120dd588c96631e266483790d2cd3f883f66f4
        c2e7bdb2747a29de75088cac02a29c10dc3bbc
      35/
        13a5221a131ea18fa6e04ec98ab8e306d6757c
        5f2230940baa069ffb5627a42923445941d06f
      36/
        2dfbf805088266f80261136a0d3e8803898796
      38/
        267654cd80ba5e7e549b8385688d0dc8ef81ff
      39/
        584dae458b91ca9abdca53f9c8cc5a484b97c0
        c637c889413858d81833e91447fbd86a105c31
        ddaa0a1662a16d4bed68b5f00eb789376af67c
      3a/
        28b5c3dcc80e82689df7622b9d73041a737b36
        882a9b62d80bf5ca77eccd47a0da644b092347
      3c/
        6c57c7a5a0bf6998193136db26f5459bb9171b
        a15b11440a25f49e320de5f5f0bc04e81c7cc6
      3d/
        18e559d61ec6bc9fec6f549a7890a43aedf304
        e427da87f0a7d2cdb8af4ccb16cf7f73d4c401
      3e/
        4dafd7691ec3dd1bde9bd59fcdf28f4098af60
        e972b88be666ad384fcdb25249c2cdd74260d6
      41/
        9845770072fcc3e254b3a41f080426265f0eda
        9f3b0d4b1483339db94525c683a6663afac1e1
      42/
        0298caf723214e4ede35e42ad7d56bbf7be4b8
        415ba3df9187024b32f5667ecced22a4a51f32
        595efb99cc2f55484cfda26bcec2cdf02219f9
      43/
        deef336344327856d1d860ffb3407000840513
      44/
        a22b5940ed22e2a04d6093dd30ed791c441592
      45/
        a1421ecb1fb92f95faca87c8748c2648a58afd
        ea234cb91b3420333e1752b0c8f14dc4e08c9c
      46/
        a29fd2a2ef9d52e6ec457931cb5e874d24bd8a
      48/
        0b130d632ca677c11f23d9fe82cf4014d15e0c
        b8f448bd5c8cace7fa482e9281b1f836bdc640
      4c/
        db36f62668e963b26a42e3030fcc299fe3f55a
      4d/
        d738d6463eac7b454eeb9fa96bcf1e2f486a4f
      4f/
        994894eb2c7d18d16bfe83c88d60b0740d8802
        edcf43b25bd44e4e0fbb06954ba5cc9451c22a
      52/
        d7c1a18ef930c1c1c98bf248a2e842d15379ff
      53/
        07e0809921b46189f314499d02be4faa6772e7
        26d681a358ad0f30e0e6137763b25ed3813393
      56/
        028992b3e0f68e084b289347cfdb3e9453e9dd
      57/
        bd8279b9e982261ccff432113f3fae4f0ac51c
      58/
        9c9062493fa10a00760ab9a98485bacf0bcfb6
      5e/
        eae694ae1c7c16bc977e84ccd415c2113efea2
      5f/
        2411d567b115a13565f5354f5e9a162f24a345
      61/
        29feb5c75b4413880e15b4e76e31dee7b45f89
        efc86772780c5190adcbc73eb739b3d292bfa7
      62/
        907838025ec3e0009b09ea0c383a67b3ed6680
      65/
        9d2f11fde57f3cc3c2a8d5d60f8fe5479b92ff
      67/
        021574883946a5e8a244453fa4869d2547f19a
        39dd22283de3c81abc1959398b87adb215b5b3
      6d/
        865406aaaea9879f077b737ac203306abb9e8f
        ea44ab256461f55f054b63f1e82379251af10e
      6e/
        0865849ffaffe3c16a97d8df0428a813ef43c1
        e1fa34f83ee5d6685c30b0684793c0d406aab0
      72/
        308b59813d15637bdb4462ee2e5705b354bb22
        32676ad66a4d556998272eebb32fb4568d502e
        63f477640e2b25706b08db2b0ab34e45fc6b38
        dfd07de33c644e5be7185e0111ec4727cf9467
      74/
        76edb315d4a8bb670a333a8a0f102ce2105405
      76/
        50be1332e30e87a4d56178ef6db340065aff2f
        696ae0f5c0350e25cc62f892b8c8e03a61ce66
        a61f3fd9dc64dd502566996a7c6f9cbda4e122
      77/
        907e45232406ba49a0de1019cd672740518b52
      79/
        3ab6a06621a5827dabf94dc303366b409e7d78
        5aeef71f00d4c48a4315bf8eced76dc2d3a56f
      7a/
        205d911dae4499a5957c11f70426eceec86139
      7c/
        72e7dbefae2a2dd9f5b26bab1a194f0ae0700f
      7d/
        0d7c2b67e6a4b5ab3c01f5417c717d1f51e52a
        7d6bab7cd95ea4af27fd1781fd4e57a4e7bdd7
        bcf05e56e4a378de1f43bf544fc13fc05a1ad5
      7f/
        92744e31f87d08096844b04acbf5fb45c62efc
      80/
        575028454e49098fcac7da363c615ab6838d00
      82/
        f741be7012214e29cb1b7e452aa701fd37c75d
      83/
        5c1af93b81cc26e7c3d9403fc96f1c60de7515
      84/
        2798011a3f194239dda371cd92b3b606418abb
      86/
        d2e82f7f33c7ce049891c2387e47d0c282c3b0
        eecf0e40e7e01e022163d14e7f557482c6a1d6
      89/
        ccd8534dbf253b68b8ab392bed537085641801
      8a/
        69f975adb32451e3c1090b62857d1e27cd3b04
        f0f895f24c72be1493fa46fa26ba80ed4bd0c1
      8b/
        d66ba2773e5362cb3d2578eff8dba1cc8a0354
      8c/
        d0beec64208d1801b02eb3d5732570f716708e
      8d/
        94250928a83dbb62e59823cbf273ec14178f22
      8e/
        66a6c876b707b5559d907d27f61365d24e1f85
      91/
        9f90966c62717988fe0c3aacb1a900f1e72cb4
      94/
        3f49c6131b8b6d0d063ca6bd7387e62b21de53
      95/
        0d92e48c3164e57dc7c01df4e44a6c1c0bc173
      97/
        d61d380edd68fe5e76eede90e99cc14e1d857b
      9c/
        2e8b4dc8699a85bf6e105b5652418429bba17d
        35ff5a2c8019394506ae45a311b2ad37174ab0
      9e/
        eecf53488c2bf7144ceb9f6e123fda2a27e7f2
      9f/
        acd9e873402569445133d0b91ac3665a77c761
        dc040050624556464ffa5112dde397ccd792c6
      a1/
        9693d4b6c943939f6b36383e9ceaa6491bbd1d
        e7d58650dedca81c92e10a43c6ee9c9f484420
      a2/
        7a13e496942e077a2e4392e79a14be597fd0e6
        7ebc9f9b4dfe82cf9cfea0ab6b549aa9c04c6a
      a3/
        36e5fe3ad8f45caa601a8c853601ec4c8f0ad4
      a5/
        8d010bd1849ea7514bb1d0751c29ca3ba6f2f7
        fb821f3a960767b8861c06a9879fb2535ce7f3
      a6/
        3975dc55b149a1531c06adcb3866fad511d1da
        e468cc369ffaf54bc3600c257e772165232b49
      a7/
        756130bb1fd01db91b8be0a7bd844faccebe79
        afc0621c9b834d8f139495b113ad430ebdd065
      a8/
        210548c126f9a57e25b9fa7bdc3d15d88a152e
      aa/
        9674bff49341d7c6b0a6ab8ba34e8ec3372c68
      ab/
        7098f5e7d3e65d1dc78736269630ae8873a82b
      ac/
        197e9723b0c8d0c03b3e5698343bdcc78fdc93
        77806aed7ad5dc8f60c6c66e8b709b437766ab
        9f9be8794d6e4f9f54185a0942bd88d846ea1f
      ad/
        29b33bf959eed8dce123de959454f89e26c2c0
        99bba2735d03e995d5a854935a67d871214c16
      ae/
        0a0841941a896a8f3bc32244cfef832aab8225
      af/
        1fc84185401155dab36f87f271cb2cb924a958
      b0/
        f1d69a4cdfb7075c9fff3c750c173dee824d35
      b1/
        725e7c3b8f783170d0479f18e67c416ad4e633
      b2/
        3a63d157cf47e7778eca8cc2904d52be305b5d
      b3/
        078c8228687749f4723fd9fa35f9f64dbbc242
        e4b3d8822d9208dfb2b5b5dddf512c4b80c844
        f232bdd903e435a62026e1f27119f15961782c
      b4/
        0bdefdc1827e0dbcfe065f9350ee23574af4ab
      b7/
        44a021db5e8dee616f2a2b299181d4abf5c556
        752a996e380022274105861387fd7e8bf0ea02
      b8/
        5d4173ea3a9dcf2599eb5d41406a0cded0eb40
      ba/
        68d800e780b486a3488fab3acba5ffb7a5131d
      bb/
        3865865a1f62bf3f323712d38f3835c249f185
      be/
        19ca58bb5ada17902ef40ae0e7907a44b95ebf
        ae79856ebed36ffcfc2e640570278beb01b8d8
        cf22bf1b11f7e257acb2b1de382a2c40e3e3a6
      bf/
        bd8e76e775d579271d141a026c5e3e7abc3cd4
        c3170b74f820792ed9bdc1195795662271f2ef
      c1/
        d21363b7bcc1ac97a45405a4c5c8ec3f789d6e
      c2/
        906e28dec7b8d1539d30f6cc1dd6cf11cfff08
        a04f00f272a7328de06acb0e4c4331cceff1fd
      c3/
        3afe698b93d7de76711ff8cd706821aa722007
        baeee8d9b6021c48fcdf93b3068c20b7ba0b74
      c8/
        ad7372a7def273034b6a078fef7ca207f3b6ad
      cf/
        aae9373284c74755573f8ee0cfe7d57812d7a9
        c9a6092bfd0b3e43af5256f837a6ab3b43f990
      d0/
        830085ecfecaf0a57fcbecea0ba419c9502d37
      d1/
        5b66c07e8827fdd0ff4354331ccaa2a00e7de2
        6e97d7914485c17e0395f580442377db5a6658
      d4/
        1acbc388a89af3183b27562f3ae301a51ede7c
        369ec968473b131b989bd32ed2431e61002096
        4f8353405ccca528e4d91f00dd71825cf5ef37
      d5/
        8de88f57ddaa8d332ac4d3b4bcd776dbd6572a
        99a575e43c7c83e9fe8dadf5982b4514f1a9b3
        e583ccc795e00605e7c75a4eba3162fca04b6a
      d6/
        c8f4d9982919fbcc4daf2c64056af136a9b68a
        e9212fe9ba23ffa73805da67ca595142ba353a
      d7/
        3c2aa346c4756a3785227ab1fa619dc44e90d9
        6d7d738d3cace730bd2cbcfbaa64dbc3c08941
      d8/
        d1ed3944b547bbe2511caa5f1b4bfc1a347766
      d9/
        2ec90575ca8351eef4c3c7b7eca228689d0f01
      da/
        fba3c24607c591762386d2af6047361a2e041f
      db/
        86821b7ebe4cc027e9176c2fa8ccf89d90c207
      dc/
        b57f8e7615e87a5d2d391db65f9c50052ab8f8
      de/
        8f2375bab107f80db8c9855adc53638dfa95b8
      e2/
        612971942d9216aa098999000dbf0599ea74bb
        de9eb013671744299ebd688689ba41db573009
      e4/
        23a6d30dd2a4e9a9235da54e88e8af30e90934
        fce11e6093eba1cad00b3d2eb227a99936e0b7
      e5/
        0711076ac8770cd864d52f8c9e402c761083be
        25e4aa93a233985efa5c6980965c982da2ebb1
        9a65a16d5731fa0068fbdb27c0aa8b23e77f1c
      e6/
        9de29bb2d1d6434b8b29ae775ad8c2e48c5391
      e7/
        c459bebff100d6a85bd8fd47478eefd6518774
      e9/
        03f469138532261bea6eb9e185fe490403534c
        a63bebc59f897bfcd9dba47ab7d62c44dd4314
      ea/
        a6379d7ff568b3928386891be87431e5ff37c2
      eb/
        0bd551fbd0728313d2506a006a98dcf3ce2d5d
      ed/
        241cfe9cef4bfbdedee9213a46728ed3f5cf9b
      ee/
        d71643e8aed4707ef95898270508c15ecb108f
      ef/
        a77066fea8765db4e8fd7c7cb90f4552e77afb
      f0/
        2431dcbc87e191f13e71aaa2a241a1aaa9a5d4
        c50edbf917d522043159af34a2e6d444df180e
      f1/
        e585438b9f9ea720321a283870ca34e9558499
      f2/
        1b2295f71004f83504b9b53baa11fcb390cdf6
      f3/
        ba9de61264fb775bc62fb9fa6a086230c79af4
      f4/
        37427c723f2716bb1e6d54dde5bd532bd632f9
        4e9bbc4bffc0a5955d72b5ec195677dfc8d7ad
      f5/
        3cc46fccfb11d32b826605287372f10bcb1e77
      f6/
        df3351505ab09ea62622a396aa7cb59eeeeff1
      f7/
        0ab3ca71f74ca3687a641251a8624ce3586cdb
      f9/
        867422c063e9ea439b567ef2afb06f4eb0b434
      fa/
        9820ad9c9fc2e4c97af8c2e2353588587613e2
      fb/
        18f9b4ddd9a0db174e5d0a3d453e2344690ff8
        2eb0c874fd0f9196e363705ea8fcdb30117d81
      fc/
        57024cbae6ddcf9674749fb76b7cc17bad70c8
      fd/
        b19d3a12e64485de2dbf1f567a2b402b7c3316
      fe/
        79e7225bcbf1077fff19be1e6048abe838ca9b
        b0a73567855a2ea3e16e2870479f7416dbacf6
      ff/
        212f6d63b551ff9b9254bb89d0e0641d7afd3a
        4bc98b7cec201a37c9e5c7f983594e49cba2b4
        e3e680c86868fe4470fe6d7ccd27be6a1b0c44
      info/
      pack/
    refs/
      heads/
        main
        feature/
          add-SQLite
      remotes/
        origin/
          main
          feature/
            add-SQLite
      tags/
  alembic/
    env.py
    README
    script.py.mako
    versions/
      269625ee49fe_add_status_column_to_assessment_table.py
      5976b03ee0fc_create_interrogation_records_table.py
      72143d5034d8_create_users_table.py
      a43553c2237e_add_attributes_and_assessment_.py
      __pycache__/
        269625ee49fe_add_status_column_to_assessment_table.cpython-311.pyc
        5976b03ee0fc_create_interrogation_records_table.cpython-311.pyc
        72143d5034d8_create_users_table.cpython-311.pyc
        a43553c2237e_add_attributes_and_assessment_.cpython-311.pyc
    __pycache__/
      env.cpython-311.pyc
  app/
    main.py
    __init__.py
    core/
      celery_app.py
      config.py
      deps.py
      redis_client.py
      security.py
      __init__.py
      __pycache__/
        celery_app.cpython-311.pyc
        config.cpython-311.pyc
        deps.cpython-311.pyc
        security.cpython-311.pyc
        __init__.cpython-311.pyc
    crud/
      assessment.py
      attribute.py
      interrogation.py
      stats.py
      user.py
      __init__.py
      __pycache__/
        assessment.cpython-311.pyc
        user.cpython-311.pyc
        __init__.cpython-311.pyc
    db/
      base_class.py
      init_db.py
      session.py
      __init__.py
      __pycache__/
        base_class.cpython-311.pyc
        init_db.cpython-311.pyc
        session.cpython-311.pyc
        __init__.cpython-311.pyc
    models/
      assessment.py
      association_tables.py
      attribute.py
      interrogation.py
      user.py
      __init__.py
      __pycache__/
        assessment.cpython-311.pyc
        association_tables.cpython-311.pyc
        attribute.cpython-311.pyc
        interrogation.cpython-311.pyc
        user.cpython-311.pyc
        __init__.cpython-311.pyc
    routers/
      admin.py
      assessments.py
      auth.py
      encyclopedia.py
      reports.py
      scales.py
      sse.py
      __init__.py
      __pycache__/
        assessments.cpython-311.pyc
        auth.cpython-311.pyc
        encyclopedia.cpython-311.pyc
        pages.cpython-311.pyc
        reports.cpython-311.pyc
        scales.cpython-311.pyc
        sse.cpython-311.pyc
        __init__.cpython-311.pyc
    schemas/
      assessment.py
      attribute.py
      encyclopedia.py
      guidance.py
      interrogation.py
      report.py
      scale.py
      stats.py
      token.py
      user.py
      __init__.py
      __pycache__/
        assessment.cpython-311.pyc
        encyclopedia.cpython-311.pyc
        report.cpython-311.pyc
        scale.cpython-311.pyc
        token.cpython-311.pyc
        user.cpython-311.pyc
        __init__.cpython-311.pyc
    tasks/
      analysis.py
      __init__.py
      __pycache__/
        analysis.cpython-311.pyc
        __init__.cpython-311.pyc
    __pycache__/
      main.cpython-311.pyc
      __init__.cpython-311.pyc
  config/
    config.yaml
    psychology_encyclopedia.json
  input/
    images/
      dog_and_girl.jpeg
      image1.jpg
      TestPic.jpg
      testpic1.jpg
      v2-b7ebd7f0a95bb411884521c43f19c846_xld.jpg
    questionnaires/
      1测你性格最真实的一面.json
      2亲子关系问卷量表.json
      3焦虑症自评量表 (SAS).json
      4标准量表：抑郁症自测量表 (SDS).json
      5人际关系综合诊断量表.json
      6情绪稳定性测验量表.json
      7汉密尔顿抑郁量表HAMD24.json
      8艾森克人格问卷EPQ85成人版.json
      9开心测试.json
  logs/
    app.log
  output/
    descriptions/
      testpic1_desc.txt
    reports/
      testpic1_report.txt
  src/
    ai_utils.py
    api_tester.py
    data_entry.py
    data_handler.py
    guidance_generator.py
    image_processor.py
    import_questions.py
    interrogation_ai.py
    report_generator.py
    utils.py
    __pycache__/
      ai_utils.cpython-311.pyc
      api_tester.cpython-311.pyc
      data_handler.cpython-311.pyc
      image_processor.cpython-311.pyc
      report_generator.cpython-311.pyc
      utils.cpython-311.pyc
  uploads/
    10000000000000_20250416201304_testpic1.jpg
    10000000000000_20250416204749_testpic1.jpg
    10000000000000_20250416210311_testpic1.jpg
    10000000000000_20250416213454_testpic1.jpg
    1111111111111111111111_20250424130717_neu_logo2.png
    1111111111111111111111_20250424130719_neu_logo2.png
    1111111111111111111111_20250424130724_neu_logo2.png
    1111111111111111111111_20250424130725_neu_logo2.png
    1111111111111111111111_20250424130733_neu_logo2.png
    1111111111111111111111_20250424132106_testpic1.jpg
    1111111111111111111111_20250424153455_neu_logo2.png
    111111111111111111120_20250425001629_testpic1.jpg
    111111111111111111121_20250425002213_testpic1.jpg
    111111111111111111_20250426163131_testpic1.jpg
    111111111111111112_20250426163306_testpic1.jpg
    111111111111111114_20250426002453_testpic1.jpg
    111111111111111115_20250426163917_testpic1.jpg
    111111111111111118_20250426164308_testpic1.jpg
    111111111111111119_20250426164652_testpic1.jpg
    111111111111111120_20250426165549_testpic1.jpg
    1223123123123123_20250417140740_bc3ca98535f261d084345205d7ed53b.jpg
    211401200000000000_20250416173352_testpic1.jpg
    211401200000000001_20250416173600_testpic1.jpg
    211401200000000001_20250416174140_testpic1.jpg
    211401200000000001_20250416174244_testpic1.jpg
    211401200000000001_20250416174317_testpic1.jpg
    211401200000000001_20250416174651_testpic1.jpg
    211401200000000001_20250416175232_testpic1.jpg
    211401200000000001_20250416180044_testpic1.jpg
    211401200000000008_20250416200605_testpic1.jpg
    211401200000000008_20250416222213_testpic1.jpg
    211401200001010001_20250425201116_testpic1.jpg
    211401200001010002_20250425202345_testpic1.jpg
    211401200001010003_20250425205400_testpic1.jpg
    211401200001010004_20250425222951_testpic1.jpg
    211401200001010005_20250425224806_testpic1.jpg
    211401200001010006_20250425225357_testpic1.jpg
    211401200001010007_20250425231129_testpic1.jpg
    211401200001010008_20250425232215_testpic1.jpg
    211401200001010009_20250425233352_testpic1.jpg
    211401200001010010_20250426223700_testpic1.jpg
    211401200001010055_20250426224255_testpic1.jpg
    string_20250417112455_bc3ca98535f261d084345205d7ed53b.jpg
    string_20250417142126_bc3ca98535f261d084345205d7ed53b.jpg
    string_20250417142942_bc3ca98535f261d084345205d7ed53b.jpg
    _20250426002802_testpic1.jpg
    _20250426003334_testpic1.jpg
    _20250426004050_testpic1.jpg
    _20250426004708_testpic1.jpg
    _20250426145725_testpic1.jpg
    _20250426151101_testpic1.jpg
    _20250426151421_testpic1.jpg
    _20250426151844_testpic1.jpg
    _20250426152017_testpic1.jpg
    _20250426153346_testpic1.jpg
    _20250426153449_testpic1.jpg
    _20250426154346_testpic1.jpg
    _20250426155032_testpic1.jpg
    _20250426155702_testpic1.jpg
    _20250426160837_testpic1.jpg
    _20250426162610_testpic1.jpg
    _20250426173648_APP_.png
  __pycache__/
    main.cpython-311.pyc
  文档部分/
    finding_work.xlsx
    倾听者 心理学项目需求分析 (2).md
    倾听者 心理学项目需求分析.md
    倾听者 心理学项目需求分析.pdf
    倾听者_任务需求分析与技术选型.pdf
    倾听者项目详细流程表_V2.1.xlsx
    绘画分析AI智能解析(4)(1).xlsx


==================================================
第二部分：文件内容
==================================================

--------------------------------------------------
文件路径: code2txt.py
--------------------------------------------------
import os
from datetime import datetime

def generate_project_documentation(root_dir, output_file):
    """
    将项目文件夹内容生成到TXT文件，包括目录结构和文件内容
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        # 写入文档头部
        f.write(f"项目文档生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # 第一部分：项目架构
        f.write("=" * 50 + "\n")
        f.write("第一部分：项目目录结构\n")
        f.write("=" * 50 + "\n\n")
        
        # 遍历目录结构
        for root, dirs, files in os.walk(root_dir):
            # 计算层级深度
            level = root.replace(root_dir, '').count(os.sep)
            indent = "  " * level
            # 写入目录名
            f.write(f"{indent}{os.path.basename(root)}/\n")
            # 写入文件名
            for file in files:
                f.write(f"{indent}  {file}\n")
        f.write("\n\n")
        
        # 第二部分：文件内容
        f.write("=" * 50 + "\n")
        f.write("第二部分：文件内容\n")
        f.write("=" * 50 + "\n\n")
        
        # 遍历所有文件并写入内容
        for root, _, files in os.walk(root_dir):
            for file in files:
                file_path = os.path.join(root, file)
                # 只处理文本类型的文件，可以根据需要扩展
                if file.endswith(('.py', '.txt', '.md', '.json', '.html', '.css', '.js')):
                    f.write("-" * 50 + "\n")
                    # 写入文件相对路径
                    relative_path = os.path.relpath(file_path, root_dir)
                    f.write(f"文件路径: {relative_path}\n")
                    f.write("-" * 50 + "\n")
                    
                    try:
                        with open(file_path, 'r', encoding='utf-8') as source_file:
                            content = source_file.read()
                            f.write(content)
                    except Exception as e:
                        f.write(f"读取文件出错: {str(e)}\n")
                    f.write("\n\n")

def main():
    # 获取当前目录，可以修改为指定目录
    current_dir = os.getcwd()
    output_filename = "project_documentation.txt"
    
    try:
        generate_project_documentation(current_dir, output_filename)
        print(f"文档已生成: {os.path.abspath(output_filename)}")
    except Exception as e:
        print(f"生成文档时出错: {str(e)}")

if __name__ == "__main__":
    main()

--------------------------------------------------
文件路径: Combiner.py
--------------------------------------------------
import os
import sys
from tqdm import tqdm

# 定义允许被合并内容的文件扩展名
ALLOWED_EXTENSIONS_FOR_CONTENT = ('.py','.yaml','.json','.csv','.html')

# --- 恢复 generate_tree 到原始版本，以包含所有文件 --txt-
def generate_tree(root_dir):
    """生成类似 tree /f 的目录树字符串 (包含所有文件和目录)"""
    tree_output = [root_dir]
    # os.walk 会自然遍历所有子目录和文件
    for root, dirs, files in os.walk(root_dir):
        level = root.replace(root_dir, '').count(os.sep)
        indent = '    ' * level
        if level > 0:  # 避免重复输出根目录
            tree_output.append(f"{indent}{os.path.basename(root)}")
        
        # --- 在目录树中列出该目录下的所有文件 ---
        sub_indent = indent + '    '
        for file in files:
            # 不进行扩展名过滤，列出所有文件
            tree_output.append(f"{sub_indent}{file}")
            
    return "\n".join(tree_output)

def concatenate_files(root_dir, output_file):
    # 计算总文件数以初始化进度条 (反映遍历的所有文件)
    total_files = sum(len(files) for _, _, files in os.walk(root_dir))

    # 预计算将被合并内容的文件数（可选，如果你想让进度条只反映合并进度）
    # included_files_count = 0
    # for root, dirs, files in os.walk(root_dir):
    #     for file in files:
    #         _, ext = os.path.splitext(file)
    #         if ext.lower() in ALLOWED_EXTENSIONS_FOR_CONTENT:
    #             included_files_count += 1
    # print(f"找到 {included_files_count} 个符合条件的文件将被合并内容。")


    with open(output_file, 'w', encoding='utf-8') as outfile:
        # --- 写入完整的目录树 ---
        outfile.write("Full Project Directory Tree:\n") # 标题明确说明是完整树
        outfile.write(generate_tree(root_dir) + "\n") # 调用未经过滤的 generate_tree
        outfile.write("=====\n\n")
        outfile.write("Concatenated File Content (Filtered):\n") # 明确说明内容是过滤后的

        # 使用 tqdm 显示进度条 (total 仍然是所有文件数，反映遍历进度)
        # 如果使用上面的 included_files_count，这里改为 total=included_files_count
        with tqdm(total=total_files, desc="扫描文件并合并内容", unit="file") as pbar:
            for root, dirs, files in os.walk(root_dir):
                for file in files:
                    file_path = os.path.join(root, file)

                    # --- 核心过滤逻辑：只处理指定扩展名的文件内容 ---
                    _, ext = os.path.splitext(file)

                    # 检查扩展名是否在允许合并内容的列表中
                    if ext.lower() in ALLOWED_EXTENSIONS_FOR_CONTENT:
                        # --- 文件内容合并逻辑 ---
                        try:
                            with open(file_path, 'r', encoding='utf-8') as infile:
                                content = infile.read()
                                outfile.write("FILE: " + file_path + "\n")
                                outfile.write(content + "\n")
                                outfile.write("-----\n")
                            print(f"已合并内容: {file_path} - 成功")
                        except Exception as e:
                            # 即使读取失败，也记录下来
                            outfile.write(f"FILE: {file_path}\n")
                            outfile.write(f"读取文件出错 (尝试合并内容时): {str(e)}\n")
                            outfile.write("-----\n")
                            print(f"尝试合并内容失败: {file_path} - 错误: {str(e)}")
                    else:
                        # --- 文件类型不符，跳过内容合并 ---
                        # 不需要写入文件内容，只在控制台打印信息
                        print(f"已跳过内容合并 (类型不符): {file_path}")

                    # --- 更新进度条 ---
                    # 每个访问的文件都更新进度条
                    pbar.update(1)

if __name__ == "__main__":
    root_dir = sys.argv[1] if len(sys.argv) > 1 else os.getcwd()
    output_file = 'project_text1.txt'
    print(f"正在扫描目录: {root_dir}")
    print(f"将生成包含所有文件的目录树，并合并后缀为 {', '.join(ALLOWED_EXTENSIONS_FOR_CONTENT)} 的文件内容到: {output_file}")
    concatenate_files(root_dir, output_file)
    print(f"处理完成。输出文件: {output_file}")

--------------------------------------------------
文件路径: create_initial_user.py
--------------------------------------------------
# create_initial_user.py
import sys
import os
import traceback
import logging
import asyncio # 导入 asyncio 用于运行异步代码
from getpass import getpass # 用于安全地获取密码输入

# --- 设置项目路径 (确保能找到 app 包) ---
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
# --------------------------------------------

# --- 导入必要的模块 ---
try:
    # 从 app.schemas 直接导入 UserCreate
    from app.schemas import UserCreate
    # 导入异步数据库组件
    from sqlalchemy.ext.asyncio import AsyncSession
    from app.db.session import AsyncSessionLocal, async_engine
    # 导入异步 CRUD 函数
    from app.crud.user import get_user_by_username, create_user
    # 导入设置 (如果需要)
    from app.core.config import settings
    # 导入数据库 Base 和 User 模型
    from app.db.base_class import Base
    from app.models.user import User
except ImportError as e:
    print(f"导入应用模块时出错: {e}")
    print("请检查以下几点:")
    print("1. 您是否在 'PsychologyAnalysis' 目录下运行此脚本。")
    print("2. 必要的文件（如 app/db/session.py, app/crud/user.py, app/models/user.py, app/schemas/user.py）是否存在。")
    print("3. requirements.txt 中的所有依赖项是否已在您的环境中安装。")
    print("4. CRUD 函数 (get_user_by_username, create_user) 是否已定义为 'async def'。")
    print("5. app/schemas/__init__.py 是否正确导出了 UserCreate。")
    sys.exit(1)
except Exception as e:
     print(f"导入过程中发生意外错误: {e}")
     sys.exit(1)
# ------------------------

# --- 配置日志 ---
APP_LOGGER_NAME = settings.APP_NAME or "QingtingzheApp"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(APP_LOGGER_NAME)
# ----------------

async def create_tables():
    """异步创建数据库表（如果尚不存在）"""
    logger.info("正在异步初始化数据库表...")
    try:
        async with async_engine.begin() as conn:
            # Base.metadata 包含了所有继承自 Base 的模型信息
            await conn.run_sync(Base.metadata.create_all)
        logger.info("数据库表已确认/创建成功。")
    except Exception as e:
        logger.error(f"创建数据库表时出错: {e}", exc_info=True)
        print(f"错误：无法创建数据库表。正在退出。错误: {e}")
        sys.exit(1)

async def create_user_interactively(db: AsyncSession) -> None:
    """交互式地获取用户信息并异步创建初始用户。"""
    logger.info("--- 正在创建初始用户 ---")

    while True:
        username = input("输入用户名: ").strip()
        if not username:
            print("用户名不能为空。")
            continue
        # 调用异步 CRUD 函数获取用户
        existing_user = await get_user_by_username(db, username=username)
        if existing_user:
            print(f"用户名 '{username}' 已存在。请选择其他用户名。")
        else:
            break

    email_str = input(f"为 {username} 输入邮箱 (可选, 直接按 Enter 跳过): ").strip()
    email = email_str if email_str else None
    full_name = input(f"为 {username} 输入全名 (可选, 直接按 Enter 跳过): ").strip() or None

    while True:
        password = getpass("输入密码 (最少6位): ")
        if len(password) < 6:
            print("密码太短 (最少6位)。")
            continue
        password_confirm = getpass("确认密码: ")
        if password != password_confirm:
            print("两次输入的密码不匹配，请重试。")
        else:
            break

    is_superuser_input = input("是否将此用户设为超级用户? (y/N): ").strip().lower()
    is_superuser = True if is_superuser_input == 'y' else False

    # 使用 Pydantic schema 准备用户数据
    user_in = UserCreate( # <-- 直接使用导入的 UserCreate
        username=username,
        email=email,
        full_name=full_name,
        password=password, # 传递明文密码，哈希在 crud.create_user 中完成
        is_superuser=is_superuser,
        is_active=True # 初始用户通常是激活的
    )

    try:
        logger.info(f"尝试创建用户: {user_in.username} (超级用户: {is_superuser})")
        # --- 关键修改：使用正确的关键字参数名 ---
        # created_user = await create_user(db=db, user=user_in) # 旧的，错误的调用
        created_user = await create_user(db=db, user_in=user_in) # <--- 使用 'user_in='
        # -----------------------------------------
        logger.info(f"成功创建用户: '{created_user.username}' (ID: {created_user.id})")
        print(f"\n用户 '{created_user.username}' 创建成功!")
    except ValueError as ve: # 捕获来自 CRUD 的特定错误 (如重复用户)
         logger.error(f"创建用户 '{username}' 失败: {ve}")
         print(f"\n错误：无法创建用户。{ve}")
    except Exception as e:
        logger.error(f"创建用户 '{username}' 时发生意外错误: {e}", exc_info=True)
        print(f"\n错误：发生意外错误: {e}")


async def main():
    """主异步执行函数"""
    await create_tables() # 先确保表存在

    logger.info("正在创建数据库会话...")
    # 正确使用异步会话上下文管理器
    async with AsyncSessionLocal() as session:
        await create_user_interactively(session)

    logger.info("脚本执行完毕。")


if __name__ == "__main__":
    print("开始执行用户创建脚本...")
    try:
        # 运行主异步函数
        asyncio.run(main())
    except KeyboardInterrupt:
         print("\n用户中断了脚本执行。")
    except Exception as e:
         print(f"\n在顶层发生意外错误: {e}")
         traceback.print_exc() # 打印完整的错误堆栈

--------------------------------------------------
文件路径: find _token.py
--------------------------------------------------
import dashscope
from dashscope import Account

# 配置您的API Key
dashscope.api_key = 'sk-0ffc64dd9d614c0088ee28e3a072420a'

# 查询账户信息
account = Account()
balance = account.get_balance()
print(balance)

--------------------------------------------------
文件路径: main.py
--------------------------------------------------
# 文件路径: main.py
import os
import yaml
import sqlite3
import json
from concurrent.futures import ThreadPoolExecutor, as_completed  # 新增导入
from src.image_processor import ImageProcessor
from src.report_generator import ReportGenerator
from src.data_handler import DataHandler
from src.utils import setup_logging

def process_single_task(image_path, image_processor, report_generator, data_handler, logger):
    """处理单个图片和量表数据的分析任务"""
    logger.info(f"开始处理图片: {image_path}")
    try:
        # 处理图片
        description = image_processor.process_image(image_path)
        desc_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 
                               f"output/descriptions/{os.path.splitext(os.path.basename(image_path))[0]}_desc.txt")
        os.makedirs(os.path.dirname(desc_file), exist_ok=True)
        with open(desc_file, "w", encoding='utf-8') as f:
            f.write(description)
        logger.info(f"图片描述已保存至: {desc_file}")

        # 加载数据
        subject_info, questionnaire_type, questionnaire_data = data_handler.load_data_by_image(image_path)
        if not subject_info:
            logger.warning(f"图片 {image_path} 无对应量表数据，仅保存描述")
            return

        # 计算得分并生成报告
        score = calculate_score(questionnaire_type, questionnaire_data)
        report = report_generator.generate_report(description, questionnaire_data, subject_info, questionnaire_type, score)
        report_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 
                                 f"output/reports/{os.path.splitext(os.path.basename(image_path))[0]}_report.txt")
        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        with open(report_file, "w", encoding='utf-8') as f:
            f.write(report)
        logger.info(f"报告已保存至: {report_file}")
    except Exception as e:
        logger.error(f"处理 {image_path} 时出错: {str(e)}")

def main(image_paths=None, max_workers=4):
    logger = setup_logging()
    logger.info("心理学图像和数据分析项目启动（并发模式）")

    project_root = os.path.dirname(os.path.abspath(__file__))
    os.chdir(project_root)
    config_path = os.path.join(project_root, "config/config.yaml")
    with open(config_path, "r", encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # 初始化处理器
    image_processor = ImageProcessor(config)
    report_generator = ReportGenerator(config)
    data_handler = DataHandler(db_path=os.path.join(project_root, "psychology_analysis.db"))

    # 如果没有指定图片路径，处理 input/images 目录下的所有图片
    if not image_paths:
        image_dir = os.path.join(project_root, "input/images")
        image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        logger.info(f"未指定图片路径，将处理目录 {image_dir} 中的所有图片: {len(image_paths)} 张")

    if not image_paths:
        logger.warning("没有找到任何图片需要处理")
        return

    # 使用线程池并发处理
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # 提交所有任务
        future_to_image = {executor.submit(process_single_task, image_path, image_processor, report_generator, data_handler, logger): image_path 
                         for image_path in image_paths}
        
        # 处理完成的任务
        for future in as_completed(future_to_image):
            image_path = future_to_image[future]
            try:
                future.result()  # 获取结果，如果有异常会抛出
                logger.info(f"任务完成: {image_path}")
            except Exception as e:
                logger.error(f"任务 {image_path} 执行失败: {str(e)}")

def calculate_score(questionnaire_type, questionnaire_data):
    """根据量表类型计算总分"""
    scores = [int(value) for value in questionnaire_data.values()]
    total_score = sum(scores)
    return total_score

if __name__ == "__main__":
    # 示例：可以传入特定图片路径列表
    specific_images = [
        os.path.join(os.path.dirname(os.path.abspath(__file__)), "input/images/TestPic.jpg"),
        os.path.join(os.path.dirname(os.path.abspath(__file__)), "input/images/testpic1.jpg")
    ]
    main(image_paths=specific_images, max_workers=4)  # 设置最大线程数为4
    # 或不指定图片路径，处理所有图片
    # main(max_workers=4)

--------------------------------------------------
文件路径: project_documentation.txt
--------------------------------------------------
项目文档生成时间: 2025-05-16 15:38:06

==================================================
第一部分：项目目录结构
==================================================

PsychologyAnalysis/
  .env
  .gitignore
  alembic.ini
  code2txt.py
  Combiner.py
  create_initial_user.py
  create_structure.bat
  find _token.py
  main.py
  project_documentation.txt
  project_text.txt
  psychology_analysis.db
  README.md
  requirements.txt
  run_celery_worker.py
  .git/
    COMMIT_EDITMSG
    config
    description
    HEAD
    index
    hooks/
      applypatch-msg.sample
      commit-msg.sample
      fsmonitor-watchman.sample
      post-update.sample
      pre-applypatch.sample
      pre-commit.sample
      pre-merge-commit.sample
      pre-push.sample
      pre-rebase.sample
      pre-receive.sample
      prepare-commit-msg.sample
      push-to-checkout.sample
      update.sample
    info/
      exclude
    logs/
      HEAD
      refs/
        heads/
          main
          feature/
            add-SQLite
        remotes/
          origin/
            main
            feature/
              add-SQLite
    objects/
      00/
        1ad4bb4a59cbdb0ac1ddf134f9503ee4eb941d
        f1f9a338f4ecda0461e2b1e919d3679f88b02c
      01/
        b512c1b975ae8ed48bec78d2f2a797aee04175
      03/
        5c221979f77795910253f051d9d03dff9c4085
        a64fae01884fb7377a512d0118eac53cfe9345
      04/
        eb1421784606bd918c72ce21ee7f0278bebf15
      0c/
        dec50e08b0ab11ba7d53f7b989a6c614c31abb
      0d/
        2b14ced5984d5fcbd7e0c3d4b8824f31c1fa86
        e0b6d0d203267db04218b4aa21ed5542ab10d7
      0f/
        8f77aaad74eba52509e01efc00b4bd4f2db5a0
        be2c23a32171ca42eed24e8b48eb6ae70e50bc
      10/
        59cc4ab69a96f51712a550fe291d6dd4fdb2b7
      12/
        170485ae2e5af27f6a5612b1af82aa1aca1d16
      13/
        14ce251df032572faa5b4b1c54741b762c2859
        50414ac811c9720f85d6c3236bbd79d7d0a0af
      14/
        755b8a01c4534431afaa63e9a9e69855fe9693
      15/
        530d58381cfaa592f432d06d0e3b4d75979fa1
      16/
        428857e8476be65ef739ea56109654e7d44ed0
      17/
        9554ed9b3425e2cedc13865bd9a73f5cbe290a
      18/
        0028419bd3d3c73905a966756a8c23bd13192b
      19/
        0edb5d839e6ba4394ea560eb1aacf1e0ff2ba7
        9a7229ad454705cff2987ca2c9aa8844d6e672
      1a/
        33b1dc7c13e7687022f838a0701e0beecc73ec
        62c857ba278139810ddc9efba3815ce8a475ac
        6f5248e38415868d72b959451bca579a80cc0a
      1b/
        19c678ab390732d2693b1bc7ddd31c48e9986c
        a8056d26b4e150beab2d9c4cb520985febb136
      1c/
        17b125e8414f73946d0502609d81a080166e1e
        21834c10f9c325ff3f5f8afaf7ac14706952be
        f1118dba58a12d8abdf85a1264f9302dc989ce
      1e/
        ae7d3c4e6aa8867a3921270d822ec7492a23d7
      1f/
        11b9b2ac4c648475275a6821b3e2de1a44bd37
      20/
        336ee91d617c0235f9d18d07e6a6c9f47aafad
      22/
        59b8381631ec194eeadb805f64296cfadb01fc
      23/
        1c188fbd04c8f681e6fa84ca53015c009785a7
      25/
        54701d69ac6a2ca0ad95cff5cfdc419216e794
        57f71c9ab0fca990b555b577551c4d8388a2b8
      26/
        27167eeb025b72af9103bbf8120620c63f1bb5
        ed667e9c00a416625d8a9f03dd8f6b26be218d
      27/
        c2f6dc9369ae95547ad0780469b687901bb84f
      2b/
        8466c8c4b97b5b581473c6ec56ebf403bb0b18
        8d7c06000aedc326c3645fc1dda5d8acc245b9
      2d/
        0cae5aa5df3e38bbc4c530a43b2e5eaf23f3b3
      2f/
        09c0b9da126a06729c3e25128161354403c93f
      34/
        06ce50c42fef949edef25cdf5ac8becec27af3
        120dd588c96631e266483790d2cd3f883f66f4
        c2e7bdb2747a29de75088cac02a29c10dc3bbc
      35/
        13a5221a131ea18fa6e04ec98ab8e306d6757c
        5f2230940baa069ffb5627a42923445941d06f
      36/
        2dfbf805088266f80261136a0d3e8803898796
      38/
        267654cd80ba5e7e549b8385688d0dc8ef81ff
      39/
        584dae458b91ca9abdca53f9c8cc5a484b97c0
        c637c889413858d81833e91447fbd86a105c31
        ddaa0a1662a16d4bed68b5f00eb789376af67c
      3a/
        28b5c3dcc80e82689df7622b9d73041a737b36
        882a9b62d80bf5ca77eccd47a0da644b092347
      3c/
        6c57c7a5a0bf6998193136db26f5459bb9171b
        a15b11440a25f49e320de5f5f0bc04e81c7cc6
      3d/
        18e559d61ec6bc9fec6f549a7890a43aedf304
        e427da87f0a7d2cdb8af4ccb16cf7f73d4c401
      3e/
        4dafd7691ec3dd1bde9bd59fcdf28f4098af60
        e972b88be666ad384fcdb25249c2cdd74260d6
      41/
        9845770072fcc3e254b3a41f080426265f0eda
        9f3b0d4b1483339db94525c683a6663afac1e1
      42/
        0298caf723214e4ede35e42ad7d56bbf7be4b8
        415ba3df9187024b32f5667ecced22a4a51f32
        595efb99cc2f55484cfda26bcec2cdf02219f9
      43/
        deef336344327856d1d860ffb3407000840513
      44/
        a22b5940ed22e2a04d6093dd30ed791c441592
      45/
        a1421ecb1fb92f95faca87c8748c2648a58afd
        ea234cb91b3420333e1752b0c8f14dc4e08c9c
      46/
        a29fd2a2ef9d52e6ec457931cb5e874d24bd8a
      48/
        0b130d632ca677c11f23d9fe82cf4014d15e0c
        b8f448bd5c8cace7fa482e9281b1f836bdc640
      4c/
        db36f62668e963b26a42e3030fcc299fe3f55a
      4d/
        d738d6463eac7b454eeb9fa96bcf1e2f486a4f
      4f/
        994894eb2c7d18d16bfe83c88d60b0740d8802
        edcf43b25bd44e4e0fbb06954ba5cc9451c22a
      52/
        d7c1a18ef930c1c1c98bf248a2e842d15379ff
      53/
        07e0809921b46189f314499d02be4faa6772e7
        26d681a358ad0f30e0e6137763b25ed3813393
      56/
        028992b3e0f68e084b289347cfdb3e9453e9dd
      57/
        bd8279b9e982261ccff432113f3fae4f0ac51c
      58/
        9c9062493fa10a00760ab9a98485bacf0bcfb6
      5e/
        eae694ae1c7c16bc977e84ccd415c2113efea2
      5f/
        2411d567b115a13565f5354f5e9a162f24a345
      61/
        29feb5c75b4413880e15b4e76e31dee7b45f89
        efc86772780c5190adcbc73eb739b3d292bfa7
      62/
        907838025ec3e0009b09ea0c383a67b3ed6680
      65/
        9d2f11fde57f3cc3c2a8d5d60f8fe5479b92ff
      67/
        021574883946a5e8a244453fa4869d2547f19a
        39dd22283de3c81abc1959398b87adb215b5b3
      6d/
        865406aaaea9879f077b737ac203306abb9e8f
        ea44ab256461f55f054b63f1e82379251af10e
      6e/
        0865849ffaffe3c16a97d8df0428a813ef43c1
        e1fa34f83ee5d6685c30b0684793c0d406aab0
      72/
        308b59813d15637bdb4462ee2e5705b354bb22
        32676ad66a4d556998272eebb32fb4568d502e
        63f477640e2b25706b08db2b0ab34e45fc6b38
        dfd07de33c644e5be7185e0111ec4727cf9467
      74/
        76edb315d4a8bb670a333a8a0f102ce2105405
      76/
        50be1332e30e87a4d56178ef6db340065aff2f
        696ae0f5c0350e25cc62f892b8c8e03a61ce66
        a61f3fd9dc64dd502566996a7c6f9cbda4e122
      77/
        907e45232406ba49a0de1019cd672740518b52
      79/
        3ab6a06621a5827dabf94dc303366b409e7d78
        5aeef71f00d4c48a4315bf8eced76dc2d3a56f
      7a/
        205d911dae4499a5957c11f70426eceec86139
      7c/
        72e7dbefae2a2dd9f5b26bab1a194f0ae0700f
      7d/
        0d7c2b67e6a4b5ab3c01f5417c717d1f51e52a
        7d6bab7cd95ea4af27fd1781fd4e57a4e7bdd7
        bcf05e56e4a378de1f43bf544fc13fc05a1ad5
      7f/
        92744e31f87d08096844b04acbf5fb45c62efc
      80/
        575028454e49098fcac7da363c615ab6838d00
      82/
        f741be7012214e29cb1b7e452aa701fd37c75d
      83/
        5c1af93b81cc26e7c3d9403fc96f1c60de7515
      84/
        2798011a3f194239dda371cd92b3b606418abb
      86/
        d2e82f7f33c7ce049891c2387e47d0c282c3b0
        eecf0e40e7e01e022163d14e7f557482c6a1d6
      89/
        ccd8534dbf253b68b8ab392bed537085641801
      8a/
        69f975adb32451e3c1090b62857d1e27cd3b04
        f0f895f24c72be1493fa46fa26ba80ed4bd0c1
      8b/
        d66ba2773e5362cb3d2578eff8dba1cc8a0354
      8c/
        d0beec64208d1801b02eb3d5732570f716708e
      8d/
        94250928a83dbb62e59823cbf273ec14178f22
      8e/
        66a6c876b707b5559d907d27f61365d24e1f85
      91/
        9f90966c62717988fe0c3aacb1a900f1e72cb4
      94/
        3f49c6131b8b6d0d063ca6bd7387e62b21de53
      95/
        0d92e48c3164e57dc7c01df4e44a6c1c0bc173
      97/
        d61d380edd68fe5e76eede90e99cc14e1d857b
      9c/
        2e8b4dc8699a85bf6e105b5652418429bba17d
        35ff5a2c8019394506ae45a311b2ad37174ab0
      9e/
        eecf53488c2bf7144ceb9f6e123fda2a27e7f2
      9f/
        acd9e873402569445133d0b91ac3665a77c761
        dc040050624556464ffa5112dde397ccd792c6
      a1/
        9693d4b6c943939f6b36383e9ceaa6491bbd1d
        e7d58650dedca81c92e10a43c6ee9c9f484420
      a2/
        7a13e496942e077a2e4392e79a14be597fd0e6
        7ebc9f9b4dfe82cf9cfea0ab6b549aa9c04c6a
      a3/
        36e5fe3ad8f45caa601a8c853601ec4c8f0ad4
      a5/
        8d010bd1849ea7514bb1d0751c29ca3ba6f2f7
        fb821f3a960767b8861c06a9879fb2535ce7f3
      a6/
        3975dc55b149a1531c06adcb3866fad511d1da
        e468cc369ffaf54bc3600c257e772165232b49
      a7/
        756130bb1fd01db91b8be0a7bd844faccebe79
        afc0621c9b834d8f139495b113ad430ebdd065
      a8/
        210548c126f9a57e25b9fa7bdc3d15d88a152e
      aa/
        9674bff49341d7c6b0a6ab8ba34e8ec3372c68
      ab/
        7098f5e7d3e65d1dc78736269630ae8873a82b
      ac/
        197e9723b0c8d0c03b3e5698343bdcc78fdc93
        77806aed7ad5dc8f60c6c66e8b709b437766ab
        9f9be8794d6e4f9f54185a0942bd88d846ea1f
      ad/
        29b33bf959eed8dce123de959454f89e26c2c0
        99bba2735d03e995d5a854935a67d871214c16
      ae/
        0a0841941a896a8f3bc32244cfef832aab8225
      af/
        1fc84185401155dab36f87f271cb2cb924a958
      b0/
        f1d69a4cdfb7075c9fff3c750c173dee824d35
      b1/
        725e7c3b8f783170d0479f18e67c416ad4e633
      b2/
        3a63d157cf47e7778eca8cc2904d52be305b5d
      b3/
        078c8228687749f4723fd9fa35f9f64dbbc242
        e4b3d8822d9208dfb2b5b5dddf512c4b80c844
        f232bdd903e435a62026e1f27119f15961782c
      b4/
        0bdefdc1827e0dbcfe065f9350ee23574af4ab
      b7/
        44a021db5e8dee616f2a2b299181d4abf5c556
        752a996e380022274105861387fd7e8bf0ea02
      b8/
        5d4173ea3a9dcf2599eb5d41406a0cded0eb40
      ba/
        68d800e780b486a3488fab3acba5ffb7a5131d
      bb/
        3865865a1f62bf3f323712d38f3835c249f185
      be/
        19ca58bb5ada17902ef40ae0e7907a44b95ebf
        ae79856ebed36ffcfc2e640570278beb01b8d8
        cf22bf1b11f7e257acb2b1de382a2c40e3e3a6
      bf/
        bd8e76e775d579271d141a026c5e3e7abc3cd4
        c3170b74f820792ed9bdc1195795662271f2ef
      c1/
        d21363b7bcc1ac97a45405a4c5c8ec3f789d6e
      c2/
        906e28dec7b8d1539d30f6cc1dd6cf11cfff08
        a04f00f272a7328de06acb0e4c4331cceff1fd
      c3/
        3afe698b93d7de76711ff8cd706821aa722007
        baeee8d9b6021c48fcdf93b3068c20b7ba0b74
      c8/
        ad7372a7def273034b6a078fef7ca207f3b6ad
      cf/
        aae9373284c74755573f8ee0cfe7d57812d7a9
        c9a6092bfd0b3e43af5256f837a6ab3b43f990
      d0/
        830085ecfecaf0a57fcbecea0ba419c9502d37
      d1/
        5b66c07e8827fdd0ff4354331ccaa2a00e7de2
        6e97d7914485c17e0395f580442377db5a6658
      d4/
        1acbc388a89af3183b27562f3ae301a51ede7c
        369ec968473b131b989bd32ed2431e61002096
        4f8353405ccca528e4d91f00dd71825cf5ef37
      d5/
        8de88f57ddaa8d332ac4d3b4bcd776dbd6572a
        99a575e43c7c83e9fe8dadf5982b4514f1a9b3
        e583ccc795e00605e7c75a4eba3162fca04b6a
      d6/
        c8f4d9982919fbcc4daf2c64056af136a9b68a
        e9212fe9ba23ffa73805da67ca595142ba353a
      d7/
        3c2aa346c4756a3785227ab1fa619dc44e90d9
        6d7d738d3cace730bd2cbcfbaa64dbc3c08941
      d8/
        d1ed3944b547bbe2511caa5f1b4bfc1a347766
      d9/
        2ec90575ca8351eef4c3c7b7eca228689d0f01
      da/
        fba3c24607c591762386d2af6047361a2e041f
      db/
        86821b7ebe4cc027e9176c2fa8ccf89d90c207
      dc/
        b57f8e7615e87a5d2d391db65f9c50052ab8f8
      de/
        8f2375bab107f80db8c9855adc53638dfa95b8
      e2/
        612971942d9216aa098999000dbf0599ea74bb
        de9eb013671744299ebd688689ba41db573009
      e4/
        23a6d30dd2a4e9a9235da54e88e8af30e90934
        fce11e6093eba1cad00b3d2eb227a99936e0b7
      e5/
        0711076ac8770cd864d52f8c9e402c761083be
        25e4aa93a233985efa5c6980965c982da2ebb1
        9a65a16d5731fa0068fbdb27c0aa8b23e77f1c
      e6/
        9de29bb2d1d6434b8b29ae775ad8c2e48c5391
      e7/
        c459bebff100d6a85bd8fd47478eefd6518774
      e9/
        03f469138532261bea6eb9e185fe490403534c
        a63bebc59f897bfcd9dba47ab7d62c44dd4314
      ea/
        a6379d7ff568b3928386891be87431e5ff37c2
      eb/
        0bd551fbd0728313d2506a006a98dcf3ce2d5d
      ed/
        241cfe9cef4bfbdedee9213a46728ed3f5cf9b
      ee/
        d71643e8aed4707ef95898270508c15ecb108f
      ef/
        a77066fea8765db4e8fd7c7cb90f4552e77afb
      f0/
        2431dcbc87e191f13e71aaa2a241a1aaa9a5d4
        c50edbf917d522043159af34a2e6d444df180e
      f1/
        e585438b9f9ea720321a283870ca34e9558499
      f2/
        1b2295f71004f83504b9b53baa11fcb390cdf6
      f3/
        ba9de61264fb775bc62fb9fa6a086230c79af4
      f4/
        37427c723f2716bb1e6d54dde5bd532bd632f9
        4e9bbc4bffc0a5955d72b5ec195677dfc8d7ad
      f5/
        3cc46fccfb11d32b826605287372f10bcb1e77
      f6/
        df3351505ab09ea62622a396aa7cb59eeeeff1
      f7/
        0ab3ca71f74ca3687a641251a8624ce3586cdb
      f9/
        867422c063e9ea439b567ef2afb06f4eb0b434
      fa/
        9820ad9c9fc2e4c97af8c2e2353588587613e2
      fb/
        18f9b4ddd9a0db174e5d0a3d453e2344690ff8
        2eb0c874fd0f9196e363705ea8fcdb30117d81
      fc/
        57024cbae6ddcf9674749fb76b7cc17bad70c8
      fd/
        b19d3a12e64485de2dbf1f567a2b402b7c3316
      fe/
        79e7225bcbf1077fff19be1e6048abe838ca9b
        b0a73567855a2ea3e16e2870479f7416dbacf6
      ff/
        212f6d63b551ff9b9254bb89d0e0641d7afd3a
        4bc98b7cec201a37c9e5c7f983594e49cba2b4
        e3e680c86868fe4470fe6d7ccd27be6a1b0c44
      info/
      pack/
    refs/
      heads/
        main
        feature/
          add-SQLite
      remotes/
        origin/
          main
          feature/
            add-SQLite
      tags/
  alembic/
    env.py
    README
    script.py.mako
    versions/
      269625ee49fe_add_status_column_to_assessment_table.py
      5976b03ee0fc_create_interrogation_records_table.py
      72143d5034d8_create_users_table.py
      a43553c2237e_add_attributes_and_assessment_.py
      __pycache__/
        269625ee49fe_add_status_column_to_assessment_table.cpython-311.pyc
        5976b03ee0fc_create_interrogation_records_table.cpython-311.pyc
        72143d5034d8_create_users_table.cpython-311.pyc
        a43553c2237e_add_attributes_and_assessment_.cpython-311.pyc
    __pycache__/
      env.cpython-311.pyc
  app/
    main.py
    __init__.py
    core/
      celery_app.py
      config.py
      deps.py
      redis_client.py
      security.py
      __init__.py
      __pycache__/
        celery_app.cpython-311.pyc
        config.cpython-311.pyc
        deps.cpython-311.pyc
        security.cpython-311.pyc
        __init__.cpython-311.pyc
    crud/
      assessment.py
      attribute.py
      interrogation.py
      stats.py
      user.py
      __init__.py
      __pycache__/
        assessment.cpython-311.pyc
        user.cpython-311.pyc
        __init__.cpython-311.pyc
    db/
      base_class.py
      init_db.py
      session.py
      __init__.py
      __pycache__/
        base_class.cpython-311.pyc
        init_db.cpython-311.pyc
        session.cpython-311.pyc
        __init__.cpython-311.pyc
    models/
      assessment.py
      association_tables.py
      attribute.py
      interrogation.py
      user.py
      __init__.py
      __pycache__/
        assessment.cpython-311.pyc
        association_tables.cpython-311.pyc
        attribute.cpython-311.pyc
        interrogation.cpython-311.pyc
        user.cpython-311.pyc
        __init__.cpython-311.pyc
    routers/
      admin.py
      assessments.py
      auth.py
      encyclopedia.py
      reports.py
      scales.py
      sse.py
      __init__.py
      __pycache__/
        assessments.cpython-311.pyc
        auth.cpython-311.pyc
        encyclopedia.cpython-311.pyc
        pages.cpython-311.pyc
        reports.cpython-311.pyc
        scales.cpython-311.pyc
        sse.cpython-311.pyc
        __init__.cpython-311.pyc
    schemas/
      assessment.py
      attribute.py
      encyclopedia.py
      guidance.py
      interrogation.py
      report.py
      scale.py
      stats.py
      token.py
      user.py
      __init__.py
      __pycache__/
        assessment.cpython-311.pyc
        encyclopedia.cpython-311.pyc
        report.cpython-311.pyc
        scale.cpython-311.pyc
        token.cpython-311.pyc
        user.cpython-311.pyc
        __init__.cpython-311.pyc
    tasks/
      analysis.py
      __init__.py
      __pycache__/
        analysis.cpython-311.pyc
        __init__.cpython-311.pyc
    __pycache__/
      main.cpython-311.pyc
      __init__.cpython-311.pyc
  config/
    config.yaml
    psychology_encyclopedia.json
  input/
    images/
      dog_and_girl.jpeg
      image1.jpg
      TestPic.jpg
      testpic1.jpg
      v2-b7ebd7f0a95bb411884521c43f19c846_xld.jpg
    questionnaires/
      1测你性格最真实的一面.json
      2亲子关系问卷量表.json
      3焦虑症自评量表 (SAS).json
      4标准量表：抑郁症自测量表 (SDS).json
      5人际关系综合诊断量表.json
      6情绪稳定性测验量表.json
      7汉密尔顿抑郁量表HAMD24.json
      8艾森克人格问卷EPQ85成人版.json
      9开心测试.json
  logs/
    app.log
  output/
    descriptions/
      testpic1_desc.txt
    reports/
      testpic1_report.txt
  src/
    ai_utils.py
    api_tester.py
    data_entry.py
    data_handler.py
    guidance_generator.py
    image_processor.py
    import_questions.py
    interrogation_ai.py
    report_generator.py
    utils.py
    __pycache__/
      ai_utils.cpython-311.pyc
      api_tester.cpython-311.pyc
      data_handler.cpython-311.pyc
      image_processor.cpython-311.pyc
      report_generator.cpython-311.pyc
      utils.cpython-311.pyc
  uploads/
    10000000000000_20250416201304_testpic1.jpg
    10000000000000_20250416204749_testpic1.jpg
    10000000000000_20250416210311_testpic1.jpg
    10000000000000_20250416213454_testpic1.jpg
    1111111111111111111111_20250424130717_neu_logo2.png
    1111111111111111111111_20250424130719_neu_logo2.png
    1111111111111111111111_20250424130724_neu_logo2.png
    1111111111111111111111_20250424130725_neu_logo2.png
    1111111111111111111111_20250424130733_neu_logo2.png
    1111111111111111111111_20250424132106_testpic1.jpg
    1111111111111111111111_20250424153455_neu_logo2.png
    111111111111111111120_20250425001629_testpic1.jpg
    111111111111111111121_20250425002213_testpic1.jpg
    111111111111111111_20250426163131_testpic1.jpg
    111111111111111112_20250426163306_testpic1.jpg
    111111111111111114_20250426002453_testpic1.jpg
    111111111111111115_20250426163917_testpic1.jpg
    111111111111111118_20250426164308_testpic1.jpg
    111111111111111119_20250426164652_testpic1.jpg
    111111111111111120_20250426165549_testpic1.jpg
    1223123123123123_20250417140740_bc3ca98535f261d084345205d7ed53b.jpg
    211401200000000000_20250416173352_testpic1.jpg
    211401200000000001_20250416173600_testpic1.jpg
    211401200000000001_20250416174140_testpic1.jpg
    211401200000000001_20250416174244_testpic1.jpg
    211401200000000001_20250416174317_testpic1.jpg
    211401200000000001_20250416174651_testpic1.jpg
    211401200000000001_20250416175232_testpic1.jpg
    211401200000000001_20250416180044_testpic1.jpg
    211401200000000008_20250416200605_testpic1.jpg
    211401200000000008_20250416222213_testpic1.jpg
    211401200001010001_20250425201116_testpic1.jpg
    211401200001010002_20250425202345_testpic1.jpg
    211401200001010003_20250425205400_testpic1.jpg
    211401200001010004_20250425222951_testpic1.jpg
    211401200001010005_20250425224806_testpic1.jpg
    211401200001010006_20250425225357_testpic1.jpg
    211401200001010007_20250425231129_testpic1.jpg
    211401200001010008_20250425232215_testpic1.jpg
    211401200001010009_20250425233352_testpic1.jpg
    211401200001010010_20250426223700_testpic1.jpg
    211401200001010055_20250426224255_testpic1.jpg
    string_20250417112455_bc3ca98535f261d084345205d7ed53b.jpg
    string_20250417142126_bc3ca98535f261d084345205d7ed53b.jpg
    string_20250417142942_bc3ca98535f261d084345205d7ed53b.jpg
    _20250426002802_testpic1.jpg
    _20250426003334_testpic1.jpg
    _20250426004050_testpic1.jpg
    _20250426004708_testpic1.jpg
    _20250426145725_testpic1.jpg
    _20250426151101_testpic1.jpg
    _20250426151421_testpic1.jpg
    _20250426151844_testpic1.jpg
    _20250426152017_testpic1.jpg
    _20250426153346_testpic1.jpg
    _20250426153449_testpic1.jpg
    _20250426154346_testpic1.jpg
    _20250426155032_testpic1.jpg
    _20250426155702_testpic1.jpg
    _20250426160837_testpic1.jpg
    _20250426162610_testpic1.jpg
    _20250426173648_APP_.png
  __pycache__/
    main.cpython-311.pyc
  文档部分/
    finding_work.xlsx
    倾听者 心理学项目需求分析 (2).md
    倾听者 心理学项目需求分析.md
    倾听者 心理学项目需求分析.pdf
    倾听者_任务需求分析与技术选型.pdf
    倾听者项目详细流程表_V2.1.xlsx
    绘画分析AI智能解析(4)(1).xlsx


==================================================
第二部分：文件内容
==================================================

--------------------------------------------------
文件路径: code2txt.py
--------------------------------------------------
import os
from datetime import datetime

def generate_project_documentation(root_dir, output_file):
    """
    将项目文件夹内容生成到TXT文件，包括目录结构和文件内容
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        # 写入文档头部
        f.write(f"项目文档生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # 第一部分：项目架构
        f.write("=" * 50 + "\n")
        f.write("第一部分：项目目录结构\n")
        f.write("=" * 50 + "\n\n")
        
        # 遍历目录结构
        for root, dirs, files in os.walk(root_dir):
            # 计算层级深度
            level = root.replace(root_dir, '').count(os.sep)
            indent = "  " * level
            # 写入目录名
            f.write(f"{indent}{os.path.basename(root)}/\n")
            # 写入文件名
            for file in files:
                f.write(f"{indent}  {file}\n")
        f.write("\n\n")
        
        # 第二部分：文件内容
        f.write("=" * 50 + "\n")
        f.write("第二部分：文件内容\n")
        f.write("=" * 50 + "\n\n")
        
        # 遍历所有文件并写入内容
        for root, _, files in os.walk(root_dir):
            for file in files:
                file_path = os.path.join(root, file)
                # 只处理文本类型的文件，可以根据需要扩展
                if file.endswith(('.py', '.txt', '.md', '.json', '.html', '.css', '.js')):
                    f.write("-" * 50 + "\n")
                    # 写入文件相对路径
                    relative_path = os.path.relpath(file_path, root_dir)
                    f.write(f"文件路径: {relative_path}\n")
                    f.write("-" * 50 + "\n")
                    
                    try:
                        with open(file_path, 'r', encoding='utf-8') as source_file:
                            content = source_file.read()
                            f.write(content)
                    except Exception as e:
                        f.write(f"读取文件出错: {str(e)}\n")
                    f.write("\n\n")

def main():
    # 获取当前目录，可以修改为指定目录
    current_dir = os.getcwd()
    output_filename = "project_documentation.txt"
    
    try:
        generate_project_documentation(current_dir, output_filename)
        print(f"文档已生成: {os.path.abspath(output_filename)}")
    except Exception as e:
        print(f"生成文档时出错: {str(e)}")

if __name__ == "__main__":
    main()

--------------------------------------------------
文件路径: Combiner.py
--------------------------------------------------
import os
import sys
from tqdm import tqdm

# 定义允许被合并内容的文件扩展名
ALLOWED_EXTENSIONS_FOR_CONTENT = ('.py','.yaml','.json','.csv','.html')

# --- 恢复 generate_tree 到原始版本，以包含所有文件 --txt-
def generate_tree(root_dir):
    """生成类似 tree /f 的目录树字符串 (包含所有文件和目录)"""
    tree_output = [root_dir]
    # os.walk 会自然遍历所有子目录和文件
    for root, dirs, files in os.walk(root_dir):
        level = root.replace(root_dir, '').count(os.sep)
        indent = '    ' * level
        if level > 0:  # 避免重复输出根目录
            tree_output.append(f"{indent}{os.path.basename(root)}")
        
        # --- 在目录树中列出该目录下的所有文件 ---
        sub_indent = indent + '    '
        for file in files:
            # 不进行扩展名过滤，列出所有文件
            tree_output.append(f"{sub_indent}{file}")
            
    return "\n".join(tree_output)

def concatenate_files(root_dir, output_file):
    # 计算总文件数以初始化进度条 (反映遍历的所有文件)
    total_files = sum(len(files) for _, _, files in os.walk(root_dir))

    # 预计算将被合并内容的文件数（可选，如果你想让进度条只反映合并进度）
    # included_files_count = 0
    # for root, dirs, files in os.walk(root_dir):
    #     for file in files:
    #         _, ext = os.path.splitext(file)
    #         if ext.lower() in ALLOWED_EXTENSIONS_FOR_CONTENT:
    #             included_files_count += 1
    # print(f"找到 {included_files_count} 个符合条件的文件将被合并内容。")


    with open(output_file, 'w', encoding='utf-8') as outfile:
        # --- 写入完整的目录树 ---
        outfile.write("Full Project Directory Tree:\n") # 标题明确说明是完整树
        outfile.write(generate_tree(root_dir) + "\n") # 调用未经过滤的 generate_tree
        outfile.write("=====\n\n")
        outfile.write("Concatenated File Content (Filtered):\n") # 明确说明内容是过滤后的

        # 使用 tqdm 显示进度条 (total 仍然是所有文件数，反映遍历进度)
        # 如果使用上面的 included_files_count，这里改为 total=included_files_count
        with tqdm(total=total_files, desc="扫描文件并合并内容", unit="file") as pbar:
            for root, dirs, files in os.walk(root_dir):
                for file in files:
                    file_path = os.path.join(root, file)

                    # --- 核心过滤逻辑：只处理指定扩展名的文件内容 ---
                    _, ext = os.path.splitext(file)

                    # 检查扩展名是否在允许合并内容的列表中
                    if ext.lower() in ALLOWED_EXTENSIONS_FOR_CONTENT:
                        # --- 文件内容合并逻辑 ---
                        try:
                            with open(file_path, 'r', encoding='utf-8') as infile:
                                content = infile.read()
                                outfile.write("FILE: " + file_path + "\n")
                                outfile.write(content + "\n")
                                outfile.write("-----\n")
                            print(f"已合并内容: {file_path} - 成功")
                        except Exception as e:
                            # 即使读取失败，也记录下来
                            outfile.write(f"FILE: {file_path}\n")
                            outfile.write(f"读取文件出错 (尝试合并内容时): {str(e)}\n")
                            outfile.write("-----\n")
                            print(f"尝试合并内容失败: {file_path} - 错误: {str(e)}")
                    else:
                        # --- 文件类型不符，跳过内容合并 ---
                        # 不需要写入文件内容，只在控制台打印信息
                        print(f"已跳过内容合并 (类型不符): {file_path}")

                    # --- 更新进度条 ---
                    # 每个访问的文件都更新进度条
                    pbar.update(1)

if __name__ == "__main__":
    root_dir = sys.argv[1] if len(sys.argv) > 1 else os.getcwd()
    output_file = 'project_text1.txt'
    print(f"正在扫描目录: {root_dir}")
    print(f"将生成包含所有文件的目录树，并合并后缀为 {', '.join(ALLOWED_EXTENSIONS_FOR_CONTENT)} 的文件内容到: {output_file}")
    concatenate_files(root_dir, output_file)
    print(f"处理完成。输出文件: {output_file}")

--------------------------------------------------
文件路径: create_initial_user.py
--------------------------------------------------


--------------------------------------------------
文件路径: project_text.txt
--------------------------------------------------
Full Project Directory Tree:
C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis
    .env
    .gitignore
    alembic.ini
    code2txt.py
    Combiner.py
    create_initial_user.py
    create_structure.bat
    find _token.py
    main.py
    project_documentation.txt
    project_text.txt
    psychology_analysis.db
    README.md
    requirements.txt
    run_celery_worker.py
    .git
        COMMIT_EDITMSG
        config
        description
        HEAD
        index
        hooks
            applypatch-msg.sample
            commit-msg.sample
            fsmonitor-watchman.sample
            post-update.sample
            pre-applypatch.sample
            pre-commit.sample
            pre-merge-commit.sample
            pre-push.sample
            pre-rebase.sample
            pre-receive.sample
            prepare-commit-msg.sample
            push-to-checkout.sample
            update.sample
        info
            exclude
        logs
            HEAD
            refs
                heads
                    main
                    feature
                        add-SQLite
                remotes
                    origin
                        main
                        feature
                            add-SQLite
        objects
            00
                1ad4bb4a59cbdb0ac1ddf134f9503ee4eb941d
                f1f9a338f4ecda0461e2b1e919d3679f88b02c
            01
                b512c1b975ae8ed48bec78d2f2a797aee04175
            03
                5c221979f77795910253f051d9d03dff9c4085
                a64fae01884fb7377a512d0118eac53cfe9345
            04
                eb1421784606bd918c72ce21ee7f0278bebf15
            0c
                dec50e08b0ab11ba7d53f7b989a6c614c31abb
            0d
                2b14ced5984d5fcbd7e0c3d4b8824f31c1fa86
                e0b6d0d203267db04218b4aa21ed5542ab10d7
            0f
                8f77aaad74eba52509e01efc00b4bd4f2db5a0
                be2c23a32171ca42eed24e8b48eb6ae70e50bc
            10
                59cc4ab69a96f51712a550fe291d6dd4fdb2b7
            12
                170485ae2e5af27f6a5612b1af82aa1aca1d16
            13
                14ce251df032572faa5b4b1c54741b762c2859
                50414ac811c9720f85d6c3236bbd79d7d0a0af
            14
                755b8a01c4534431afaa63e9a9e69855fe9693
            15
                530d58381cfaa592f432d06d0e3b4d75979fa1
            16
                428857e8476be65ef739ea56109654e7d44ed0
            17
                9554ed9b3425e2cedc13865bd9a73f5cbe290a
            18
                0028419bd3d3c73905a966756a8c23bd13192b
            19
                0edb5d839e6ba4394ea560eb1aacf1e0ff2ba7
                9a7229ad454705cff2987ca2c9aa8844d6e672
            1a
                33b1dc7c13e7687022f838a0701e0beecc73ec
                62c857ba278139810ddc9efba3815ce8a475ac
                6f5248e38415868d72b959451bca579a80cc0a
            1b
                19c678ab390732d2693b1bc7ddd31c48e9986c
                a8056d26b4e150beab2d9c4cb520985febb136
            1c
                17b125e8414f73946d0502609d81a080166e1e
                21834c10f9c325ff3f5f8afaf7ac14706952be
                f1118dba58a12d8abdf85a1264f9302dc989ce
            1e
                ae7d3c4e6aa8867a3921270d822ec7492a23d7
            1f
                11b9b2ac4c648475275a6821b3e2de1a44bd37
            20
                336ee91d617c0235f9d18d07e6a6c9f47aafad
            22
                59b8381631ec194eeadb805f64296cfadb01fc
            23
                1c188fbd04c8f681e6fa84ca53015c009785a7
            25
                54701d69ac6a2ca0ad95cff5cfdc419216e794
                57f71c9ab0fca990b555b577551c4d8388a2b8
            26
                27167eeb025b72af9103bbf8120620c63f1bb5
                ed667e9c00a416625d8a9f03dd8f6b26be218d
            27
                c2f6dc9369ae95547ad0780469b687901bb84f
            2b
                8466c8c4b97b5b581473c6ec56ebf403bb0b18
                8d7c06000aedc326c3645fc1dda5d8acc245b9
            2d
                0cae5aa5df3e38bbc4c530a43b2e5eaf23f3b3
            2f
                09c0b9da126a06729c3e25128161354403c93f
            34
                06ce50c42fef949edef25cdf5ac8becec27af3
                120dd588c96631e266483790d2cd3f883f66f4
                c2e7bdb2747a29de75088cac02a29c10dc3bbc
            35
                13a5221a131ea18fa6e04ec98ab8e306d6757c
                5f2230940baa069ffb5627a42923445941d06f
            36
                2dfbf805088266f80261136a0d3e8803898796
            38
                267654cd80ba5e7e549b8385688d0dc8ef81ff
            39
                584dae458b91ca9abdca53f9c8cc5a484b97c0
                c637c889413858d81833e91447fbd86a105c31
                ddaa0a1662a16d4bed68b5f00eb789376af67c
            3a
                28b5c3dcc80e82689df7622b9d73041a737b36
                882a9b62d80bf5ca77eccd47a0da644b092347
            3c
                6c57c7a5a0bf6998193136db26f5459bb9171b
                a15b11440a25f49e320de5f5f0bc04e81c7cc6
            3d
                18e559d61ec6bc9fec6f549a7890a43aedf304
                e427da87f0a7d2cdb8af4ccb16cf7f73d4c401
            3e
                4dafd7691ec3dd1bde9bd59fcdf28f4098af60
                e972b88be666ad384fcdb25249c2cdd74260d6
            41
                9845770072fcc3e254b3a41f080426265f0eda
                9f3b0d4b1483339db94525c683a6663afac1e1
            42
                0298caf723214e4ede35e42ad7d56bbf7be4b8
                415ba3df9187024b32f5667ecced22a4a51f32
                595efb99cc2f55484cfda26bcec2cdf02219f9
            43
                deef336344327856d1d860ffb3407000840513
            44
                a22b5940ed22e2a04d6093dd30ed791c441592
            45
                a1421ecb1fb92f95faca87c8748c2648a58afd
                ea234cb91b3420333e1752b0c8f14dc4e08c9c
            46
                a29fd2a2ef9d52e6ec457931cb5e874d24bd8a
            48
                0b130d632ca677c11f23d9fe82cf4014d15e0c
                b8f448bd5c8cace7fa482e9281b1f836bdc640
            4c
                db36f62668e963b26a42e3030fcc299fe3f55a
            4d
                d738d6463eac7b454eeb9fa96bcf1e2f486a4f
            4f
                994894eb2c7d18d16bfe83c88d60b0740d8802
                edcf43b25bd44e4e0fbb06954ba5cc9451c22a
            52
                d7c1a18ef930c1c1c98bf248a2e842d15379ff
            53
                07e0809921b46189f314499d02be4faa6772e7
                26d681a358ad0f30e0e6137763b25ed3813393
            56
                028992b3e0f68e084b289347cfdb3e9453e9dd
            57
                bd8279b9e982261ccff432113f3fae4f0ac51c
            58
                9c9062493fa10a00760ab9a98485bacf0bcfb6
            5e
                eae694ae1c7c16bc977e84ccd415c2113efea2
            5f
                2411d567b115a13565f5354f5e9a162f24a345
            61
                29feb5c75b4413880e15b4e76e31dee7b45f89
                efc86772780c5190adcbc73eb739b3d292bfa7
            62
                907838025ec3e0009b09ea0c383a67b3ed6680
            65
                9d2f11fde57f3cc3c2a8d5d60f8fe5479b92ff
            67
                021574883946a5e8a244453fa4869d2547f19a
                39dd22283de3c81abc1959398b87adb215b5b3
            6d
                865406aaaea9879f077b737ac203306abb9e8f
                ea44ab256461f55f054b63f1e82379251af10e
            6e
                0865849ffaffe3c16a97d8df0428a813ef43c1
                e1fa34f83ee5d6685c30b0684793c0d406aab0
            72
                308b59813d15637bdb4462ee2e5705b354bb22
                32676ad66a4d556998272eebb32fb4568d502e
                63f477640e2b25706b08db2b0ab34e45fc6b38
                dfd07de33c644e5be7185e0111ec4727cf9467
            74
                76edb315d4a8bb670a333a8a0f102ce2105405
            76
                50be1332e30e87a4d56178ef6db340065aff2f
                696ae0f5c0350e25cc62f892b8c8e03a61ce66
                a61f3fd9dc64dd502566996a7c6f9cbda4e122
            77
                907e45232406ba49a0de1019cd672740518b52
            79
                3ab6a06621a5827dabf94dc303366b409e7d78
                5aeef71f00d4c48a4315bf8eced76dc2d3a56f
            7a
                205d911dae4499a5957c11f70426eceec86139
            7c
                72e7dbefae2a2dd9f5b26bab1a194f0ae0700f
            7d
                0d7c2b67e6a4b5ab3c01f5417c717d1f51e52a
                7d6bab7cd95ea4af27fd1781fd4e57a4e7bdd7
                bcf05e56e4a378de1f43bf544fc13fc05a1ad5
            7f
                92744e31f87d08096844b04acbf5fb45c62efc
            80
                575028454e49098fcac7da363c615ab6838d00
            82
                f741be7012214e29cb1b7e452aa701fd37c75d
            83
                5c1af93b81cc26e7c3d9403fc96f1c60de7515
            84
                2798011a3f194239dda371cd92b3b606418abb
            86
                d2e82f7f33c7ce049891c2387e47d0c282c3b0
                eecf0e40e7e01e022163d14e7f557482c6a1d6
            89
                ccd8534dbf253b68b8ab392bed537085641801
            8a
                69f975adb32451e3c1090b62857d1e27cd3b04
                f0f895f24c72be1493fa46fa26ba80ed4bd0c1
            8b
                d66ba2773e5362cb3d2578eff8dba1cc8a0354
            8c
                d0beec64208d1801b02eb3d5732570f716708e
            8d
                94250928a83dbb62e59823cbf273ec14178f22
            8e
                66a6c876b707b5559d907d27f61365d24e1f85
            91
                9f90966c62717988fe0c3aacb1a900f1e72cb4
            94
                3f49c6131b8b6d0d063ca6bd7387e62b21de53
            95
                0d92e48c3164e57dc7c01df4e44a6c1c0bc173
            97
                d61d380edd68fe5e76eede90e99cc14e1d857b
            9c
                2e8b4dc8699a85bf6e105b5652418429bba17d
                35ff5a2c8019394506ae45a311b2ad37174ab0
            9e
                eecf53488c2bf7144ceb9f6e123fda2a27e7f2
            9f
                acd9e873402569445133d0b91ac3665a77c761
                dc040050624556464ffa5112dde397ccd792c6
            a1
                9693d4b6c943939f6b36383e9ceaa6491bbd1d
                e7d58650dedca81c92e10a43c6ee9c9f484420
            a2
                7a13e496942e077a2e4392e79a14be597fd0e6
                7ebc9f9b4dfe82cf9cfea0ab6b549aa9c04c6a
            a3
                36e5fe3ad8f45caa601a8c853601ec4c8f0ad4
            a5
                8d010bd1849ea7514bb1d0751c29ca3ba6f2f7
                fb821f3a960767b8861c06a9879fb2535ce7f3
            a6
                3975dc55b149a1531c06adcb3866fad511d1da
                e468cc369ffaf54bc3600c257e772165232b49
            a7
                756130bb1fd01db91b8be0a7bd844faccebe79
                afc0621c9b834d8f139495b113ad430ebdd065
            a8
                210548c126f9a57e25b9fa7bdc3d15d88a152e
            aa
                9674bff49341d7c6b0a6ab8ba34e8ec3372c68
            ab
                7098f5e7d3e65d1dc78736269630ae8873a82b
            ac
                197e9723b0c8d0c03b3e5698343bdcc78fdc93
                77806aed7ad5dc8f60c6c66e8b709b437766ab
                9f9be8794d6e4f9f54185a0942bd88d846ea1f
            ad
                29b33bf959eed8dce123de959454f89e26c2c0
                99bba2735d03e995d5a854935a67d871214c16
            ae
                0a0841941a896a8f3bc32244cfef832aab8225
            af
                1fc84185401155dab36f87f271cb2cb924a958
            b0
                f1d69a4cdfb7075c9fff3c750c173dee824d35
            b1
                725e7c3b8f783170d0479f18e67c416ad4e633
            b2
                3a63d157cf47e7778eca8cc2904d52be305b5d
            b3
                078c8228687749f4723fd9fa35f9f64dbbc242
                e4b3d8822d9208dfb2b5b5dddf512c4b80c844
                f232bdd903e435a62026e1f27119f15961782c
            b4
                0bdefdc1827e0dbcfe065f9350ee23574af4ab
            b7
                44a021db5e8dee616f2a2b299181d4abf5c556
                752a996e380022274105861387fd7e8bf0ea02
            b8
                5d4173ea3a9dcf2599eb5d41406a0cded0eb40
            ba
                68d800e780b486a3488fab3acba5ffb7a5131d
            bb
                3865865a1f62bf3f323712d38f3835c249f185
            be
                19ca58bb5ada17902ef40ae0e7907a44b95ebf
                ae79856ebed36ffcfc2e640570278beb01b8d8
                cf22bf1b11f7e257acb2b1de382a2c40e3e3a6
            bf
                bd8e76e775d579271d141a026c5e3e7abc3cd4
                c3170b74f820792ed9bdc1195795662271f2ef
            c1
                d21363b7bcc1ac97a45405a4c5c8ec3f789d6e
            c2
                906e28dec7b8d1539d30f6cc1dd6cf11cfff08
                a04f00f272a7328de06acb0e4c4331cceff1fd
            c3
                3afe698b93d7de76711ff8cd706821aa722007
                baeee8d9b6021c48fcdf93b3068c20b7ba0b74
            c8
                ad7372a7def273034b6a078fef7ca207f3b6ad
            cf
                aae9373284c74755573f8ee0cfe7d57812d7a9
                c9a6092bfd0b3e43af5256f837a6ab3b43f990
            d0
                830085ecfecaf0a57fcbecea0ba419c9502d37
            d1
                5b66c07e8827fdd0ff4354331ccaa2a00e7de2
                6e97d7914485c17e0395f580442377db5a6658
            d4
                1acbc388a89af3183b27562f3ae301a51ede7c
                369ec968473b131b989bd32ed2431e61002096
                4f8353405ccca528e4d91f00dd71825cf5ef37
            d5
                8de88f57ddaa8d332ac4d3b4bcd776dbd6572a
                99a575e43c7c83e9fe8dadf5982b4514f1a9b3
                e583ccc795e00605e7c75a4eba3162fca04b6a
            d6
                c8f4d9982919fbcc4daf2c64056af136a9b68a
                e9212fe9ba23ffa73805da67ca595142ba353a
            d7
                3c2aa346c4756a3785227ab1fa619dc44e90d9
                6d7d738d3cace730bd2cbcfbaa64dbc3c08941
            d8
                d1ed3944b547bbe2511caa5f1b4bfc1a347766
            d9
                2ec90575ca8351eef4c3c7b7eca228689d0f01
            da
                fba3c24607c591762386d2af6047361a2e041f
            db
                86821b7ebe4cc027e9176c2fa8ccf89d90c207
            dc
                b57f8e7615e87a5d2d391db65f9c50052ab8f8
            de
                8f2375bab107f80db8c9855adc53638dfa95b8
            e2
                612971942d9216aa098999000dbf0599ea74bb
                de9eb013671744299ebd688689ba41db573009
            e4
                23a6d30dd2a4e9a9235da54e88e8af30e90934
                fce11e6093eba1cad00b3d2eb227a99936e0b7
            e5
                0711076ac8770cd864d52f8c9e402c761083be
                25e4aa93a233985efa5c6980965c982da2ebb1
                9a65a16d5731fa0068fbdb27c0aa8b23e77f1c
            e6
                9de29bb2d1d6434b8b29ae775ad8c2e48c5391
            e7
                c459bebff100d6a85bd8fd47478eefd6518774
            e9
                03f469138532261bea6eb9e185fe490403534c
                a63bebc59f897bfcd9dba47ab7d62c44dd4314
            ea
                a6379d7ff568b3928386891be87431e5ff37c2
            eb
                0bd551fbd0728313d2506a006a98dcf3ce2d5d
            ed
                241cfe9cef4bfbdedee9213a46728ed3f5cf9b
            ee
                d71643e8aed4707ef95898270508c15ecb108f
            ef
                a77066fea8765db4e8fd7c7cb90f4552e77afb
            f0
                2431dcbc87e191f13e71aaa2a241a1aaa9a5d4
                c50edbf917d522043159af34a2e6d444df180e
            f1
                e585438b9f9ea720321a283870ca34e9558499
            f2
                1b2295f71004f83504b9b53baa11fcb390cdf6
            f3
                ba9de61264fb775bc62fb9fa6a086230c79af4
            f4
                37427c723f2716bb1e6d54dde5bd532bd632f9
                4e9bbc4bffc0a5955d72b5ec195677dfc8d7ad
            f5
                3cc46fccfb11d32b826605287372f10bcb1e77
            f6
                df3351505ab09ea62622a396aa7cb59eeeeff1
            f7
                0ab3ca71f74ca3687a641251a8624ce3586cdb
            f9
                867422c063e9ea439b567ef2afb06f4eb0b434
            fa
                9820ad9c9fc2e4c97af8c2e2353588587613e2
            fb
                18f9b4ddd9a0db174e5d0a3d453e2344690ff8
                2eb0c874fd0f9196e363705ea8fcdb30117d81
            fc
                57024cbae6ddcf9674749fb76b7cc17bad70c8
            fd
                b19d3a12e64485de2dbf1f567a2b402b7c3316
            fe
                79e7225bcbf1077fff19be1e6048abe838ca9b
                b0a73567855a2ea3e16e2870479f7416dbacf6
            ff
                212f6d63b551ff9b9254bb89d0e0641d7afd3a
                4bc98b7cec201a37c9e5c7f983594e49cba2b4
                e3e680c86868fe4470fe6d7ccd27be6a1b0c44
            info
            pack
        refs
            heads
                main
                feature
                    add-SQLite
            remotes
                origin
                    main
                    feature
                        add-SQLite
            tags
    alembic
        env.py
        README
        script.py.mako
        versions
            72143d5034d8_create_users_table.py
            __pycache__
                72143d5034d8_create_users_table.cpython-311.pyc
        __pycache__
            env.cpython-311.pyc
    app
        main.py
        __init__.py
        core
            celery_app.py
            config.py
            deps.py
            security.py
            __init__.py
            __pycache__
                celery_app.cpython-311.pyc
                config.cpython-311.pyc
                deps.cpython-311.pyc
                security.cpython-311.pyc
                __init__.cpython-311.pyc
        crud
            assessment.py
            user.py
            __init__.py
            __pycache__
                assessment.cpython-311.pyc
                user.cpython-311.pyc
                __init__.cpython-311.pyc
        db
            base_class.py
            init_db.py
            session.py
            __init__.py
            __pycache__
                base_class.cpython-311.pyc
                init_db.cpython-311.pyc
                session.cpython-311.pyc
                __init__.cpython-311.pyc
        models
            assessment.py
            user.py
            __init__.py
            __pycache__
                assessment.cpython-311.pyc
                user.cpython-311.pyc
                __init__.cpython-311.pyc
        routers
            assessments.py
            auth.py
            encyclopedia.py
            reports.py
            scales.py
            __init__.py
            __pycache__
                assessments.cpython-311.pyc
                auth.cpython-311.pyc
                encyclopedia.cpython-311.pyc
                pages.cpython-311.pyc
                reports.cpython-311.pyc
                scales.cpython-311.pyc
                __init__.cpython-311.pyc
        schemas
            assessment.py
            encyclopedia.py
            report.py
            scale.py
            token.py
            user.py
            __init__.py
            __pycache__
                assessment.cpython-311.pyc
                encyclopedia.cpython-311.pyc
                report.cpython-311.pyc
                scale.cpython-311.pyc
                token.cpython-311.pyc
                user.cpython-311.pyc
                __init__.cpython-311.pyc
        tasks
            analysis.py
            __init__.py
            __pycache__
                analysis.cpython-311.pyc
                __init__.cpython-311.pyc
        __pycache__
            main.cpython-311.pyc
            __init__.cpython-311.pyc
    config
        config.yaml
        psychology_encyclopedia.json
    input
        images
            dog_and_girl.jpeg
            image1.jpg
            TestPic.jpg
            testpic1.jpg
            v2-b7ebd7f0a95bb411884521c43f19c846_xld.jpg
        questionnaires
            1测你性格最真实的一面.json
            2亲子关系问卷量表.json
            3焦虑症自评量表 (SAS).json
            4标准量表：抑郁症自测量表 (SDS).json
            5人际关系综合诊断量表.json
            6情绪稳定性测验量表.json
            7汉密尔顿抑郁量表HAMD24.json
            8艾森克人格问卷EPQ85成人版.json
    logs
        app.log
    output
        descriptions
            testpic1_desc.txt
        reports
            testpic1_report.txt
    src
        ai_utils.py
        api_tester.py
        data_entry.py
        data_handler.py
        image_processor.py
        import_questions.py
        report_generator.py
        utils.py
        __pycache__
            ai_utils.cpython-311.pyc
            api_tester.cpython-311.pyc
            data_handler.cpython-311.pyc
            image_processor.cpython-311.pyc
            report_generator.cpython-311.pyc
            utils.cpython-311.pyc
    uploads
        10000000000000_20250416201304_testpic1.jpg
        10000000000000_20250416204749_testpic1.jpg
        10000000000000_20250416210311_testpic1.jpg
        10000000000000_20250416213454_testpic1.jpg
        1111111111111111111111_20250424130717_neu_logo2.png
        1111111111111111111111_20250424130719_neu_logo2.png
        1111111111111111111111_20250424130724_neu_logo2.png
        1111111111111111111111_20250424130725_neu_logo2.png
        1111111111111111111111_20250424130733_neu_logo2.png
        1111111111111111111111_20250424132106_testpic1.jpg
        1111111111111111111111_20250424153455_neu_logo2.png
        111111111111111111120_20250425001629_testpic1.jpg
        111111111111111111121_20250425002213_testpic1.jpg
        1223123123123123_20250417140740_bc3ca98535f261d084345205d7ed53b.jpg
        211401200000000000_20250416173352_testpic1.jpg
        211401200000000001_20250416173600_testpic1.jpg
        211401200000000001_20250416174140_testpic1.jpg
        211401200000000001_20250416174244_testpic1.jpg
        211401200000000001_20250416174317_testpic1.jpg
        211401200000000001_20250416174651_testpic1.jpg
        211401200000000001_20250416175232_testpic1.jpg
        211401200000000001_20250416180044_testpic1.jpg
        211401200000000008_20250416200605_testpic1.jpg
        211401200000000008_20250416222213_testpic1.jpg
        211401200001010001_20250425201116_testpic1.jpg
        211401200001010002_20250425202345_testpic1.jpg
        211401200001010003_20250425205400_testpic1.jpg
        211401200001010004_20250425222951_testpic1.jpg
        string_20250417112455_bc3ca98535f261d084345205d7ed53b.jpg
        string_20250417142126_bc3ca98535f261d084345205d7ed53b.jpg
        string_20250417142942_bc3ca98535f261d084345205d7ed53b.jpg
    __pycache__
        main.cpython-311.pyc
    文档部分
        finding_work.xlsx
        倾听者 心理学项目需求分析 (2).md
        倾听者 心理学项目需求分析.md
        倾听者 心理学项目需求分析.pdf
        倾听者_任务需求分析与技术选型.pdf
        倾听者项目详细流程表_V2.1.xlsx
        绘画分析AI智能解析(4)(1).xlsx
=====

Concatenated File Content (Filtered):
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.env
# .env Example
# --------------------------------------
# API Keys
# --------------------------------------
# 替换为你真实的 Dashscope API Key
DASHSCOPE_API_KEY="sk-0ffc64dd9d614c0088ee28e3a072420a"

# --------------------------------------
# Database Configuration (Example for future MySQL)
# --------------------------------------
# DATABASE_URL="mysql+mysqlclient://user:password@host:port/database_name?charset=utf8mb4"

# --------------------------------------
# Application Settings
# --------------------------------------
# 设置应用名称或环境 (development, staging, production)
ENVIRONMENT="development"
APP_NAME="Qingtingzhe AI Analysis"
# API V1 前缀 (可选)
API_V1_STR="/api/v1"

# --------------------------------------
# Security (Generate strong secrets for production)
# --------------------------------------
# 用于 JWT 签名的密钥，运行 openssl rand -hex 32 生成
SECRET_KEY="01c33c328775c0d492af2a0b1ed692f96347ec4c54d0622560022bb3cf45d2e3"
# JWT 算法和过期时间（秒）
ALGORITHM="HS256"
ACCESS_TOKEN_EXPIRE_MINUTES=30

# --------------------------------------
# CORS Origins (允许的前端地址)
# --------------------------------------
# 开发时可以允许本地地址，生产环境需要精确指定
# 使用逗号分隔，例如 "http://localhost:8080,http://127.0.0.1:8080,https://your-frontend.com"
BACKEND_CORS_ORIGINS=http://localhost,http://localhost:8080,http://127.0.0.1,http://127.0.0.1:8080

# --------------------------------------
# Logging Level
# --------------------------------------
LOG_LEVEL="INFO" # DEBUG, INFO, WARNING, ERROR, CRITICAL

# --- CORS 配置 ---
# 允许来自 FastAPI 服务本身 (运行在 8000 端口) 的请求
# 包含 localhost 和 127.0.0.1 以及模拟器特殊 IP
BACKEND_CORS_ORIGINS_STR="http://localhost:8000,http://127.0.0.1:8000,http://10.0.2.2:8000"
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.gitignore

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\code2txt.py
import os
from datetime import datetime

def generate_project_documentation(root_dir, output_file):
    """
    将项目文件夹内容生成到TXT文件，包括目录结构和文件内容
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        # 写入文档头部
        f.write(f"项目文档生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # 第一部分：项目架构
        f.write("=" * 50 + "\n")
        f.write("第一部分：项目目录结构\n")
        f.write("=" * 50 + "\n\n")
        
        # 遍历目录结构
        for root, dirs, files in os.walk(root_dir):
            # 计算层级深度
            level = root.replace(root_dir, '').count(os.sep)
            indent = "  " * level
            # 写入目录名
            f.write(f"{indent}{os.path.basename(root)}/\n")
            # 写入文件名
            for file in files:
                f.write(f"{indent}  {file}\n")
        f.write("\n\n")
        
        # 第二部分：文件内容
        f.write("=" * 50 + "\n")
        f.write("第二部分：文件内容\n")
        f.write("=" * 50 + "\n\n")
        
        # 遍历所有文件并写入内容
        for root, _, files in os.walk(root_dir):
            for file in files:
                file_path = os.path.join(root, file)
                # 只处理文本类型的文件，可以根据需要扩展
                if file.endswith(('.py', '.txt', '.md', '.json', '.html', '.css', '.js')):
                    f.write("-" * 50 + "\n")
                    # 写入文件相对路径
                    relative_path = os.path.relpath(file_path, root_dir)
                    f.write(f"文件路径: {relative_path}\n")
                    f.write("-" * 50 + "\n")
                    
                    try:
                        with open(file_path, 'r', encoding='utf-8') as source_file:
                            content = source_file.read()
                            f.write(content)
                    except Exception as e:
                        f.write(f"读取文件出错: {str(e)}\n")
                    f.write("\n\n")

def main():
    # 获取当前目录，可以修改为指定目录
    current_dir = os.getcwd()
    output_filename = "project_documentation.txt"
    
    try:
        generate_project_documentation(current_dir, output_filename)
        print(f"文档已生成: {os.path.abspath(output_filename)}")
    except Exception as e:
        print(f"生成文档时出错: {str(e)}")

if __name__ == "__main__":
    main()
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\Combiner.py
import os
import sys
from tqdm import tqdm

# 定义允许被合并内容的文件扩展名
ALLOWED_EXTENSIONS_FOR_CONTENT = ('.py')

# --- 恢复 generate_tree 到原始版本，以包含所有文件 ---
def generate_tree(root_dir):
    """生成类似 tree /f 的目录树字符串 (包含所有文件和目录)"""
    tree_output = [root_dir]
    # os.walk 会自然遍历所有子目录和文件
    for root, dirs, files in os.walk(root_dir):
        level = root.replace(root_dir, '').count(os.sep)
        indent = '    ' * level
        if level > 0:  # 避免重复输出根目录
            tree_output.append(f"{indent}{os.path.basename(root)}")
        
        # --- 在目录树中列出该目录下的所有文件 ---
        sub_indent = indent + '    '
        for file in files:
            # 不进行扩展名过滤，列出所有文件
            tree_output.append(f"{sub_indent}{file}")
            
    return "\n".join(tree_output)

def concatenate_files(root_dir, output_file):
    # 计算总文件数以初始化进度条 (反映遍历的所有文件)
    total_files = sum(len(files) for _, _, files in os.walk(root_dir))

    # 预计算将被合并内容的文件数（可选，如果你想让进度条只反映合并进度）
    # included_files_count = 0
    # for root, dirs, files in os.walk(root_dir):
    #     for file in files:
    #         _, ext = os.path.splitext(file)
    #         if ext.lower() in ALLOWED_EXTENSIONS_FOR_CONTENT:
    #             included_files_count += 1
    # print(f"找到 {included_files_count} 个符合条件的文件将被合并内容。")


    with open(output_file, 'w', encoding='utf-8') as outfile:
        # --- 写入完整的目录树 ---
        outfile.write("Full Project Directory Tree:\n") # 标题明确说明是完整树
        outfile.write(generate_tree(root_dir) + "\n") # 调用未经过滤的 generate_tree
        outfile.write("=====\n\n")
        outfile.write("Concatenated File Content (Filtered):\n") # 明确说明内容是过滤后的

        # 使用 tqdm 显示进度条 (total 仍然是所有文件数，反映遍历进度)
        # 如果使用上面的 included_files_count，这里改为 total=included_files_count
        with tqdm(total=total_files, desc="扫描文件并合并内容", unit="file") as pbar:
            for root, dirs, files in os.walk(root_dir):
                for file in files:
                    file_path = os.path.join(root, file)

                    # --- 核心过滤逻辑：只处理指定扩展名的文件内容 ---
                    _, ext = os.path.splitext(file)

                    # 检查扩展名是否在允许合并内容的列表中
                    if ext.lower() in ALLOWED_EXTENSIONS_FOR_CONTENT:
                        # --- 文件内容合并逻辑 ---
                        try:
                            with open(file_path, 'r', encoding='utf-8') as infile:
                                content = infile.read()
                                outfile.write("FILE: " + file_path + "\n")
                                outfile.write(content + "\n")
                                outfile.write("-----\n")
                            print(f"已合并内容: {file_path} - 成功")
                        except Exception as e:
                            # 即使读取失败，也记录下来
                            outfile.write(f"FILE: {file_path}\n")
                            outfile.write(f"读取文件出错 (尝试合并内容时): {str(e)}\n")
                            outfile.write("-----\n")
                            print(f"尝试合并内容失败: {file_path} - 错误: {str(e)}")
                    else:
                        # --- 文件类型不符，跳过内容合并 ---
                        # 不需要写入文件内容，只在控制台打印信息
                        print(f"已跳过内容合并 (类型不符): {file_path}")

                    # --- 更新进度条 ---
                    # 每个访问的文件都更新进度条
                    pbar.update(1)

if __name__ == "__main__":
    root_dir = sys.argv[1] if len(sys.argv) > 1 else os.getcwd()
    output_file = 'project_text.txt'
    print(f"正在扫描目录: {root_dir}")
    print(f"将生成包含所有文件的目录树，并合并后缀为 {', '.join(ALLOWED_EXTENSIONS_FOR_CONTENT)} 的文件内容到: {output_file}")
    concatenate_files(root_dir, output_file)
    print(f"处理完成。输出文件: {output_file}")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\create_initial_user.py
# create_initial_user.py
import sys
import os
import traceback
import logging
import asyncio # 导入 asyncio 用于运行异步代码
from getpass import getpass # 用于安全地获取密码输入

# --- 设置项目路径 (确保能找到 app 包) ---
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
# --------------------------------------------

# --- 导入必要的模块 ---
try:
    # 从 app.schemas 直接导入 UserCreate
    from app.schemas import UserCreate
    # 导入异步数据库组件
    from sqlalchemy.ext.asyncio import AsyncSession
    from app.db.session import AsyncSessionLocal, async_engine
    # 导入异步 CRUD 函数
    from app.crud.user import get_user_by_username, create_user
    # 导入设置 (如果需要)
    from app.core.config import settings
    # 导入数据库 Base 和 User 模型
    from app.db.base_class import Base
    from app.models.user import User
except ImportError as e:
    print(f"导入应用模块时出错: {e}")
    print("请检查以下几点:")
    print("1. 您是否在 'PsychologyAnalysis' 目录下运行此脚本。")
    print("2. 必要的文件（如 app/db/session.py, app/crud/user.py, app/models/user.py, app/schemas/user.py）是否存在。")
    print("3. requirements.txt 中的所有依赖项是否已在您的环境中安装。")
    print("4. CRUD 函数 (get_user_by_username, create_user) 是否已定义为 'async def'。")
    print("5. app/schemas/__init__.py 是否正确导出了 UserCreate。")
    sys.exit(1)
except Exception as e:
     print(f"导入过程中发生意外错误: {e}")
     sys.exit(1)
# ------------------------

# --- 配置日志 ---
APP_LOGGER_NAME = settings.APP_NAME or "QingtingzheApp"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(APP_LOGGER_NAME)
# ----------------

async def create_tables():
    """异步创建数据库表（如果尚不存在）"""
    logger.info("正在异步初始化数据库表...")
    try:
        async with async_engine.begin() as conn:
            # Base.metadata 包含了所有继承自 Base 的模型信息
            await conn.run_sync(Base.metadata.create_all)
        logger.info("数据库表已确认/创建成功。")
    except Exception as e:
        logger.error(f"创建数据库表时出错: {e}", exc_info=True)
        print(f"错误：无法创建数据库表。正在退出。错误: {e}")
        sys.exit(1)

async def create_user_interactively(db: AsyncSession) -> None:
    """交互式地获取用户信息并异步创建初始用户。"""
    logger.info("--- 正在创建初始用户 ---")

    while True:
        username = input("输入用户名: ").strip()
        if not username:
            print("用户名不能为空。")
            continue
        # 调用异步 CRUD 函数获取用户
        existing_user = await get_user_by_username(db, username=username)
        if existing_user:
            print(f"用户名 '{username}' 已存在。请选择其他用户名。")
        else:
            break

    email_str = input(f"为 {username} 输入邮箱 (可选, 直接按 Enter 跳过): ").strip()
    email = email_str if email_str else None
    full_name = input(f"为 {username} 输入全名 (可选, 直接按 Enter 跳过): ").strip() or None

    while True:
        password = getpass("输入密码 (最少6位): ")
        if len(password) < 6:
            print("密码太短 (最少6位)。")
            continue
        password_confirm = getpass("确认密码: ")
        if password != password_confirm:
            print("两次输入的密码不匹配，请重试。")
        else:
            break

    is_superuser_input = input("是否将此用户设为超级用户? (y/N): ").strip().lower()
    is_superuser = True if is_superuser_input == 'y' else False

    # 使用 Pydantic schema 准备用户数据
    user_in = UserCreate( # <-- 直接使用导入的 UserCreate
        username=username,
        email=email,
        full_name=full_name,
        password=password, # 传递明文密码，哈希在 crud.create_user 中完成
        is_superuser=is_superuser,
        is_active=True # 初始用户通常是激活的
    )

    try:
        logger.info(f"尝试创建用户: {user_in.username} (超级用户: {is_superuser})")
        # --- 关键修改：使用正确的关键字参数名 ---
        # created_user = await create_user(db=db, user=user_in) # 旧的，错误的调用
        created_user = await create_user(db=db, user_in=user_in) # <--- 使用 'user_in='
        # -----------------------------------------
        logger.info(f"成功创建用户: '{created_user.username}' (ID: {created_user.id})")
        print(f"\n用户 '{created_user.username}' 创建成功!")
    except ValueError as ve: # 捕获来自 CRUD 的特定错误 (如重复用户)
         logger.error(f"创建用户 '{username}' 失败: {ve}")
         print(f"\n错误：无法创建用户。{ve}")
    except Exception as e:
        logger.error(f"创建用户 '{username}' 时发生意外错误: {e}", exc_info=True)
        print(f"\n错误：发生意外错误: {e}")


async def main():
    """主异步执行函数"""
    await create_tables() # 先确保表存在

    logger.info("正在创建数据库会话...")
    # 正确使用异步会话上下文管理器
    async with AsyncSessionLocal() as session:
        await create_user_interactively(session)

    logger.info("脚本执行完毕。")


if __name__ == "__main__":
    print("开始执行用户创建脚本...")
    try:
        # 运行主异步函数
        asyncio.run(main())
    except KeyboardInterrupt:
         print("\n用户中断了脚本执行。")
    except Exception as e:
         print(f"\n在顶层发生意外错误: {e}")
         traceback.print_exc() # 打印完整的错误堆栈
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\find _token.py
import dashscope
from dashscope import Account

# 配置您的API Key
dashscope.api_key = 'sk-0ffc64dd9d614c0088ee28e3a072420a'

# 查询账户信息
account = Account()
balance = account.get_balance()
print(balance)
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\main.py
# 文件路径: main.py
import os
import yaml
import sqlite3
import json
from concurrent.futures import ThreadPoolExecutor, as_completed  # 新增导入
from src.image_processor import ImageProcessor
from src.report_generator import ReportGenerator
from src.data_handler import DataHandler
from src.utils import setup_logging

def process_single_task(image_path, image_processor, report_generator, data_handler, logger):
    """处理单个图片和量表数据的分析任务"""
    logger.info(f"开始处理图片: {image_path}")
    try:
        # 处理图片
        description = image_processor.process_image(image_path)
        desc_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 
                               f"output/descriptions/{os.path.splitext(os.path.basename(image_path))[0]}_desc.txt")
        os.makedirs(os.path.dirname(desc_file), exist_ok=True)
        with open(desc_file, "w", encoding='utf-8') as f:
            f.write(description)
        logger.info(f"图片描述已保存至: {desc_file}")

        # 加载数据
        subject_info, questionnaire_type, questionnaire_data = data_handler.load_data_by_image(image_path)
        if not subject_info:
            logger.warning(f"图片 {image_path} 无对应量表数据，仅保存描述")
            return

        # 计算得分并生成报告
        score = calculate_score(questionnaire_type, questionnaire_data)
        report = report_generator.generate_report(description, questionnaire_data, subject_info, questionnaire_type, score)
        report_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 
                                 f"output/reports/{os.path.splitext(os.path.basename(image_path))[0]}_report.txt")
        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        with open(report_file, "w", encoding='utf-8') as f:
            f.write(report)
        logger.info(f"报告已保存至: {report_file}")
    except Exception as e:
        logger.error(f"处理 {image_path} 时出错: {str(e)}")

def main(image_paths=None, max_workers=4):
    logger = setup_logging()
    logger.info("心理学图像和数据分析项目启动（并发模式）")

    project_root = os.path.dirname(os.path.abspath(__file__))
    os.chdir(project_root)
    config_path = os.path.join(project_root, "config/config.yaml")
    with open(config_path, "r", encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # 初始化处理器
    image_processor = ImageProcessor(config)
    report_generator = ReportGenerator(config)
    data_handler = DataHandler(db_path=os.path.join(project_root, "psychology_analysis.db"))

    # 如果没有指定图片路径，处理 input/images 目录下的所有图片
    if not image_paths:
        image_dir = os.path.join(project_root, "input/images")
        image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        logger.info(f"未指定图片路径，将处理目录 {image_dir} 中的所有图片: {len(image_paths)} 张")

    if not image_paths:
        logger.warning("没有找到任何图片需要处理")
        return

    # 使用线程池并发处理
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # 提交所有任务
        future_to_image = {executor.submit(process_single_task, image_path, image_processor, report_generator, data_handler, logger): image_path 
                         for image_path in image_paths}
        
        # 处理完成的任务
        for future in as_completed(future_to_image):
            image_path = future_to_image[future]
            try:
                future.result()  # 获取结果，如果有异常会抛出
                logger.info(f"任务完成: {image_path}")
            except Exception as e:
                logger.error(f"任务 {image_path} 执行失败: {str(e)}")

def calculate_score(questionnaire_type, questionnaire_data):
    """根据量表类型计算总分"""
    scores = [int(value) for value in questionnaire_data.values()]
    total_score = sum(scores)
    return total_score

if __name__ == "__main__":
    # 示例：可以传入特定图片路径列表
    specific_images = [
        os.path.join(os.path.dirname(os.path.abspath(__file__)), "input/images/TestPic.jpg"),
        os.path.join(os.path.dirname(os.path.abspath(__file__)), "input/images/testpic1.jpg")
    ]
    main(image_paths=specific_images, max_workers=4)  # 设置最大线程数为4
    # 或不指定图片路径，处理所有图片
    # main(max_workers=4)
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\run_celery_worker.py
# run_celery_worker.py
import os
import sys

# --- 1. 定位项目根目录 ---
# __file__ 指向这个脚本文件 (run_celery_worker.py)
# os.path.dirname(__file__) 指向项目根目录 (PsychologyAnalysis/)
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
print(f"[Worker Start Script] Project Root detected: {PROJECT_ROOT}")

# --- 2. 将项目根目录添加到 sys.path ---
# 这样 Celery 启动时就能找到 app 和 src 包
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
    print(f"[Worker Start Script] Added {PROJECT_ROOT} to sys.path")
else:
    print(f"[Worker Start Script] {PROJECT_ROOT} already in sys.path")

# --- 3. 导入 Celery App 实例 ---
# 现在可以安全地导入了
try:
    from app.core.celery_app import celery_app
    print("[Worker Start Script] Successfully imported celery_app")
except ImportError as e:
    print(f"[Worker Start Script] CRITICAL ERROR: Could not import celery_app: {e}")
    print("Please ensure app/core/celery_app.py exists and dependencies are installed.")
    sys.exit(1)
except Exception as e:
     print(f"[Worker Start Script] CRITICAL ERROR during celery_app import: {e}")
     sys.exit(1)


# --- 4. (可选) 加载 .env 文件 (如果 Celery 任务需要直接访问环境变量) ---
# Celery worker 默认不一定加载 .env。如果你的任务代码 (如 ai_utils)
# 需要读取 .env 中的变量 (除了 Pydantic Settings 已经加载的)，
# 在这里加载它。
# from dotenv import load_dotenv
# dotenv_path = os.path.join(PROJECT_ROOT, '.env')
# if os.path.exists(dotenv_path):
#     load_dotenv(dotenv_path=dotenv_path)
#     print(f"[Worker Start Script] Loaded environment variables from: {dotenv_path}")
# else:
#     print("[Worker Start Script] .env file not found, skipping dotenv load.")


# --- 5. 准备 Celery Worker 的命令行参数 ---
# 你可以在这里定义参数，或者从命令行读取
worker_args = [
    'worker',             # 命令
    '--loglevel=info',    # 日志级别
    # '-P', 'solo',       # 在 Windows 上需要添加这个参数
    # '-c', '4',          # (可选) 并发数 (Linux/macOS)
    # '--pool=prefork',   # (可选) 进程池类型 (Linux/macOS 默认)
]

# --- 针对 Windows 添加 solo 进程池 ---
if sys.platform == "win32":
     if '-P' not in worker_args and '--pool' not in worker_args:
         print("[Worker Start Script] Windows detected, adding '-P solo' argument.")
         worker_args.extend(['-P', 'solo'])

# --- 6. 执行 Celery Worker 命令 ---
print(f"[Worker Start Script] Starting Celery worker with args: {worker_args}")
# 使用 celery_app.worker_main 来启动 worker，它会处理命令行参数
celery_app.worker_main(argv=worker_args)
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\COMMIT_EDITMSG
update:

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\config
[core]
	repositoryformatversion = 0
	filemode = false
	bare = false
	logallrefupdates = true
	symlinks = false
	ignorecase = true
[remote "origin"]
	url = git@github.com:panda-like-bamboo/PsychologyAnalysis.git
	fetch = +refs/heads/*:refs/remotes/origin/*
[branch "main"]
	remote = origin
	merge = refs/heads/main
[branch "feature/add-SQLite"]
	vscode-merge-base = origin/main
	remote = origin
	merge = refs/heads/feature/add-SQLite

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\description
Unnamed repository; edit this file 'description' to name the repository.

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\HEAD
ref: refs/heads/feature/add-SQLite

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\index
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa3 in position 11: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\info\exclude
# git ls-files --others --exclude-from=.git/info/exclude
# Lines that start with '#' are comments.
# For a project mostly in C, the following would be a good set of
# exclude patterns (uncomment them if you want to use them):
# *.[oa]
# *~

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\logs\HEAD
0000000000000000000000000000000000000000 c2a04f00f272a7328de06acb0e4c4331cceff1fd panda-like-bamboo <dier334824851@163.com> 1742809358 +0800	commit (initial): Initial commit
c2a04f00f272a7328de06acb0e4c4331cceff1fd c2a04f00f272a7328de06acb0e4c4331cceff1fd panda-like-bamboo <dier334824851@163.com> 1742811839 +0800	checkout: moving from main to feature/add-SQLite
c2a04f00f272a7328de06acb0e4c4331cceff1fd 82f741be7012214e29cb1b7e452aa701fd37c75d panda-like-bamboo <dier334824851@163.com> 1742832601 +0800	commit: 本机html可交互demo
82f741be7012214e29cb1b7e452aa701fd37c75d 04eb1421784606bd918c72ce21ee7f0278bebf15 panda-like-bamboo <dier334824851@163.com> 1745515378 +0800	commit: update:

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\logs\refs\heads\main
0000000000000000000000000000000000000000 c2a04f00f272a7328de06acb0e4c4331cceff1fd panda-like-bamboo <dier334824851@163.com> 1742809358 +0800	commit (initial): Initial commit

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\logs\refs\heads\feature\add-SQLite
0000000000000000000000000000000000000000 c2a04f00f272a7328de06acb0e4c4331cceff1fd panda-like-bamboo <dier334824851@163.com> 1742811839 +0800	branch: Created from HEAD
c2a04f00f272a7328de06acb0e4c4331cceff1fd 82f741be7012214e29cb1b7e452aa701fd37c75d panda-like-bamboo <dier334824851@163.com> 1742832601 +0800	commit: 本机html可交互demo
82f741be7012214e29cb1b7e452aa701fd37c75d 04eb1421784606bd918c72ce21ee7f0278bebf15 panda-like-bamboo <dier334824851@163.com> 1745515378 +0800	commit: update:

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\logs\refs\remotes\origin\main
0000000000000000000000000000000000000000 c2a04f00f272a7328de06acb0e4c4331cceff1fd panda-like-bamboo <dier334824851@163.com> 1742810995 +0800	update by push

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\logs\refs\remotes\origin\feature\add-SQLite
0000000000000000000000000000000000000000 04eb1421784606bd918c72ce21ee7f0278bebf15 panda-like-bamboo <dier334824851@163.com> 1745515408 +0800	update by push

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\00\1ad4bb4a59cbdb0ac1ddf134f9503ee4eb941d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 23: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\00\f1f9a338f4ecda0461e2b1e919d3679f88b02c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd0 in position 20: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\01\b512c1b975ae8ed48bec78d2f2a797aee04175
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbd in position 4: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\03\5c221979f77795910253f051d9d03dff9c4085
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcc in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\03\a64fae01884fb7377a512d0118eac53cfe9345
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\04\eb1421784606bd918c72ce21ee7f0278bebf15
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\0c\dec50e08b0ab11ba7d53f7b989a6c614c31abb
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\0d\2b14ced5984d5fcbd7e0c3d4b8824f31c1fa86
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 16: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\0d\e0b6d0d203267db04218b4aa21ed5542ab10d7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 18: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\0f\8f77aaad74eba52509e01efc00b4bd4f2db5a0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\0f\be2c23a32171ca42eed24e8b48eb6ae70e50bc
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8f in position 5: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\10\59cc4ab69a96f51712a550fe291d6dd4fdb2b7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\12\170485ae2e5af27f6a5612b1af82aa1aca1d16
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbd in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\13\14ce251df032572faa5b4b1c54741b762c2859
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdb in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\13\50414ac811c9720f85d6c3236bbd79d7d0a0af
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\14\755b8a01c4534431afaa63e9a9e69855fe9693
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd0 in position 20: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\15\530d58381cfaa592f432d06d0e3b4d75979fa1
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\16\428857e8476be65ef739ea56109654e7d44ed0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 14: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\17\9554ed9b3425e2cedc13865bd9a73f5cbe290a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\18\0028419bd3d3c73905a966756a8c23bd13192b
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\19\0edb5d839e6ba4394ea560eb1aacf1e0ff2ba7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\19\9a7229ad454705cff2987ca2c9aa8844d6e672
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1a\33b1dc7c13e7687022f838a0701e0beecc73ec
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbb in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1a\62c857ba278139810ddc9efba3815ce8a475ac
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1a\6f5248e38415868d72b959451bca579a80cc0a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1b\19c678ab390732d2693b1bc7ddd31c48e9986c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 20: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1b\a8056d26b4e150beab2d9c4cb520985febb136
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1c\17b125e8414f73946d0502609d81a080166e1e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdb in position 6: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1c\21834c10f9c325ff3f5f8afaf7ac14706952be
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb1 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1c\f1118dba58a12d8abdf85a1264f9302dc989ce
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x92 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1e\ae7d3c4e6aa8867a3921270d822ec7492a23d7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb6 in position 20: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1f\11b9b2ac4c648475275a6821b3e2de1a44bd37
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd0 in position 20: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\20\336ee91d617c0235f9d18d07e6a6c9f47aafad
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\22\59b8381631ec194eeadb805f64296cfadb01fc
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 14: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\23\1c188fbd04c8f681e6fa84ca53015c009785a7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\25\54701d69ac6a2ca0ad95cff5cfdc419216e794
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9c in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\25\57f71c9ab0fca990b555b577551c4d8388a2b8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd0 in position 19: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\26\27167eeb025b72af9103bbf8120620c63f1bb5
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\26\ed667e9c00a416625d8a9f03dd8f6b26be218d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb1 in position 4: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\27\c2f6dc9369ae95547ad0780469b687901bb84f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\2b\8466c8c4b97b5b581473c6ec56ebf403bb0b18
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\2b\8d7c06000aedc326c3645fc1dda5d8acc245b9
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\2d\0cae5aa5df3e38bbc4c530a43b2e5eaf23f3b3
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\2f\09c0b9da126a06729c3e25128161354403c93f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\34\06ce50c42fef949edef25cdf5ac8becec27af3
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\34\120dd588c96631e266483790d2cd3f883f66f4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\34\c2e7bdb2747a29de75088cac02a29c10dc3bbc
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\35\13a5221a131ea18fa6e04ec98ab8e306d6757c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\35\5f2230940baa069ffb5627a42923445941d06f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\36\2dfbf805088266f80261136a0d3e8803898796
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\38\267654cd80ba5e7e549b8385688d0dc8ef81ff
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb1 in position 28: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\39\584dae458b91ca9abdca53f9c8cc5a484b97c0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdf in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\39\c637c889413858d81833e91447fbd86a105c31
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\39\ddaa0a1662a16d4bed68b5f00eb789376af67c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb6 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3a\28b5c3dcc80e82689df7622b9d73041a737b36
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3a\882a9b62d80bf5ca77eccd47a0da644b092347
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3c\6c57c7a5a0bf6998193136db26f5459bb9171b
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdb in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3c\a15b11440a25f49e320de5f5f0bc04e81c7cc6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcb in position 19: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3d\18e559d61ec6bc9fec6f549a7890a43aedf304
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3d\e427da87f0a7d2cdb8af4ccb16cf7f73d4c401
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3e\4dafd7691ec3dd1bde9bd59fcdf28f4098af60
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3e\e972b88be666ad384fcdb25249c2cdd74260d6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 22: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\41\9845770072fcc3e254b3a41f080426265f0eda
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdb in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\41\9f3b0d4b1483339db94525c683a6663afac1e1
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\42\0298caf723214e4ede35e42ad7d56bbf7be4b8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb0 in position 7: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\42\415ba3df9187024b32f5667ecced22a4a51f32
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\42\595efb99cc2f55484cfda26bcec2cdf02219f9
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\43\deef336344327856d1d860ffb3407000840513
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\44\a22b5940ed22e2a04d6093dd30ed791c441592
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\45\a1421ecb1fb92f95faca87c8748c2648a58afd
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9c in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\45\ea234cb91b3420333e1752b0c8f14dc4e08c9c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\46\a29fd2a2ef9d52e6ec457931cb5e874d24bd8a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\48\0b130d632ca677c11f23d9fe82cf4014d15e0c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\48\b8f448bd5c8cace7fa482e9281b1f836bdc640
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\4c\db36f62668e963b26a42e3030fcc299fe3f55a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\4d\d738d6463eac7b454eeb9fa96bcf1e2f486a4f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbf in position 11: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\4f\994894eb2c7d18d16bfe83c88d60b0740d8802
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\4f\edcf43b25bd44e4e0fbb06954ba5cc9451c22a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdf in position 9: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\52\d7c1a18ef930c1c1c98bf248a2e842d15379ff
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\53\07e0809921b46189f314499d02be4faa6772e7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\53\26d681a358ad0f30e0e6137763b25ed3813393
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\56\028992b3e0f68e084b289347cfdb3e9453e9dd
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\57\bd8279b9e982261ccff432113f3fae4f0ac51c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 15: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\58\9c9062493fa10a00760ab9a98485bacf0bcfb6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\5e\eae694ae1c7c16bc977e84ccd415c2113efea2
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc9 in position 7: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\5f\2411d567b115a13565f5354f5e9a162f24a345
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x90 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\61\29feb5c75b4413880e15b4e76e31dee7b45f89
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xe3 in position 21: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\61\efc86772780c5190adcbc73eb739b3d292bfa7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcb in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\62\907838025ec3e0009b09ea0c383a67b3ed6680
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 14: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\65\9d2f11fde57f3cc3c2a8d5d60f8fe5479b92ff
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\67\021574883946a5e8a244453fa4869d2547f19a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdb in position 6: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\67\39dd22283de3c81abc1959398b87adb215b5b3
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 9-10: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\6d\865406aaaea9879f077b737ac203306abb9e8f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 14: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\6d\ea44ab256461f55f054b63f1e82379251af10e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\6e\0865849ffaffe3c16a97d8df0428a813ef43c1
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\6e\e1fa34f83ee5d6685c30b0684793c0d406aab0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\72\308b59813d15637bdb4462ee2e5705b354bb22
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\72\32676ad66a4d556998272eebb32fb4568d502e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\72\63f477640e2b25706b08db2b0ab34e45fc6b38
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\72\dfd07de33c644e5be7185e0111ec4727cf9467
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 18: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\74\76edb315d4a8bb670a333a8a0f102ce2105405
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\76\50be1332e30e87a4d56178ef6db340065aff2f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\76\696ae0f5c0350e25cc62f892b8c8e03a61ce66
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\76\a61f3fd9dc64dd502566996a7c6f9cbda4e122
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 23: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\77\907e45232406ba49a0de1019cd672740518b52
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\79\3ab6a06621a5827dabf94dc303366b409e7d78
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\79\5aeef71f00d4c48a4315bf8eced76dc2d3a56f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\7a\205d911dae4499a5957c11f70426eceec86139
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\7c\72e7dbefae2a2dd9f5b26bab1a194f0ae0700f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\7d\0d7c2b67e6a4b5ab3c01f5417c717d1f51e52a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb6 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\7d\7d6bab7cd95ea4af27fd1781fd4e57a4e7bdd7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\7d\bcf05e56e4a378de1f43bf544fc13fc05a1ad5
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\7f\92744e31f87d08096844b04acbf5fb45c62efc
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\80\575028454e49098fcac7da363c615ab6838d00
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\82\f741be7012214e29cb1b7e452aa701fd37c75d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\83\5c1af93b81cc26e7c3d9403fc96f1c60de7515
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\84\2798011a3f194239dda371cd92b3b606418abb
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbd in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\86\d2e82f7f33c7ce049891c2387e47d0c282c3b0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 22: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\86\eecf0e40e7e01e022163d14e7f557482c6a1d6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\89\ccd8534dbf253b68b8ab392bed537085641801
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\8a\69f975adb32451e3c1090b62857d1e27cd3b04
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\8a\f0f895f24c72be1493fa46fa26ba80ed4bd0c1
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\8b\d66ba2773e5362cb3d2578eff8dba1cc8a0354
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\8c\d0beec64208d1801b02eb3d5732570f716708e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\8d\94250928a83dbb62e59823cbf273ec14178f22
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd3 in position 6: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\8e\66a6c876b707b5559d907d27f61365d24e1f85
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\91\9f90966c62717988fe0c3aacb1a900f1e72cb4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\94\3f49c6131b8b6d0d063ca6bd7387e62b21de53
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\95\0d92e48c3164e57dc7c01df4e44a6c1c0bc173
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x92 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\97\d61d380edd68fe5e76eede90e99cc14e1d857b
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\9c\2e8b4dc8699a85bf6e105b5652418429bba17d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb7 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\9c\35ff5a2c8019394506ae45a311b2ad37174ab0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\9e\eecf53488c2bf7144ceb9f6e123fda2a27e7f2
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\9f\acd9e873402569445133d0b91ac3665a77c761
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\9f\dc040050624556464ffa5112dde397ccd792c6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x94 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a1\9693d4b6c943939f6b36383e9ceaa6491bbd1d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a1\e7d58650dedca81c92e10a43c6ee9c9f484420
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a2\7a13e496942e077a2e4392e79a14be597fd0e6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a2\7ebc9f9b4dfe82cf9cfea0ab6b549aa9c04c6a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x91 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a3\36e5fe3ad8f45caa601a8c853601ec4c8f0ad4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a5\8d010bd1849ea7514bb1d0751c29ca3ba6f2f7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a5\fb821f3a960767b8861c06a9879fb2535ce7f3
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb1 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a6\3975dc55b149a1531c06adcb3866fad511d1da
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb6 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a6\e468cc369ffaf54bc3600c257e772165232b49
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a7\756130bb1fd01db91b8be0a7bd844faccebe79
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a7\afc0621c9b834d8f139495b113ad430ebdd065
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 14: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a8\210548c126f9a57e25b9fa7bdc3d15d88a152e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\aa\9674bff49341d7c6b0a6ab8ba34e8ec3372c68
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ab\7098f5e7d3e65d1dc78736269630ae8873a82b
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcd in position 18: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ac\197e9723b0c8d0c03b3e5698343bdcc78fdc93
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ac\77806aed7ad5dc8f60c6c66e8b709b437766ab
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ac\9f9be8794d6e4f9f54185a0942bd88d846ea1f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ad\29b33bf959eed8dce123de959454f89e26c2c0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xde in position 9: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ad\99bba2735d03e995d5a854935a67d871214c16
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ae\0a0841941a896a8f3bc32244cfef832aab8225
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\af\1fc84185401155dab36f87f271cb2cb924a958
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b0\f1d69a4cdfb7075c9fff3c750c173dee824d35
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b1\725e7c3b8f783170d0479f18e67c416ad4e633
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b2\3a63d157cf47e7778eca8cc2904d52be305b5d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b3\078c8228687749f4723fd9fa35f9f64dbbc242
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 23: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b3\e4b3d8822d9208dfb2b5b5dddf512c4b80c844
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b3\f232bdd903e435a62026e1f27119f15961782c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b4\0bdefdc1827e0dbcfe065f9350ee23574af4ab
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xf5 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b7\44a021db5e8dee616f2a2b299181d4abf5c556
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b7\752a996e380022274105861387fd7e8bf0ea02
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbb in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b8\5d4173ea3a9dcf2599eb5d41406a0cded0eb40
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ba\68d800e780b486a3488fab3acba5ffb7a5131d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\bb\3865865a1f62bf3f323712d38f3835c249f185
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\be\19ca58bb5ada17902ef40ae0e7907a44b95ebf
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\be\ae79856ebed36ffcfc2e640570278beb01b8d8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\be\cf22bf1b11f7e257acb2b1de382a2c40e3e3a6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\bf\bd8e76e775d579271d141a026c5e3e7abc3cd4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\bf\c3170b74f820792ed9bdc1195795662271f2ef
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb6 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\c1\d21363b7bcc1ac97a45405a4c5c8ec3f789d6e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\c2\906e28dec7b8d1539d30f6cc1dd6cf11cfff08
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\c2\a04f00f272a7328de06acb0e4c4331cceff1fd
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\c3\3afe698b93d7de76711ff8cd706821aa722007
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc3 in position 6: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\c3\baeee8d9b6021c48fcdf93b3068c20b7ba0b74
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcd in position 9: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\c8\ad7372a7def273034b6a078fef7ca207f3b6ad
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb2 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\cf\aae9373284c74755573f8ee0cfe7d57812d7a9
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb2 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\cf\c9a6092bfd0b3e43af5256f837a6ab3b43f990
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb0 in position 7: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d0\830085ecfecaf0a57fcbecea0ba419c9502d37
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d1\5b66c07e8827fdd0ff4354331ccaa2a00e7de2
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdf in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d1\6e97d7914485c17e0395f580442377db5a6658
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xef in position 9: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d4\1acbc388a89af3183b27562f3ae301a51ede7c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d4\369ec968473b131b989bd32ed2431e61002096
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb7 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d4\4f8353405ccca528e4d91f00dd71825cf5ef37
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d5\8de88f57ddaa8d332ac4d3b4bcd776dbd6572a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdb in position 6: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d5\99a575e43c7c83e9fe8dadf5982b4514f1a9b3
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d5\e583ccc795e00605e7c75a4eba3162fca04b6a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d6\c8f4d9982919fbcc4daf2c64056af136a9b68a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcd in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d6\e9212fe9ba23ffa73805da67ca595142ba353a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d7\3c2aa346c4756a3785227ab1fa619dc44e90d9
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d7\6d7d738d3cace730bd2cbcfbaa64dbc3c08941
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x92 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d8\d1ed3944b547bbe2511caa5f1b4bfc1a347766
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d9\2ec90575ca8351eef4c3c7b7eca228689d0f01
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\da\fba3c24607c591762386d2af6047361a2e041f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\db\86821b7ebe4cc027e9176c2fa8ccf89d90c207
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb0 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\dc\b57f8e7615e87a5d2d391db65f9c50052ab8f8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb4 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\de\8f2375bab107f80db8c9855adc53638dfa95b8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e2\612971942d9216aa098999000dbf0599ea74bb
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e2\de9eb013671744299ebd688689ba41db573009
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb7 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e4\23a6d30dd2a4e9a9235da54e88e8af30e90934
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e4\fce11e6093eba1cad00b3d2eb227a99936e0b7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8c in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e5\0711076ac8770cd864d52f8c9e402c761083be
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e5\25e4aa93a233985efa5c6980965c982da2ebb1
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e5\9a65a16d5731fa0068fbdb27c0aa8b23e77f1c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e6\9de29bb2d1d6434b8b29ae775ad8c2e48c5391
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e7\c459bebff100d6a85bd8fd47478eefd6518774
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e9\03f469138532261bea6eb9e185fe490403534c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc9 in position 22: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e9\a63bebc59f897bfcd9dba47ab7d62c44dd4314
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ea\a6379d7ff568b3928386891be87431e5ff37c2
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xf1 in position 22: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\eb\0bd551fbd0728313d2506a006a98dcf3ce2d5d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ed\241cfe9cef4bfbdedee9213a46728ed3f5cf9b
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb4 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ee\d71643e8aed4707ef95898270508c15ecb108f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x94 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ef\a77066fea8765db4e8fd7c7cb90f4552e77afb
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f0\2431dcbc87e191f13e71aaa2a241a1aaa9a5d4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd3 in position 6: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f0\c50edbf917d522043159af34a2e6d444df180e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f1\e585438b9f9ea720321a283870ca34e9558499
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xda in position 10: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f2\1b2295f71004f83504b9b53baa11fcb390cdf6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 22: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f3\ba9de61264fb775bc62fb9fa6a086230c79af4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f4\37427c723f2716bb1e6d54dde5bd532bd632f9
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbd in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f4\4e9bbc4bffc0a5955d72b5ec195677dfc8d7ad
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f5\3cc46fccfb11d32b826605287372f10bcb1e77
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc9 in position 23: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f6\df3351505ab09ea62622a396aa7cb59eeeeff1
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f7\0ab3ca71f74ca3687a641251a8624ce3586cdb
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f9\867422c063e9ea439b567ef2afb06f4eb0b434
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fa\9820ad9c9fc2e4c97af8c2e2353588587613e2
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 14: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fb\18f9b4ddd9a0db174e5d0a3d453e2344690ff8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fb\2eb0c874fd0f9196e363705ea8fcdb30117d81
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fc\57024cbae6ddcf9674749fb76b7cc17bad70c8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xf1 in position 22: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fd\b19d3a12e64485de2dbf1f567a2b402b7c3316
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd0 in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fe\79e7225bcbf1077fff19be1e6048abe838ca9b
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fe\b0a73567855a2ea3e16e2870479f7416dbacf6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ff\212f6d63b551ff9b9254bb89d0e0641d7afd3a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 17: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ff\4bc98b7cec201a37c9e5c7f983594e49cba2b4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ff\e3e680c86868fe4470fe6d7ccd27be6a1b0c44
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x93 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\refs\heads\main
c2a04f00f272a7328de06acb0e4c4331cceff1fd

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\refs\heads\feature\add-SQLite
04eb1421784606bd918c72ce21ee7f0278bebf15

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\refs\remotes\origin\main
c2a04f00f272a7328de06acb0e4c4331cceff1fd

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\refs\remotes\origin\feature\add-SQLite
04eb1421784606bd918c72ce21ee7f0278bebf15

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\alembic\env.py
# FILE: alembic/env.py (Complete: Imports settings, Base, Models, and handles sync URL)
import os
import sys
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

# +++ 添加这部分来设置路径和导入你的应用模块 +++
# 1. 计算项目根目录 (PROJECT_ROOT)
#    确保你的项目结构是 PsychologyAnalysis/alembic/env.py
try:
    # This assumes the env.py file is in PsychologyAnalysis/alembic/
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
except NameError: # Fallback if __file__ is not defined
    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '.'))
    print(f"Warning: __file__ not defined, assuming project root is CWD: {PROJECT_ROOT}")

# 2. 将项目根目录添加到 sys.path，这样 Python 就能找到 'app' 包
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
    print(f"[Alembic env.py] Added project root to sys.path: {PROJECT_ROOT}")
else:
    print(f"[Alembic env.py] Project root already in sys.path: {PROJECT_ROOT}")


# 3. 从你的应用导入 settings, Base, 和所有模型
try:
    # 导入配置
    from app.core.config import settings
    # 导入 SQLAlchemy Base
    from app.db.base_class import Base
    # +++ 显式导入所有需要被 Alembic 管理的模型 +++
    from app.models.user import User           # 导入 User 模型
    from app.models.assessment import Assessment # 导入 Assessment 模型
    # 如果还有其他模型，也在这里导入:
    # from app.models.another_model import AnotherModel
    print("[Alembic env.py] Successfully imported settings, Base, and Models (User, Assessment).")
except ImportError as e:
    print(f"[Alembic env.py] Error importing app modules: {e}")
    print(f"Please ensure PROJECT_ROOT ({PROJECT_ROOT}) is correct and contains the 'app' package "
          f"with core.config, db.base_class, models.user, and models.assessment modules.")
    sys.exit(1)
except Exception as e_import:
     print(f"[Alembic env.py] An unexpected error occurred during import: {e_import}")
     sys.exit(1)
# --- 添加结束 ---


# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    try:
        fileConfig(config.config_file_name)
        print(f"[Alembic env.py] Logging configured from: {config.config_file_name}")
    except Exception as e_log:
        # Avoid crashing if logging config fails, just warn
        print(f"[Alembic env.py] Warning: Could not configure logging from {config.config_file_name}: {e_log}")


# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata

# +++ 设置 Alembic 需要知道的模型元数据 (现在包含了导入的所有模型) +++
print(f"[Alembic env.py] Setting target_metadata from {Base.__module__}.Base")
target_metadata = Base.metadata
# --- 设置结束 ---

try:
    known_tables = list(target_metadata.tables.keys())
    print(f"[Alembic env.py] DEBUG: Tables registered in Base.metadata before comparison: {known_tables}")
except Exception as e_debug:
    print(f"[Alembic env.py] DEBUG: Error getting table names from metadata: {e_debug}")
# +++ 调试代码结束 +++

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def get_sync_database_url() -> str:
    """Gets the database URL from settings and ensures it's sync for Alembic."""
    url = settings.DATABASE_URL
    # Replace async sqlite driver with sync one
    if url and url.startswith("sqlite+aiosqlite"):
        sync_url = url.replace("sqlite+aiosqlite", "sqlite", 1)
        # print(f"[Alembic env.py] Converted DB URL for Alembic: {sync_url}") # Reduced verbosity
        return sync_url
    # Add similar replacements for other async drivers if needed
    # elif url and url.startswith("postgresql+asyncpg"):
    #     return url.replace("postgresql+asyncpg", "postgresql", 1)
    # print(f"[Alembic env.py] Using DB URL as is for Alembic: {url}") # Reduced verbosity
    return url # Return original if no known async driver found


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode."""
    sync_url = get_sync_database_url() # Get the potentially modified sync URL
    print(f"[Alembic env.py] Configuring offline mode with URL: {sync_url}")

    context.configure(
        url=sync_url,
        target_metadata=target_metadata,
        literal_binds=True, # Recommended for script generation
        dialect_opts={"paramstyle": "named"},
        compare_type=True, # Enable type comparison
        compare_server_default=True, # Enable server default comparison
        # render_as_batch=True # Uncomment if needed for SQLite ALTER limitations
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""
    # Get the sync URL
    sync_url = get_sync_database_url()
    # Set the sync URL in the Alembic config object, overriding alembic.ini
    config.set_main_option("sqlalchemy.url", sync_url)
    print(f"[Alembic env.py] Set online mode sqlalchemy.url to: {sync_url}")

    try:
        # Create engine using the sync URL from the config
        connectable = engine_from_config(
            config.get_section(config.config_ini_section, {}),
            prefix="sqlalchemy.",
            poolclass=pool.NullPool, # Use NullPool for migrations
        )
        # print("[Alembic env.py] SYNC Engine created successfully for online mode.") # Reduced verbosity
    except Exception as e_engine:
         print(f"[Alembic env.py] Error creating SYNC engine from config: {e_engine}")
         sys.exit(1)

    # Connect using the synchronous engine
    with connectable.connect() as connection:
        # print("[Alembic env.py] SYNC Connection established for online mode.") # Reduced verbosity
        # Configure the context with the connection and metadata
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
            compare_server_default=True,
            # include_schemas=True, # Uncomment if using PG schemas
            # render_as_batch=True # Often needed for SQLite limitations with ALTER commands
        )
        # print("[Alembic env.py] Context configured for online mode.") # Reduced verbosity

        # Run migrations within a transaction
        try:
            print("[Alembic env.py] Beginning transaction and running migrations...")
            with context.begin_transaction():
                context.run_migrations()
            print("[Alembic env.py] Migrations ran successfully within transaction.")
        except Exception as e_migrate:
             print(f"[Alembic env.py] Error running migrations: {e_migrate}")
             # Consider re-raising if you want the command to fail explicitly
             # raise e_migrate


# --- Determine mode and run ---
if context.is_offline_mode():
    print("[Alembic env.py] Running migrations in offline mode.")
    run_migrations_offline()
else:
    print("[Alembic env.py] Running migrations in online mode.")
    run_migrations_online()

print("[Alembic env.py] Script execution finished.")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\alembic\README
Generic single-database configuration.

1.
alembic revision --autogenerate -m "Create users table and add submitter_id fk"
2.
alembic upgrade head
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\alembic\versions\72143d5034d8_create_users_table.py
"""Create users table

Revision ID: 72143d5034d8
Revises: 
Create Date: 2025-04-24 15:06:06.038114

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '72143d5034d8'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    pass


def downgrade() -> None:
    """Downgrade schema."""
    pass

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\main.py
# FILE: app/main.py (修改后，包含百科路由)
import sys
import os
import logging
import traceback

# --- 添加项目根目录到 sys.path ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
    print(f"[Main App] 已添加 {PROJECT_ROOT} 到 sys.path")
# ------------------------------------

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

# --- 导入配置、解析的 origins 和日志设置 ---
try:
    from app.core.config import settings, parsed_cors_origins
    from src.utils import setup_logging
except ImportError as e:
    print(f"[Main App] 严重错误: 无法导入核心配置或日志设置: {e}")
    traceback.print_exc()
    sys.exit(1)
except Exception as e:
    print(f"[Main App] 严重错误: 在初始导入/配置处理期间发生错误: {e}")
    traceback.print_exc()
    sys.exit(1)
# ---------------------------------------------------------

# --- 设置集中式日志 ---
APP_LOGGER_NAME = settings.APP_NAME or "QingtingzheApp"
try:
    setup_logging(log_level_str=settings.LOG_LEVEL,
                  log_dir_name=os.path.basename(settings.LOGS_DIR),
                  logger_name=APP_LOGGER_NAME)
    logger = logging.getLogger(APP_LOGGER_NAME)
    logger.info("--- 启动 FastAPI 应用 ---")
    logger.info(f"日志记录器 '{APP_LOGGER_NAME}' 配置成功。")
    logger.info(f"应用名称: {settings.APP_NAME}, 环境: {settings.ENVIRONMENT}")
except Exception as e:
    print(f"[Main App] 严重错误: 无法设置日志: {e}")
    traceback.print_exc()
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(APP_LOGGER_NAME)
    logger.error("日志设置失败，使用基础配置。")
# ---------------------------------

# --- 导入 API 路由 ---
try:
    # ******** 修改：添加 encyclopedia 路由导入 ********
    from app.routers import scales, assessments, reports, auth, encyclopedia
    # 如果旧的 tips 路由文件仍然存在，可以不导入它或注释掉：
    # from app.routers import tips
    # **************************************************
    logger.info("成功导入 API 路由。")
except ImportError as e:
    logger.critical(f"严重错误: 无法导入路由: {e}", exc_info=True)
    sys.exit(1)
except Exception as e:
    logger.critical(f"严重错误: 在路由导入期间发生错误: {e}", exc_info=True)
    sys.exit(1)
# -------------------------

# --- 创建 FastAPI 应用实例 ---
app = FastAPI(
    title=settings.APP_NAME,
    description="倾听者 AI 智能警务分析评估应用系统 API",
    version="0.3.1", # 根据需要调整版本号
    openapi_url=f"{settings.API_V1_STR}/openapi.json",
    docs_url="/docs",
    redoc_url="/redoc",
)
logger.info("FastAPI 应用实例已创建。")
# ---------------------------------

# --- CORS 中间件配置 ---
if parsed_cors_origins:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=parsed_cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    logger.info(f"CORS 中间件已为源启用: {parsed_cors_origins}")
else:
    logger.warning("CORS 源列表为空或解析失败。CORS 中间件可能不允许前端访问。")
# ----------------------------------

# --- 挂载静态文件 (已移除) ---
logger.info("静态文件挂载功能已移除。此应用现在专注于 API。")

# --- 提供根前端页面 (已移除) ---
logger.info("根路径 '/' 的 HTML 响应功能已移除。")

# --- 包含 API 路由 ---
logger.info(f"包含 API 路由，前缀: {settings.API_V1_STR}")
app.include_router(auth.router, prefix=settings.API_V1_STR, tags=["认证"]) # Authentication
app.include_router(scales.router, prefix=settings.API_V1_STR, tags=["量表"]) # Scales
app.include_router(assessments.router, prefix=settings.API_V1_STR, tags=["评估"]) # Assessments
app.include_router(reports.router, prefix=settings.API_V1_STR, tags=["报告"]) # Reports
# ******** 修改：添加 encyclopedia 路由包含，移除旧 tips ********
app.include_router(encyclopedia.router, prefix=settings.API_V1_STR, tags=["Encyclopedia"]) # Psychology Encyclopedia
# 如果旧的 tips 路由导入了，需要注释掉或移除：
# app.include_router(tips.router, prefix=settings.API_V1_STR, tags=["Tips"])
# ************************************************************
# -------------------------

# --- 包含页面路由 (已移除) ---
logger.info("页面 (Pages) 路由包含功能已移除。")

# --- 最终启动消息 ---
logger.info("FastAPI 应用已配置并准备就绪 (仅 API)。")
logger.info(f"在 /docs 或 /redoc 访问 API 文档")
# ---------------------------

# --- Uvicorn 运行部分 (如果直接运行) ---
# if __name__ == "__main__":
#     import uvicorn
#     logger.info("启动 Uvicorn 开发服务器...")
#     run_host = settings.YAML_CONFIG.get('HOST', '0.0.0.0')
#     run_port = settings.YAML_CONFIG.get('PORT', 8000)
#     logger.info(f"服务器将在 http://{run_host}:{run_port} 上运行")
#     uvicorn.run("app.main:app", host=run_host, port=run_port, reload=True) # reload=True 仅用于开发
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\__init__.py
# # FILE: app/crud/__init__.py (Corrected)
# from . import user      # 导入 user.py 中的内容
# from . import assessment # +++ 添加这一行，导入 assessment.py 中的内容 +++

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\core\celery_app.py
# app/core/celery_app.py
from celery import Celery
from app.core.config import settings # 导入你的设置

# 配置 Redis 作为 Broker 和 Backend
# 使用 settings 中的配置或默认值
REDIS_URL = "redis://localhost:6379/0" # 默认 Redis 地址和数据库 0
# 你可以在 .env 中添加 REDIS_URL 并从 settings 加载

# 创建 Celery 实例
# main 参数通常是 Celery 应用的入口点名称，这里用 'app' 或项目名
celery_app = Celery(
    "QingtingzheApp", # 与 FastAPI app name 保持一致或自定义
    broker=REDIS_URL,
    backend=REDIS_URL, # 使用 Redis 作为结果存储后端
    include=['app.tasks.analysis'] # 指定包含任务定义的模块列表
)

# 可选：Celery 配置项 (可以放在 settings 或这里)
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],  # Allow json content
    result_serializer='json',
    timezone='Asia/Shanghai', # 设置时区
    enable_utc=True,
    # task_track_started=True, # 如果需要追踪任务开始状态
    # broker_connection_retry_on_startup=True, # 启动时自动重试连接 broker
)

# 可选: 打印确认信息
print(f"[Celery Setup] Celery app configured. Broker: {REDIS_URL}, Backend: {REDIS_URL}")
print(f"[Celery Setup] Included task modules: {celery_app.conf.include}")

# 如果你需要在任务中使用 FastAPI 的依赖项或设置，
# 可以考虑更复杂的设置，但现在保持简单。
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\core\config.py
# app/core/config.py
import os
import sys
import json # <--- 确保导入 json
from typing import List, Union, Optional, Dict, Any
from pydantic_settings import BaseSettings, SettingsConfigDict
import yaml
import logging
import traceback # Import traceback for better error logging

# --- Basic Logging Setup (for config loading itself) ---
# Get a basic logger instance specifically for configuration loading issues
# This avoids potential issues if the main app logger isn't configured yet
config_logger = logging.getLogger("ConfigLoader")
# Set a default level in case main setup hasn't run
config_logger.setLevel(logging.INFO)
# Add a handler if none exist (e.g., running script directly)
if not config_logger.hasHandlers():
    ch = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    config_logger.addHandler(ch)
# ---------------------------------------------------------


# --- Project Root Calculation ---
# __file__ points to this file (config.py)
# os.path.dirname(__file__) points to app/core
# os.path.dirname(os.path.dirname(__file__)) points to app/
# os.path.dirname(os.path.dirname(os.path.dirname(__file__))) points to PsychologyAnalysis/
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
CONFIG_YAML_PATH = os.path.join(PROJECT_ROOT, "config/config.yaml")
DOTENV_PATH = os.path.join(PROJECT_ROOT, ".env")
# +++ 定义百科文件路径 +++
ENCYCLOPEDIA_JSON_PATH = os.path.join(PROJECT_ROOT, "config/psychology_encyclopedia.json")
# --- (可选) 旧的小贴士文件路径 ---
# TIPS_JSON_PATH = os.path.join(PROJECT_ROOT, "config/psychology_tips.json")
# ---------------------------------


class Settings(BaseSettings):
    """
    Application Settings loaded from .env file and YAML.
    .env file takes precedence for overlapping variables.
    """
    # --- Basic App Settings ---
    APP_NAME: str = "Qingtingzhe AI Analysis"
    ENVIRONMENT: str = "development" # Options: development, staging, production
    API_V1_STR: str = "/api/v1"     # Base path for API V1 endpoints
    LOG_LEVEL: str = "INFO"         # Default logging level (DEBUG, INFO, WARNING, ERROR)

    # --- Security Settings (JWT Authentication) ---
    SECRET_KEY: str = "a_very_unsafe_default_secret_key_please_change_in_dotenv"
    ALGORITHM: str = "HS256"        # JWT signing algorithm
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 # 1 day default

    # --- CORS ---
    BACKEND_CORS_ORIGINS_STR: str = "http://localhost:8080,http://127.0.0.1:8080" # Default development origins

    # --- Database ---
    DB_PATH_SQLITE: str = os.path.join(PROJECT_ROOT, "psychology_analysis.db")
    DATABASE_URL: str = f"sqlite+aiosqlite:///{DB_PATH_SQLITE}" # Example for async SQLite

    # --- File Storage ---
    UPLOADS_DIR: str = os.path.join(PROJECT_ROOT, "uploads")
    LOGS_DIR: str = os.path.join(PROJECT_ROOT, "logs")

    # --- Dashscope API ---
    DASHSCOPE_API_KEY: Optional[str] = None

    # --- Settings potentially loaded from config.yaml (as fallbacks or defaults) ---
    TEXT_MODEL: str = "qwen-plus"
    VISION_MODEL: str = "qwen-vl-plus"
    REPORT_PROMPT_TEMPLATE: Optional[str] = None
    YAML_CONFIG: Dict[str, Any] = {}

    # +++ Psychology Encyclopedia Settings +++
    PSYCHOLOGY_ENCYCLOPEDIA_FILE: str = ENCYCLOPEDIA_JSON_PATH # 使用上面定义的路径
    PSYCHOLOGY_ENTRIES: List[Dict[str, str]] = [] # 初始化为空列表

    # --- (可选) 旧的小贴士字段 ---
    # PSYCHOLOGY_TIPS_FILE: str = TIPS_JSON_PATH
    # PSYCHOLOGY_TIPS: List[str] = []

    # Pydantic Settings Configuration
    model_config = SettingsConfigDict(
        env_file=DOTENV_PATH, # Explicitly point to .env file
        extra='ignore'        # Ignore extra fields not defined in Settings
    )

# --- Helper function to load YAML ---
def load_yaml_config(yaml_path: str) -> Dict[str, Any]:
    """Loads configuration from a YAML file."""
    if not os.path.exists(yaml_path):
        config_logger.warning(f"YAML config file not found at {yaml_path}. Returning empty config.")
        return {}
    try:
        with open(yaml_path, 'r', encoding='utf-8') as f:
            config_data = yaml.safe_load(f)
            if config_data is None:
                config_logger.warning(f"YAML config file at {yaml_path} is empty. Returning empty config.")
                return {}
            config_logger.info(f"Successfully loaded configuration from {yaml_path}")
            return config_data
    except Exception as e:
        config_logger.error(f"Error loading YAML config from {yaml_path}: {e}", exc_info=True)
        return {}

# +++ Helper function to load Encyclopedia JSON +++
def load_encyclopedia_from_json(encyclopedia_path: str) -> List[Dict[str, str]]:
    """从 JSON 文件加载百科条目列表"""
    if not os.path.exists(encyclopedia_path):
        config_logger.warning(f"百科文件未找到: {encyclopedia_path}. 返回空列表.")
        return []
    try:
        with open(encyclopedia_path, 'r', encoding='utf-8') as f:
            entries_list = json.load(f)
            if not isinstance(entries_list, list):
                config_logger.error(f"百科文件 {encyclopedia_path} 格式错误，根元素不是列表.")
                return []

            valid_entries = []
            required_keys = {"category", "title", "content"}
            for i, entry in enumerate(entries_list):
                if isinstance(entry, dict) and required_keys.issubset(entry.keys()):
                    # 基本验证：确保值是字符串且不为空
                    if all(isinstance(entry[key], str) and entry[key].strip() for key in required_keys):
                        valid_entries.append({
                            "category": entry["category"].strip(),
                            "title": entry["title"].strip(),
                            "content": entry["content"].strip()
                        })
                    else:
                        config_logger.warning(f"百科文件 {encyclopedia_path} 中第 {i+1} 条记录的值无效 (非字符串或为空)，已跳过。")
                else:
                    config_logger.warning(f"百科文件 {encyclopedia_path} 中第 {i+1} 条记录格式错误或缺少键，已跳过。")

            config_logger.info(f"成功从 {encyclopedia_path} 加载 {len(valid_entries)} 条有效的百科条目。")
            return valid_entries
    except json.JSONDecodeError as e:
        config_logger.error(f"解析百科 JSON 文件 {encyclopedia_path} 时出错: {e}", exc_info=True)
        return []
    except Exception as e:
        config_logger.error(f"加载百科文件 {encyclopedia_path} 时发生意外错误: {e}", exc_info=True)
        return []

# --- (可选) 旧的 tips 加载函数 ---
# def load_tips_from_json(tips_path: str) -> List[str]: ...

# --- Instantiate Settings (Loads from .env) ---
try:
    settings = Settings()
except Exception as e:
    config_logger.critical(f"CRITICAL ERROR during Settings initialization (loading .env): {e}")
    traceback.print_exc()
    sys.exit(1) # Exit if basic settings fail to load

# --- CRITICAL SECURITY CHECK ---
if settings.SECRET_KEY == "a_very_unsafe_default_secret_key_please_change_in_dotenv":
    config_logger.critical("="*30 + " CRITICAL SECURITY WARNING " + "="*30)
    config_logger.critical("SECRET_KEY is using the default unsafe value!")
    config_logger.critical("Please generate a strong secret key and set it in the .env file.")
    config_logger.critical("Example generation: openssl rand -hex 32")
    config_logger.critical("="*80 + "\n")
    # sys.exit(1)
# -----------------------------

# --- Load and merge YAML config ---
yaml_config_data = load_yaml_config(CONFIG_YAML_PATH)
settings.YAML_CONFIG = yaml_config_data

# Override settings from YAML if needed (check if already set by .env)
if settings.DASHSCOPE_API_KEY is None:
    yaml_api_key = yaml_config_data.get("api_key")
    if yaml_api_key:
        settings.DASHSCOPE_API_KEY = yaml_api_key
        config_logger.warning("Loaded DASHSCOPE_API_KEY from YAML (recommend using .env).")
if settings.TEXT_MODEL == "qwen-plus":
    settings.TEXT_MODEL = yaml_config_data.get("text_model", settings.TEXT_MODEL)
if settings.VISION_MODEL == "qwen-vl-plus":
    settings.VISION_MODEL = yaml_config_data.get("vision_model", settings.VISION_MODEL)
if settings.REPORT_PROMPT_TEMPLATE is None:
    settings.REPORT_PROMPT_TEMPLATE = yaml_config_data.get("REPORT_PROMPT_TEMPLATE", settings.REPORT_PROMPT_TEMPLATE)

# --- Load Psychology Encyclopedia ---
settings.PSYCHOLOGY_ENTRIES = load_encyclopedia_from_json(settings.PSYCHOLOGY_ENCYCLOPEDIA_FILE)
if not settings.PSYCHOLOGY_ENTRIES:
    config_logger.warning("未能加载任何心理百科条目，相关功能可能受影响。")

# --- (可选) Load old tips ---
# settings.PSYCHOLOGY_TIPS = load_tips_from_json(settings.PSYCHOLOGY_TIPS_FILE)
# if not settings.PSYCHOLOGY_TIPS:
#     config_logger.warning("未能加载任何心理学小贴士。")

# --- Construct Database URL ---
config_logger.info(f"Using Database URL: {settings.DATABASE_URL}")

# --- Ensure Necessary Directories Exist ---
try:
    os.makedirs(settings.UPLOADS_DIR, exist_ok=True)
    os.makedirs(settings.LOGS_DIR, exist_ok=True)
    config_logger.info(f"Ensured directories exist: Uploads='{settings.UPLOADS_DIR}', Logs='{settings.LOGS_DIR}'")
except OSError as e:
     config_logger.error(f"Error creating necessary directories: {e}", exc_info=True)

# --- Check if essential API key is loaded ---
if not settings.DASHSCOPE_API_KEY:
    config_logger.critical("="*30 + " CRITICAL WARNING " + "="*30)
    config_logger.critical("DASHSCOPE_API_KEY is not set in .env or config.yaml!")
    config_logger.critical("AI functionality will likely fail.")
    config_logger.critical("Please set it in the .env file (recommended) or config/config.yaml.")
    config_logger.critical("="*80 + "\n")

# --- Manually Parse CORS Origins String ---
def parse_cors_origins(origins_str: str) -> List[str]:
    """Parses a comma-separated string of origins into a list of strings."""
    if not origins_str or not isinstance(origins_str, str):
        config_logger.warning(f"BACKEND_CORS_ORIGINS_STR is empty or invalid ('{origins_str}'). Using default: ['http://localhost:8080']")
        return ["http://localhost:8080"]
    try:
        parsed = [origin.strip() for origin in origins_str.split(",") if origin.strip()]
        if not parsed:
             config_logger.warning(f"Parsing BACKEND_CORS_ORIGINS_STR '{origins_str}' resulted in empty list. Using default: ['http://localhost:8080']")
             return ["http://localhost:8080"]
        config_logger.info(f"Successfully parsed CORS origins: {parsed}")
        return parsed
    except Exception as e:
        config_logger.error(f"Error parsing BACKEND_CORS_ORIGINS_STR '{origins_str}': {e}. Using default: ['http://localhost:8080']", exc_info=True)
        return ["http://localhost:8080"]

parsed_cors_origins: List[str] = parse_cors_origins(settings.BACKEND_CORS_ORIGINS_STR)
# --------------------------------------


# --- Log Final Effective Settings ---
config_logger.info("--- Effective Application Settings ---")
config_logger.info(f"APP_NAME: {settings.APP_NAME}")
config_logger.info(f"ENVIRONMENT: {settings.ENVIRONMENT}")
config_logger.info(f"LOG_LEVEL: {settings.LOG_LEVEL}")
config_logger.info(f"API_V1_STR: {settings.API_V1_STR}")
config_logger.info(f"DATABASE_URL: {settings.DATABASE_URL}")
config_logger.info(f"UPLOADS_DIR: {settings.UPLOADS_DIR}")
config_logger.info(f"LOGS_DIR: {settings.LOGS_DIR}")
config_logger.info(f"CORS Origins (Parsed): {parsed_cors_origins}")
config_logger.info(f"JWT Algorithm: {settings.ALGORITHM}")
config_logger.info(f"Token Expire Minutes: {settings.ACCESS_TOKEN_EXPIRE_MINUTES}")
config_logger.info(f"SECRET_KEY Loaded: {'Yes' if settings.SECRET_KEY != 'a_very_unsafe_default_secret_key_please_change_in_dotenv' else 'NO (Using default - UNSAFE!)'}")
config_logger.info(f"DASHSCOPE_API_KEY Loaded: {'Yes' if settings.DASHSCOPE_API_KEY else 'NO - CRITICAL!'}")
config_logger.info(f"Text Model: {settings.TEXT_MODEL}")
config_logger.info(f"Vision Model: {settings.VISION_MODEL}")
config_logger.info(f"Encyclopedia Entries Loaded: {len(settings.PSYCHOLOGY_ENTRIES)}") # <--- Log loaded entry count
# config_logger.info(f"Tips Loaded (Old method): {len(settings.PSYCHOLOGY_TIPS)}") # If keeping old tips
config_logger.info("------------------------------------")

# `settings` instance can now be imported by other modules.
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\core\deps.py
# app/core/deps.py
import logging
from typing import AsyncGenerator, Optional # Use AsyncGenerator for async yield
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from sqlalchemy.ext.asyncio import AsyncSession # Import AsyncSession

# --- Core App Imports ---
from app.core.config import settings
from app.core.security import decode_access_token # Your JWT decoding function

# --- Database and CRUD Imports ---
from app.db.session import AsyncSessionLocal # Import the async session maker
from app import crud, models, schemas # Adjust imports based on your project structure

logger = logging.getLogger(settings.APP_NAME) # Use the main app logger

# --- OAuth2 Scheme Definition ---
# Define the URL where clients will send username/password to get a token.
# This should match the path operation of your login endpoint.
oauth2_scheme = OAuth2PasswordBearer(tokenUrl=f"{settings.API_V1_STR}/auth/token")

# --- Asynchronous Database Session Dependency ---
async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """
    Dependency that provides an asynchronous database session per request.
    It ensures the session is properly closed afterwards.
    """
    async with AsyncSessionLocal() as session:
        try:
            yield session
            # Optional: You could uncomment the next line if you want commits
            # to happen automatically at the end of successful requests,
            # but manual commits in CRUD functions are generally preferred
            # for better control.
            # await session.commit()
        except Exception as e:
            # Rollback in case of exceptions during the request handling
            logger.error(f"Database session error: {e}", exc_info=True)
            await session.rollback()
            # Re-raise the exception so FastAPI can handle it
            raise
        # Session is automatically closed when exiting the 'async with' block

# --- Asynchronous Current User Dependency ---
async def get_current_user(
    db: AsyncSession = Depends(get_db),          # Depend on the async get_db
    token: str = Depends(oauth2_scheme)          # Get token from Authorization header
) -> models.User:                                # Return the SQLAlchemy User model
    """
    Decodes the JWT token, validates it, and retrieves the current user
    from the database asynchronously. Raises HTTPException if invalid.
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )

    # Decode the token using your security utility
    token_data = decode_access_token(token)
    if token_data is None or token_data.username is None:
        logger.warning("Token decoding failed or username missing in token payload.")
        raise credentials_exception

    # Retrieve the user from the database asynchronously using the async CRUD function
    try:
        # Ensure crud.user.get_user_by_username is an async function
        user = await crud.user.get_user_by_username(db, username=token_data.username)
    except Exception as e:
        logger.error(f"Database error while fetching user '{token_data.username}': {e}", exc_info=True)
        # Don't expose internal DB errors directly, raise the standard credentials exception
        raise credentials_exception

    if user is None:
        logger.warning(f"User '{token_data.username}' found in token but not in database.")
        raise credentials_exception

    # Return the validated user object
    return user

# --- Asynchronous Active User Dependency ---
async def get_current_active_user(
    current_user: models.User = Depends(get_current_user), # Depend on get_current_user
) -> models.User:
    """
    Ensures the user retrieved from the token is marked as active.
    """
    if not current_user.is_active:
        logger.warning(f"Authentication attempt by inactive user: {current_user.username}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Inactive user")
    return current_user

# --- Asynchronous Superuser Dependency ---
async def get_current_active_superuser(
    current_user: models.User = Depends(get_current_active_user), # Depend on active user
) -> models.User:
    """
    Ensures the current user is active AND is a superuser.
    Raises HTTPException if not a superuser.
    """
    if not current_user.is_superuser:
        logger.warning(f"Superuser access denied for non-superuser: {current_user.username}")
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="The user doesn't have enough privileges",
        )
    return current_user
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\core\security.py
# app/core/security.py
from datetime import datetime, timedelta, timezone
from typing import Any, Union, Optional
from jose import jwt, JWTError
from passlib.context import CryptContext
from app.core.config import settings
from app.schemas.token import TokenData # 导入令牌数据模式

# 使用 bcrypt 哈希算法
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

ALGORITHM = settings.ALGORITHM
SECRET_KEY = settings.SECRET_KEY
ACCESS_TOKEN_EXPIRE_MINUTES = settings.ACCESS_TOKEN_EXPIRE_MINUTES

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """验证明文密码与哈希密码是否匹配"""
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    """生成密码的哈希值"""
    return pwd_context.hash(password)

def create_access_token(
    subject: Union[str, Any], expires_delta: Optional[timedelta] = None
) -> str:
    """创建 JWT 访问令牌"""
    if expires_delta:
        expire = datetime.now(timezone.utc) + expires_delta
    else:
        # 使用配置中的过期时间
        expire = datetime.now(timezone.utc) + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    # 令牌中至少包含过期时间和主题 (subject, 通常是用户名)
    to_encode = {"exp": expire, "sub": str(subject)}
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

def decode_access_token(token: str) -> Optional[TokenData]:
    """解码访问令牌，验证其有效性"""
    try:
        # 解码 JWT
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        # 提取用户名
        username: str | None = payload.get("sub")
        if username is None:
            # 令牌中没有 'sub' 字段
            return None
        # 你可以在这里添加更多的载荷验证逻辑 (例如：检查 scopes)
        # 返回包含用户名的 TokenData 对象
        return TokenData(username=username)
    except JWTError:
        # 令牌无效 (格式错误、签名不匹配、已过期等)
        return None
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\core\__init__.py

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\crud\assessment.py
# FILE: app/crud/assessment.py

import logging
from typing import Optional, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
# sqlalchemy.future.select is usually needed for async queries, keep if used elsewhere
from sqlalchemy.future import select

# 导入你的 Assessment 模型 和 配置
from app.models.assessment import Assessment
from app.core.config import settings

# 获取日志记录器
logger = logging.getLogger(settings.APP_NAME)

# --- 已有的 get 函数 (示例，确保它存在) ---
async def get(db: AsyncSession, id: int) -> Optional[Assessment]:
    """通过 ID 异步获取评估记录。"""
    logger.debug(f"正在通过 ID 获取评估记录 (CRUD): {id}")
    result = await db.execute(select(Assessment).filter(Assessment.id == id))
    return result.scalar_one_or_none()

# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# +++  确保添加或替换为下面的 async def create 函数定义   +++
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
async def create(db: AsyncSession, **kwargs: Any) -> Assessment:
    """
    异步创建一条新的评估记录。
    接受包含字段的关键字参数，键名应与 Assessment 模型字段匹配。
    """
    logger.info(f"正在为 '{kwargs.get('subject_name', 'N/A')}' 创建新的评估记录 (CRUD)")

    # 直接从关键字参数创建模型实例
    # 路由层已经处理了字段映射，这里直接使用 kwargs
    try:
        db_obj = Assessment(**kwargs)
        logger.debug(f"Assessment 对象创建成功，待存入: {db_obj}")
    except TypeError as te:
        # 如果 kwargs 中的键与 Assessment 模型字段不匹配，这里会抛出 TypeError
        logger.error(f"创建 Assessment 模型实例时字段不匹配: {te}. KWARGS: {kwargs}", exc_info=True)
        # 重新抛出 ValueError，可以被路由层的异常处理捕获
        raise ValueError(f"模型初始化失败，请检查字段: {te}") from te

    # 将新对象添加到数据库会话
    db.add(db_obj)

    try:
        # 提交事务以保存到数据库
        await db.commit()
        # 刷新对象以获取数据库生成的值（如自增 ID, 默认时间戳）
        await db.refresh(db_obj)
        logger.info(f"评估记录创建成功 (CRUD)，ID: {db_obj.id}")
        # 返回创建成功的对象
        return db_obj
    except Exception as e:
        # 如果提交失败，回滚事务
        await db.rollback()
        logger.error(f"创建评估记录时数据库提交失败 (CRUD): {e}", exc_info=True)
        # 重新引发异常，让路由层处理 HTTP 响应
        raise e
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# +++                 create 函数定义结束                   +++
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


# --- 已有的 update_report_text 函数 (示例，确保它存在) ---
async def update_report_text(db: AsyncSession, assessment_id: int, report_text: str) -> Optional[Assessment]:
    """异步更新指定评估记录的 report_text。"""
    logger.debug(f"尝试更新报告文本 (CRUD)，ID: {assessment_id}")
    db_obj = await get(db, id=assessment_id) # 复用 get 函数
    if not db_obj:
        logger.warning(f"尝试更新不存在的评估记录报告 (CRUD)，ID: {assessment_id}")
        return None
    db_obj.report_text = report_text
    # updated_at 应该由数据库触发器或 SQLAlchemy 的 onupdate 自动处理
    db.add(db_obj) # 标记对象已修改
    try:
        await db.commit()
        await db.refresh(db_obj)
        logger.info(f"评估记录 ID {assessment_id} 的报告文本已更新 (CRUD)。")
        return db_obj
    except Exception as e:
        await db.rollback()
        logger.error(f"更新评估记录 ID {assessment_id} 的报告文本时数据库提交失败 (CRUD): {e}", exc_info=True)
        raise e

# 你可以根据需要添加其他的异步 CRUD 函数，比如 get_multi, remove, update 等。
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\crud\user.py
# app/crud/user.py
import logging
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select # For SQLAlchemy 2.0 style selects

# --- Core App Imports ---
from app.core.security import get_password_hash, verify_password # Your security utils
from app.models.user import User # Your SQLAlchemy User model
from app.schemas.user import UserCreate, UserUpdate # Your Pydantic schemas for User
from app.core.config import settings

logger = logging.getLogger(settings.APP_NAME)

# --- Async User CRUD Operations ---

async def get_user(db: AsyncSession, user_id: int) -> User | None:
    """
    Asynchronously retrieves a user by their ID.
    """
    logger.debug(f"Attempting to get user by ID: {user_id}")
    result = await db.execute(select(User).filter(User.id == user_id))
    user = result.scalar_one_or_none()
    if user:
        logger.debug(f"User found by ID: {user_id}")
    else:
        logger.debug(f"User not found by ID: {user_id}")
    return user

async def get_user_by_username(db: AsyncSession, username: str) -> User | None:
    """
    Asynchronously retrieves a user by their username (case-insensitive search recommended).
    """
    logger.debug(f"Attempting to get user by username: {username}")
    # Consider lowercasing username for case-insensitive lookup if needed:
    # username_lower = username.lower()
    # result = await db.execute(select(User).filter(func.lower(User.username) == username_lower))
    result = await db.execute(select(User).filter(User.username == username))
    user = result.scalar_one_or_none()
    if user:
        logger.debug(f"User found by username: {username}")
    else:
        logger.debug(f"User not found by username: {username}")
    return user


async def get_user_by_email(db: AsyncSession, email: str) -> User | None:
    """
    Asynchronously retrieves a user by their email (case-insensitive search recommended).
    """
    logger.debug(f"Attempting to get user by email: {email}")
    # email_lower = email.lower()
    # result = await db.execute(select(User).filter(func.lower(User.email) == email_lower))
    result = await db.execute(select(User).filter(User.email == email))
    user = result.scalar_one_or_none()
    if user:
        logger.debug(f"User found by email: {email}")
    else:
        logger.debug(f"User not found by email: {email}")
    return user

async def create_user(db: AsyncSession, *, user_in: UserCreate) -> User:
    """
    Asynchronously creates a new user in the database.
    """
    logger.info(f"Attempting to create user with username: {user_in.username}")
    # Check if username or email already exists (optional but good practice)
    existing_user_by_username = await get_user_by_username(db, username=user_in.username)
    if existing_user_by_username:
        logger.warning(f"Username '{user_in.username}' already exists.")
        # Consider raising a custom exception or returning None/error indicator
        raise ValueError(f"Username '{user_in.username}' is already registered.")
    if user_in.email:
        existing_user_by_email = await get_user_by_email(db, email=user_in.email)
        if existing_user_by_email:
            logger.warning(f"Email '{user_in.email}' already exists.")
            raise ValueError(f"Email '{user_in.email}' is already registered.")

    # Hash the password
    hashed_password = get_password_hash(user_in.password)

    # Create the SQLAlchemy User model instance
    db_user = User(
        username=user_in.username, # Consider storing username lowercase: .lower()
        email=user_in.email,       # Consider storing email lowercase: .lower()
        full_name=user_in.full_name,
        hashed_password=hashed_password,
        is_active=user_in.is_active if user_in.is_active is not None else True,
        is_superuser=user_in.is_superuser if user_in.is_superuser is not None else False
    )

    # Add the new user object to the session
    db.add(db_user)

    # Commit the transaction to save the user to the database
    try:
        await db.commit()
        logger.info(f"User '{user_in.username}' committed to database.")
    except Exception as e:
        await db.rollback() # Rollback on error
        logger.error(f"Database commit failed while creating user '{user_in.username}': {e}", exc_info=True)
        raise # Re-raise the exception

    # Refresh the object to get ID and defaults set by the database
    await db.refresh(db_user)
    logger.info(f"User '{db_user.username}' created successfully with ID: {db_user.id}")
    return db_user


async def update_user(db: AsyncSession, *, db_user: User, user_in: UserUpdate) -> User:
    """
    Asynchronously updates an existing user's information.
    """
    logger.info(f"Attempting to update user ID: {db_user.id}")
    # Get the dictionary representation of the input schema, excluding unset fields
    update_data = user_in.model_dump(exclude_unset=True) # Use model_dump in Pydantic V2

    # If password is being updated, hash the new one
    if "password" in update_data and update_data["password"]:
        hashed_password = get_password_hash(update_data["password"])
        del update_data["password"] # Remove plain password from update dict
        update_data["hashed_password"] = hashed_password
        logger.info(f"Password updated for user ID: {db_user.id}")
    elif "password" in update_data:
        # Handle case where password might be None or empty string in update schema
        del update_data["password"] # Don't update password if not provided a valid new one

    # Update the user object's attributes
    for field, value in update_data.items():
        setattr(db_user, field, value)

    # Add the updated user object to the session (marks it as dirty)
    db.add(db_user)

    # Commit the changes
    try:
        await db.commit()
        logger.info(f"User ID {db_user.id} changes committed.")
    except Exception as e:
        await db.rollback()
        logger.error(f"Database commit failed while updating user ID {db_user.id}: {e}", exc_info=True)
        raise

    # Refresh the object
    await db.refresh(db_user)
    logger.info(f"User ID {db_user.id} updated successfully.")
    return db_user

# Add other async CRUD functions as needed (e.g., get_multi, remove)
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\crud\__init__.py
# app/crud/__init__.py
from . import user # 导入同目录下的 user.py 文件作为一个模块
from . import assessment # +++ 添加这一行，导入 assessment.py 中的内容 +++
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\db\base_class.py
from sqlalchemy.orm import declarative_base

# 创建所有 ORM 模型将继承的基类
Base = declarative_base()
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\db\init_db.py
# app/db/init_db.py (使用 run_sync 修正)
import logging
import asyncio  # 1. 导入 asyncio 库
from app.db.session import async_engine # 确保你导入的是 async_engine
# 导入所有需要创建表的模型，以及 Base
from app.db.base_class import Base
from app.models.user import User # 导入 User 模型
# from app.models.assessment import Assessment # 如果有其他模型，也导入

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 2. 将 init_db 函数改为异步函数 (async def)
async def init_db() -> None:
    """
    根据 SQLAlchemy 模型异步地创建数据库表。
    """
    logger.info("Attempting to create database tables asynchronously...")
    try:
        # 3. 使用异步上下文管理器获取连接
        async with async_engine.begin() as conn:
            logger.info("Acquired async connection. Running create_all synchronously...")
            # 4. 在异步连接上，使用 run_sync 来执行同步的 create_all 方法
            # 这会将 create_all 的执行委托给事件循环的线程池
            await conn.run_sync(Base.metadata.create_all)
            logger.info("Base.metadata.create_all executed via run_sync.")

        logger.info("Database tables created successfully (if they didn't exist).")
    except Exception as e:
        logger.error(f"Error creating database tables: {e}", exc_info=True)
        raise e
    # finally:
        # 通常不需要手动 dispose，async with 会处理好连接释放
        # 如果需要确保引擎完全关闭（比如脚本结束时），可以取消注释下面两行
        # await async_engine.dispose()
        # logger.info("Async engine disposed.")

if __name__ == "__main__":
    print("Running database initialization...")
    # 5. 使用 asyncio.run() 来运行顶层的异步函数 init_db
    try:
        asyncio.run(init_db())
        print("Database initialization finished successfully.")
    except Exception as e:
        # 捕获在 init_db 中可能重新抛出的异常
        print(f"Database initialization failed: {e}")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\db\session.py
# app/db/session.py
import logging
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession
from app.core.config import settings # 导入你的设置

logger = logging.getLogger(settings.APP_NAME) # Or use a specific logger

# --- Asynchronous Database Engine ---
# Create an asynchronous engine using the DATABASE_URL from settings.
# Ensure settings.DATABASE_URL is like "sqlite+aiosqlite:///path/to/your.db"
logger.info(f"Creating async engine for database: {settings.DATABASE_URL}")
try:
    async_engine = create_async_engine(
        settings.DATABASE_URL,
        # echo=True,  # Uncomment for debugging SQL statements
        future=True  # Enables SQLAlchemy 2.0 style features
        # connect_args can be added here if needed, but typically not for aiosqlite
    )
    logger.info("Async engine created successfully.")
except Exception as e:
    logger.critical(f"Failed to create async engine: {e}", exc_info=True)
    raise e # Re-raise the exception to stop application startup if engine fails

# --- Asynchronous Database Session Maker ---
# Create an asynchronous session factory configured to use the async engine.
# expire_on_commit=False is recommended for FastAPI dependency usage,
# preventing attributes from being expired after commit within a request.
AsyncSessionLocal = async_sessionmaker(
    bind=async_engine,
    class_=AsyncSession,       # Specify the use of AsyncSession
    expire_on_commit=False,    # Keep objects accessible after commit within the session scope
    autocommit=False,          # Standard setting, commits are manual
    autoflush=False            # Standard setting, flushing is manual or on commit
)
logger.info("AsyncSessionLocal (async session maker) configured.")

# Note: You will typically use this AsyncSessionLocal in your dependency
# injection function (e.g., get_db in deps.py) to get session instances.
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\db\__init__.py
# 可以为空，或者导入 Base 和 SessionLocal 以便更容易访问
from .base_class import Base
#from .session import SessionLocal, engine
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\models\assessment.py
# app/models/assessment.py
from sqlalchemy import Column, Integer, String, Boolean, Text, TIMESTAMP, ForeignKey
from sqlalchemy.sql import func
from app.db.base_class import Base

class Assessment(Base):
    __tablename__ = "analysis_data" # 假设这是你现有的表名

    id = Column(Integer, primary_key=True, index=True)
    image_path = Column(Text, nullable=True)
    subject_name = Column(String(200), index=True)
    age = Column(Integer)
    gender = Column(String(10))
    questionnaire_type = Column(String(100), nullable=True)
    questionnaire_data = Column(Text, nullable=True) # 存储 JSON 字符串
    report_text = Column(Text, nullable=True)

    # 时间戳 - 推荐使用数据库默认值或 SQLAlchemy 的 server_default
    created_at = Column(TIMESTAMP, server_default=func.now())
    # 对于 updated_at，SQLAlchemy 的 onupdate 在某些后端可能效果不佳，
    # SQLite 需要触发器，MySQL/PostgreSQL 可以用 onupdate=func.now()
    # 如果使用 SQLite 触发器，这里可以只设置 server_default
    updated_at = Column(TIMESTAMP, server_default=func.now(), onupdate=func.now())

    # 确保包含所有在 data_handler._init_db 中添加的列
    id_card = Column(String(50), unique=True, index=True, nullable=True)
    occupation = Column(String(100), nullable=True)
    case_name = Column(String(200), nullable=True)
    case_type = Column(String(100), nullable=True)
    identity_type = Column(String(100), nullable=True)
    person_type = Column(String(100), nullable=True)
    marital_status = Column(String(50), nullable=True)
    children_info = Column(Text, nullable=True)
    criminal_record = Column(Integer, default=0)
    health_status = Column(Text, nullable=True)
    phone_number = Column(String(50), nullable=True)
    domicile = Column(String(200), nullable=True)

    # 外键，链接到提交此评估的用户
    # 确保 users 表和 User 模型存在
    submitter_id = Column(Integer, ForeignKey("users.id"), nullable=True) # 假设提交者可选

    # (可选) 如果你想在查询 Assessment 时方便地访问 User 对象，可以定义关系
    # from sqlalchemy.orm import relationship
    # from .user import User # 导入 User 模型
    # submitter = relationship("User")

    def __repr__(self):
        return f"<Assessment(id={self.id}, subject='{self.subject_name}', type='{self.questionnaire_type}')>"
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\models\user.py
from sqlalchemy import Column, Integer, String, Boolean
from app.db.base_class import Base # 从我们创建的基类导入

class User(Base):
    __tablename__ = "users" # 数据库中的表名

    id = Column(Integer, primary_key=True, index=True)
    # 用户名，唯一且加索引，不允许为空
    username = Column(String(100), unique=True, index=True, nullable=False)
    # 邮箱，唯一且加索引，可以为空
    email = Column(String(255), unique=True, index=True, nullable=True)
    # 存储哈希后的密码，不允许为空
    hashed_password = Column(String(255), nullable=False)
    # 全名，可以为空
    full_name = Column(String(100), nullable=True)
    # 是否激活，默认为 True
    is_active = Column(Boolean(), default=True, nullable=False)
    # 是否为超级管理员，默认为 False
    is_superuser = Column(Boolean(), default=False, nullable=False)

    # __repr__ 方法用于方便调试时打印对象信息
    def __repr__(self):
        return f"<User(id={self.id}, username='{self.username}', email='{self.email}')>"

# 你可以在这里定义其他模型，如 Assessment, Report 等
# 它们都需要继承自 Base
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\models\__init__.py
# app/models/__init__.py

# 从同级目录下的 user.py 文件中导入 User 类
from .user import User
# 从同级目录下的 assessment.py 文件中导入 Assessment 类 (如果已创建)
from .assessment import Assessment

# (可选，但推荐) 定义 __all__ 列表，明确指定从 'from app.models import *' 时应导入的内容
__all__ = [
    "User",
    "Assessment",
    # 未来添加的其他模型也在此处列出
]
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\routers\assessments.py
# FILE: app/routers/assessments.py (Corrected)
import logging
import os
import json
from datetime import datetime
from fastapi import APIRouter, HTTPException, Depends, File, UploadFile, Form, Request, status
from typing import Dict, Any, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.exc import IntegrityError # Import for potential db errors

# --- Core App Imports ---
from app.core.config import settings
from app.schemas.assessment import AssessmentSubmitResponse
# Ensure your Celery task is importable
try:
    from app.tasks.analysis import run_ai_analysis
except ImportError:
    run_ai_analysis = None # Define as None if import fails
    logging.getLogger(settings.APP_NAME or "FallbackLogger").error("Failed to import Celery task 'run_ai_analysis'. Background processing disabled.")


# --- Authentication & Database Imports ---
from app.core.deps import get_current_active_user, get_db # Use async get_db
from app import models, schemas # Import schemas for potential use
# Import the main crud package (ensure app/crud/__init__.py imports assessment)
from app import crud

# --- Utils ---
try:
    from werkzeug.utils import secure_filename
except ImportError:
    import re
    def secure_filename(filename):
        if not filename: return "invalid_filename"
        # Remove potentially unsafe characters
        filename = re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)
        # Remove leading/trailing dots or underscores
        filename = filename.strip('._')
        # Ensure filename is not empty after stripping
        return filename if filename else "invalid_filename"
    # Get logger safely
    logging.getLogger(settings.APP_NAME or "FallbackLogger").warning("werkzeug not installed. Using basic secure_filename fallback.")


logger = logging.getLogger(settings.APP_NAME) # Use the configured app logger
router = APIRouter()

# --- API Endpoint ---
@router.post(
    "/assessments/submit",
    response_model=AssessmentSubmitResponse,
    tags=["Assessments"],
    status_code=status.HTTP_202_ACCEPTED # Return 202 Accepted on success
)
async def submit_assessment(
    # --- Dependencies ---
    db: AsyncSession = Depends(get_db),
    request: Request = None, # Keep for form data parsing
    current_user: models.User = Depends(get_current_active_user), # Authentication dependency

    # --- Form Fields (Match names used in FormData in JS) ---
    name: str = Form(..., description="姓名"),
    gender: str = Form(..., description="性别"),
    age: int = Form(..., gt=0, description="年龄"),
    id_card: Optional[str] = Form(None, description="身份证号"),
    occupation: Optional[str] = Form(None, description="职业"),
    case_name: Optional[str] = Form(None, description="案件名称"),
    case_type: Optional[str] = Form(None, description="案件类型"),
    identity_type: Optional[str] = Form(None, description="人员身份"),
    person_type: Optional[str] = Form(None, description="人员类型"),
    marital_status: Optional[str] = Form(None, description="婚姻状况"),
    children_info: Optional[str] = Form(None, description="子女情况"),
    criminal_record: Optional[int] = Form(0, ge=0, le=1, description="有无犯罪前科 (0:无, 1:有)"),
    health_status: Optional[str] = Form(None, description="健康情况"),
    phone_number: Optional[str] = Form(None, description="手机号"),
    domicile: Optional[str] = Form(None, description="归属地"),
    scale_type: Optional[str] = Form(None, description="选择的量表代码"),
    # Ensure 'image' matches the name attribute of the file input in HTML/JS
    image: Optional[UploadFile] = File(None, description="上传的绘画图片")
):
    """
    Receives assessment data from an authenticated user.
    Saves the data and queues a background task for AI analysis.
    """
    # +++ Get username and ID *before* potential database errors +++
    submitter_username = current_user.username
    submitter_id = current_user.id
    logger.info(f"用户 '{submitter_username}' (ID: {submitter_id}) 正在提交新的评估，主体姓名: {name}")

    # --- 1. 收集基础信息 (Map Form fields to DB model fields) ---
    basic_info = {
        "subject_name": name, # Maps 'name' from Form to 'subject_name' in DB model
        "gender": gender,
        "age": age,
        "id_card": id_card,
        "occupation": occupation,
        "case_name": case_name,
        "case_type": case_type,
        "identity_type": identity_type,
        "person_type": person_type,
        "marital_status": marital_status,
        "children_info": children_info,
        "criminal_record": criminal_record, # Already int 0 or 1
        "health_status": health_status,
        "phone_number": phone_number,
        "domicile": domicile,
        "submitter_id": submitter_id # Add the authenticated user's ID
    }
    logger.debug(f"收集的基础信息 (待存入数据库): {basic_info}")

    # --- 2. 处理图片上传 ---
    image_relative_path = None
    image_full_path = None # Keep track of full path for saving
    image_was_saved_to_disk = False # Flag for cleanup on error

    if image and image.filename:
        # Sanitize filename
        original_filename = secure_filename(image.filename)
        if original_filename == "invalid_filename":
             logger.warning(f"用户 {submitter_username} 上传了无效的文件名。")
             # Optionally raise HTTPException or proceed without image
             # raise HTTPException(status_code=400, detail="无效的文件名。")
             image = None # Treat as no image uploaded

        if image: # Check again if image is still valid after filename check
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            id_part = secure_filename(id_card if id_card else (name if name else 'UnknownID'))
            base, ext = os.path.splitext(original_filename)
            safe_base = base[:50] # Limit base filename length
            # Standardize extension to lowercase
            ext_lower = ext.lower()
            allowed_extensions = {'.png', '.jpg', '.jpeg', '.gif'} # Example allowed types
            if ext_lower not in allowed_extensions:
                logger.warning(f"用户 {submitter_username} 上传了不允许的文件类型: {ext_lower}")
                raise HTTPException(status_code=400, detail=f"不允许的文件类型: {ext}. 请上传 {', '.join(allowed_extensions)} 文件。")

            # Construct safe filename for saving
            image_filename_to_save = f"{id_part}_{timestamp}_{safe_base}{ext_lower}"
            image_full_path = os.path.join(settings.UPLOADS_DIR, image_filename_to_save)
            # Store relative path (or just the filename) in the DB
            image_relative_path = image_filename_to_save # Or adjust based on how you serve files

            try:
                # Ensure upload directory exists
                os.makedirs(settings.UPLOADS_DIR, exist_ok=True)
                # Read file content asynchronously and save
                file_content = await image.read()
                with open(image_full_path, "wb") as buffer:
                    buffer.write(file_content)
                image_was_saved_to_disk = True # Mark as saved for potential cleanup
                logger.info(f"图片由用户 {submitter_username} 保存至: {image_full_path}")
            except Exception as e:
                logger.error(f"用户 {submitter_username} 保存上传图片至 {image_full_path} 时出错: {e}", exc_info=True)
                # Don't proceed if saving failed, inform user
                raise HTTPException(status_code=500, detail=f"保存上传文件时出错: {e}")
            finally:
                 # Ensure file is closed (UploadFile should handle this, but good practice)
                 await image.close()
    else:
        logger.info(f"用户 {submitter_username} 未上传图片。")


    # --- 3. 收集量表答案 (From dynamic form fields q1, q2...) ---
    scale_answers_dict = {}
    scale_answers_json = None
    if scale_type:
        if request is None:
             logger.error("未注入 Request 对象，无法解析量表答案。")
             # This indicates a server-side setup issue
             raise HTTPException(status_code=500, detail="内部服务器错误: 无法访问请求对象。")
        try:
            # Get all form data asynchronously
            form_data = await request.form()
            for key, value in form_data.items():
                # Check if the key starts with 'q' followed by digits
                if key.startswith('q') and key[1:].isdigit():
                    scale_answers_dict[key] = value

            if scale_answers_dict:
                # Convert the collected answers to a JSON string for DB storage
                scale_answers_json = json.dumps(scale_answers_dict, ensure_ascii=False)
                logger.info(f"用户 {submitter_username} 为量表 '{scale_type}' 收集到的答案: {len(scale_answers_dict)} 条")
                logger.debug(f"量表答案 (JSON): {scale_answers_json}")
            else:
                logger.warning(f"用户 {submitter_username} 提供了量表类型 '{scale_type}', 但未在表单中找到以 'q' 开头的答案。")
                # Depending on requirements, you might raise an error or allow submission without answers
                # raise HTTPException(status_code=400, detail=f"选择了量表 '{scale_type}' 但未提供答案。")
        except Exception as e:
             logger.error(f"用户 {submitter_username} 解析量表答案时出错: {e}", exc_info=True)
             # Proceed without scale data or raise error
             scale_answers_json = None # Ensure it's None if parsing fails
             # raise HTTPException(status_code=400, detail=f"解析量表答案时出错: {e}")

    # --- 4. 使用异步 CRUD 保存初始数据 ---
    assessment_id = None
    try:
        # --- *** Access assessment CRUD via the imported crud package *** ---
        # This now requires app/crud/__init__.py to contain `from . import assessment`
        new_assessment = await crud.assessment.create(
            db=db,
            # Pass collected data using keyword arguments matching Assessment model fields
            **basic_info, # Unpack the dictionary of basic info
            image_path=image_relative_path, # Store relative path/filename
            questionnaire_type=scale_type,
            questionnaire_data=scale_answers_json, # Store the JSON string
            report_text=None # Initial report text is empty
        )

        # Check if creation was successful and we got an ID
        if not new_assessment or not hasattr(new_assessment, 'id'):
             # This indicates an unexpected issue with the CRUD function or DB commit
             raise ValueError("数据保存操作未返回有效的评估对象ID。")

        assessment_id = new_assessment.id
        logger.info(f"评估数据由用户 {submitter_username} 保存成功。评估 ID: {assessment_id}")

    except IntegrityError as ie:
        # Handle specific DB constraint errors (like unique ID card if constraint exists)
        logger.warning(f"用户 {submitter_username} 保存评估数据时发生数据库完整性错误: {ie}", exc_info=True)
        await db.rollback()
        # Cleanup potentially saved image if DB save failed
        if image_was_saved_to_disk and image_full_path and os.path.exists(image_full_path):
            try: os.remove(image_full_path); logger.info(f"因数据库完整性错误清理了文件 {image_full_path}")
            except Exception as rm_err: logger.warning(f"数据库错误后无法移除文件 {image_full_path}: {rm_err}")
        raise HTTPException(status_code=409, detail=f"数据保存冲突: 可能身份证号已存在。") # 409 Conflict is appropriate
    except ValueError as ve:
        # Catch custom errors raised, e.g., from the CRUD function itself
        logger.warning(f"用户 {submitter_username} 保存评估数据时发生值错误: {ve}", exc_info=True)
        await db.rollback()
        if image_was_saved_to_disk and image_full_path and os.path.exists(image_full_path):
            try: os.remove(image_full_path); logger.info(f"因值错误清理了文件 {image_full_path}")
            except Exception as rm_err: logger.warning(f"数据库错误后无法移除文件 {image_full_path}: {rm_err}")
        raise HTTPException(status_code=400, detail=f"数据保存错误: {ve}")
    except Exception as e:
        # --- *** Use the pre-fetched username for logging *** ---
        logger.error(f"用户 {submitter_username} 保存评估数据时发生意外错误: {e}", exc_info=True)
        await db.rollback() # Rollback the session

        # --- Cleanup uploaded image if DB save failed ---
        if image_was_saved_to_disk and image_full_path and os.path.exists(image_full_path):
            try:
                os.remove(image_full_path)
                logger.info(f"因数据库意外错误清理了上传文件 {image_full_path}")
            except Exception as rm_err:
                logger.warning(f"数据库错误后无法移除文件 {image_full_path}: {rm_err}")

        # Re-raise as HTTPException for FastAPI to handle
        error_detail = f"数据库处理错误: {type(e).__name__}"
        # Consider hiding specific error details in production: error_detail = "服务器内部数据库错误"
        raise HTTPException(status_code=500, detail=error_detail)


    # --- 5. 触发 Celery 任务 ---
    task_id = None
    if assessment_id: # Only queue if data was saved successfully
        if run_ai_analysis:
            try:
                # Pass only the ID needed for the task
                task = run_ai_analysis.delay(assessment_id)
                task_id = task.id
                logger.info(f"已为评估 ID: {assessment_id} (提交者: {submitter_username}) 排队 AI 分析任务。任务 ID: {task_id}")
            except Exception as celery_err:
                 # Log the error, but the request itself was successful (data saved)
                 logger.error(f"为评估 ID {assessment_id} (提交者: {submitter_username}) 排队 Celery 任务失败: {celery_err}", exc_info=True)
                 # Don't raise HTTPException here, as the primary action (saving data) succeeded.
                 # The response message will indicate the queueing failure.
        else:
             logger.warning(f"Celery task 'run_ai_analysis' 未加载或不可用。评估 ID: {assessment_id} 的后台处理将不会运行。")


    # --- 6. 构建 API 响应 (Based on successful save and task queuing status) ---
    if assessment_id:
        if task_id:
            message = "评估数据已接收，正在后台进行 AI 分析。"
            status_code_resp = "processing_queued"
        elif run_ai_analysis is None: # Check if task function itself is None
            message = f"评估数据已接收 (ID: {assessment_id})，但后台分析任务未配置或导入失败。"
            status_code_resp = "warning_task_unavailable"
        else: # Task function exists but .delay() failed
            message = f"评估数据已接收 (ID: {assessment_id})，但启动后台处理任务时出错。"
            status_code_resp = "warning_queueing_failed"

        return AssessmentSubmitResponse(
            status=status_code_resp,
            message=message,
            submission_id=assessment_id
            # task_id=task_id # Optionally include task_id in response
        )
    else:
        # This case should ideally not be reached if exceptions are handled correctly above
        logger.error(f"评估 ID 未能生成，但未捕获到明确异常。提交者: {submitter_username}")
        raise HTTPException(status_code=500, detail="数据保存后未能获取评估ID。")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\routers\auth.py
# FILE: app/routers/auth.py
import logging
from datetime import timedelta
from typing import Any

from fastapi import APIRouter, Depends, HTTPException, status, Body # +++ Import Body
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.exc import IntegrityError # +++ Import for catching DB errors

from app.core.config import settings
from app.core import security
from app import crud, models, schemas
from app.core.deps import get_db, get_current_active_user

logger = logging.getLogger(settings.APP_NAME)
router = APIRouter()

# --- Existing Login Endpoint (No changes needed here) ---
@router.post("/auth/token", response_model=schemas.Token, tags=["Authentication"])
async def login_for_access_token(
    db: AsyncSession = Depends(get_db),
    form_data: OAuth2PasswordRequestForm = Depends()
) -> Any:
    """
    OAuth2 compatible token login, get an access token for future requests.
    Accepts standard form data with 'username' and 'password'.
    """
    logger.info(f"User login attempt: {form_data.username}")
    user = await crud.user.get_user_by_username(db, username=form_data.username)

    if not user or not security.verify_password(form_data.password, user.hashed_password):
        logger.warning(f"Login failed for user: {form_data.username}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )

    if not user.is_active:
        logger.warning(f"Inactive user login attempt: {form_data.username}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Inactive user")

    access_token = security.create_access_token(subject=user.username)
    logger.info(f"User login successful: {form_data.username}")

    return {"access_token": access_token, "token_type": "bearer"}

# +++ NEW Registration Endpoint +++
@router.post("/auth/register", response_model=schemas.User, status_code=status.HTTP_201_CREATED, tags=["Authentication"])
async def register_user(
    *,
    db: AsyncSession = Depends(get_db),
    # Expecting JSON body with username and password
    user_in: schemas.UserCreate = Body(...) # Use UserCreate schema, require body
) -> Any:
    """
    Create new user. Requires username and password.
    """
    logger.info(f"User registration attempt: {user_in.username}")

    # Check if user already exists
    existing_user = await crud.user.get_user_by_username(db, username=user_in.username)
    if existing_user:
        logger.warning(f"Username '{user_in.username}' already registered.")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Username already registered",
        )

    # Ensure password field is present (Pydantic validation should handle this, but double-check)
    if not user_in.password:
         raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="Password is required for registration.",
        )

    try:
        # Create the user using the async CRUD function
        # UserCreate schema already handles default values for optional fields
        # Password hashing happens inside crud.user.create_user
        user = await crud.user.create_user(db=db, user_in=user_in)
        logger.info(f"User '{user.username}' registered successfully with ID: {user.id}")
        # Return the created user data (excluding password) using the User schema
        return user
    except IntegrityError: # Catch potential race conditions or other DB unique constraint errors
        await db.rollback()
        logger.error(f"Database integrity error during registration for {user_in.username}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Username or email might already be registered (database error).",
        )
    except ValueError as ve: # Catch specific errors raised from CRUD (like explicit duplicate check)
         await db.rollback()
         logger.warning(f"Registration failed for {user_in.username}: {ve}")
         raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(ve), # Return the specific error message (e.g., "Username already registered")
         )
    except Exception as e:
        await db.rollback()
        logger.error(f"Unexpected error during registration for {user_in.username}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An internal error occurred during registration.",
        )


# --- Existing /users/me Endpoint (No changes needed) ---
@router.get("/users/me", response_model=schemas.User, tags=["Users"])
async def read_users_me(
    current_user: models.User = Depends(get_current_active_user)
):
     """
     Get current logged in user's details.
     """
     return current_user
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\routers\encyclopedia.py
# 文件: app/routers/encyclopedia.py
import logging
import random
from typing import Optional, List, Dict
from fastapi import APIRouter, HTTPException, Query, status

from app.core.config import settings
from app.schemas.encyclopedia import EncyclopediaEntry, CategoriesResponse, EntriesResponse # 导入模型

logger = logging.getLogger(settings.APP_NAME)
router = APIRouter()

# --- 辅助函数：从加载的条目中获取数据 ---
def get_all_entries() -> List[Dict[str, str]]:
    """安全地获取配置中的百科条目列表"""
    entries = settings.PSYCHOLOGY_ENTRIES
    if not isinstance(entries, list):
        logger.error("配置中的 PSYCHOLOGY_ENTRIES 不是列表！")
        return []
    return entries

# --- 端点 1: 获取分类 ---
@router.get(
    "/encyclopedia/categories",
    response_model=CategoriesResponse,
    tags=["Encyclopedia"],
    summary="获取所有心理百科分类"
)
async def get_encyclopedia_categories():
    """
    返回所有心理百科条目的唯一分类名称列表。
    """
    all_entries = get_all_entries()
    if not all_entries:
        return CategoriesResponse(categories=[])

    # 提取所有分类并去重，然后排序
    try:
        categories = sorted(list(set(entry.get("category", "未分类") for entry in all_entries)))
        return CategoriesResponse(categories=categories)
    except Exception as e:
         logger.error(f"提取百科分类时出错: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail="处理分类列表时出错")


# --- 端点 2: 获取条目 (包含过滤和随机功能) ---
@router.get(
    "/encyclopedia/entries",
    # 响应模型根据情况可能是列表或单个条目，用 Union 或 Any，或者为随机单独建模型/端点
    # 为清晰起见，我们让它主要返回列表，随机情况特殊处理返回单个条目模型
    response_model=EntriesResponse, # 主要返回列表
    tags=["Encyclopedia"],
    summary="获取心理百科条目"
)
async def get_encyclopedia_entries(
    category: Optional[str] = Query(None, description="按分类过滤条目"),
    random_tip: Optional[bool] = Query(False, description="是否从'心理小贴士'分类中随机获取一条") # 参数名改为 random_tip
    # 可以添加分页参数: page: int = Query(1, ge=1), size: int = Query(10, ge=1, le=100)
):
    """
    获取心理百科条目。
    - 提供 `category` 参数以按分类过滤。
    - 提供 `random_tip=true` 以获取一条随机的“心理小贴士”。(此时忽略 category 参数)
    """
    all_entries = get_all_entries()
    if not all_entries:
        if random_tip:
             # 返回一个默认的 EncyclopediaEntry 结构
             return EntriesResponse(entries=[EncyclopediaEntry(category="心理小贴士", title="提示", content="暂无可用小贴士。")])
        else:
             return EntriesResponse(entries=[]) # 返回空列表

    target_entries = all_entries

    # --- 处理随机小贴士逻辑 ---
    if random_tip:
        tips_category_name = "心理小贴士" # 明确指定分类名称
        tip_entries = [entry for entry in all_entries if entry.get("category") == tips_category_name]

        if not tip_entries:
            logger.warning("请求随机小贴士，但 '心理小贴士' 分类下没有条目。")
            # 返回一个默认的 EncyclopediaEntry 结构，放入列表中
            return EntriesResponse(entries=[EncyclopediaEntry(category=tips_category_name, title="提示", content="暂无可用小贴士。")])

        selected_entry_dict = random.choice(tip_entries)
        # 将选中的字典包装在列表中返回，以匹配 EntriesResponse
        selected_entry_model = EncyclopediaEntry(**selected_entry_dict)
        return EntriesResponse(entries=[selected_entry_model]) # 返回包含单个随机条目的列表

    # --- 处理按分类过滤逻辑 (如果不是请求随机小贴士) ---
    if category:
        target_entries = [entry for entry in all_entries if entry.get("category") == category]
        if not target_entries:
             # 如果指定了分类但找不到，返回空列表是合理的
             logger.info(f"请求分类 '{category}'，但未找到条目。")
             return EntriesResponse(entries=[])

    # --- (可选) 实现分页 ---
    # total = len(target_entries)
    # start = (page - 1) * size
    # end = start + size
    # paged_entries_dicts = target_entries[start:end]

    # --- 将字典列表转换为 Pydantic 模型列表 ---
    # 如果没有分页，直接使用 target_entries
    try:
        result_entries = [EncyclopediaEntry(**entry_dict) for entry_dict in target_entries]
    except Exception as e:
         logger.error(f"将百科条目字典转换为 Pydantic 模型时出错: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail="处理百科条目数据时出错")

    # 返回结果
    return EntriesResponse(entries=result_entries) # , total=total, page=page, size=size) 如果实现分页
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\routers\reports.py
# app/routers/reports.py
import logging
import json
from fastapi import APIRouter, HTTPException, Depends, status
# 导入 AsyncSession
from sqlalchemy.ext.asyncio import AsyncSession
# 导入 Pydantic 验证错误
from pydantic import ValidationError

# --- Core App Imports ---
from app.core.config import settings
# 假设 ReportResponse 结构是 { "report": ReportData | None, "message": str | None }
# 假设 ReportData 是包含报告细节的 Pydantic 模型，且配置了 from_attributes=True
from app.schemas.report import ReportResponse, ReportData

# --- Authentication & Database Imports ---
from app.core.deps import get_current_active_user, get_db
from app import models # 包含 User 和 Assessment 模型
from app import crud   # 包含 assessment CRUD 操作

logger = logging.getLogger(settings.APP_NAME)
router = APIRouter()

@router.get(
    # 注意：从之前的日志看，前端请求的是 /reports/{id}，如果 API 前缀是 /api/v1，
    # 那么这里的路径应该是 "/" 或者 "/{assessment_id}"，取决于 APIRouter 如何包含
    # 假设 APIRouter 前缀是 /api/v1/reports，那么这里是 "/{assessment_id}"
    "/{assessment_id}",
    response_model=ReportResponse, # 响应模型，会过滤掉 ReportData 中未定义的字段
    summary="获取指定评估的分析报告",
    tags=["Reports"],
    responses={ # 添加可能的响应状态码文档
        status.HTTP_200_OK: {"description": "成功获取报告或报告正在生成"},
        status.HTTP_404_NOT_FOUND: {"description": "评估记录未找到"},
        status.HTTP_403_FORBIDDEN: {"description": "无权访问此报告"},
        status.HTTP_500_INTERNAL_SERVER_ERROR: {"description": "服务器内部错误"},
    }
)
async def get_report_by_id(
    assessment_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: models.User = Depends(get_current_active_user) # 添加认证
):
    """
    根据评估 ID 获取单个评估报告的详细信息。
    - 如果报告已生成，返回包含报告内容的 ReportData。
    - 如果报告正在生成中，返回指示信息。
    需要用户已登录。
    """
    logger.info(f"用户 '{current_user.username}' (ID: {current_user.id}) 正在请求评估 ID: {assessment_id} 的报告")

    try:
        # 1. 从数据库异步获取评估记录
        # 假设 crud.assessment.get 是 async 函数
        assessment: models.Assessment | None = await crud.assessment.get(db=db, id=assessment_id)

        # 2. 检查评估是否存在
        if not assessment:
            logger.warning(f"评估 ID {assessment_id} 未找到，请求用户: {current_user.username} (ID: {current_user.id})。")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"评估 ID {assessment_id} 不存在或已被删除。",
            )

        # 3. 权限检查 (示例 - 根据需要取消注释或修改)
        # 假设 Assessment 模型有关联的 submitter_id
        # if not current_user.is_superuser and assessment.submitter_id != current_user.id:
        #     logger.warning(f"用户 '{current_user.username}' (ID: {current_user.id}) 尝试访问不属于自己的评估 {assessment_id}")
        #     raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="您无权访问此报告")

        # 4. 检查报告文本是否已生成
        if not assessment.report_text or assessment.report_text.strip() == "":
            logger.info(f"评估 ID: {assessment_id} 的报告尚未生成或为空。")
            # 返回 200 OK，但在响应体中告知前端报告未就绪
            return ReportResponse(report=None, message="报告正在生成中，请稍后刷新。")

        # 5. 报告已生成，尝试从 ORM 对象创建 ReportData Pydantic 模型
        logger.info(f"评估 ID: {assessment_id} 的报告已找到，正在准备响应数据。")
        try:
            # 假设 ReportData 配置了 from_attributes=True (ORM 模式)
            # 这会尝试从 assessment ORM 对象自动填充 ReportData 的字段
            report_data = ReportData.model_validate(assessment, from_attributes=True)

            # 如果 questionnaire_data 需要特殊处理 (例如从 JSON 字符串解析)
            if isinstance(assessment.questionnaire_data, str):
                try:
                    parsed_q_data = json.loads(assessment.questionnaire_data)
                    # 假设 ReportData 有 questionnaire_data 字段
                    report_data.questionnaire_data = parsed_q_data
                except json.JSONDecodeError:
                    logger.warning(f"无法解码评估 ID {assessment_id} 的 questionnaire_data JSON 字符串。在报告中保留原始字符串或设为错误标记。")
                    # 可以选择保留原始字符串或设置为特定错误标记
                    # report_data.questionnaire_data = {"error": "问卷数据解析失败"}
                    # 或者如果 ReportData 允许字符串：
                    # report_data.questionnaire_data = assessment.questionnaire_data

            # 6. 返回成功的响应
            logger.info(f"成功为评估 ID: {assessment_id} 构建报告响应。")
            return ReportResponse(report=report_data, message="报告获取成功。")

        except ValidationError as pydantic_err:
            # 如果即使配置了 from_attributes=True 仍然出错，说明 ReportData 的字段
            # 与 Assessment 模型字段不匹配，或者数据类型有问题。
            logger.error(f"从评估对象 (ID: {assessment_id}) 创建 ReportData 时发生 Pydantic 验证错误: {pydantic_err}", exc_info=True)
            # 记录原始数据可能有助于调试（注意隐私）
            # logger.debug(f"导致错误的评估对象属性: {assessment.__dict__}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="处理报告数据格式时出错，请联系管理员。"
            )

    except HTTPException as http_exc:
        # 直接重新抛出已知的 HTTP 异常 (如 404, 403)
        raise http_exc
    except Exception as e:
        # 捕捉其他所有意外错误
        logger.exception(f"获取评估 ID {assessment_id} 报告时发生意外服务器错误。用户: {current_user.username} (ID: {current_user.id})", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"获取报告时发生未知错误，请稍后重试或联系支持。",
        )

# 你可能还需要其他端点，例如列出用户自己的报告
# @router.get("/", response_model=list[ReportListItem], tags=["Reports"]) # ReportListItem 是一个简化的 Schema
# async def get_my_reports(
#     db: AsyncSession = Depends(get_db),
#     current_user: models.User = Depends(get_current_active_user),
#     skip: int = 0,
#     limit: int = 100
# ):
#     """获取当前用户提交的所有评估报告列表（简要信息）。"""
#     logger.info(f"用户 '{current_user.username}' 请求他/她自己的报告列表。")
#     # 假设有 crud.assessment.get_multi_by_submitter
#     assessments = await crud.assessment.get_multi_by_submitter(
#         db=db, submitter_id=current_user.id, skip=skip, limit=limit
#     )
#     # 将 assessments 转换为 ReportListItem 列表返回
#     return assessments
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\routers\scales.py
# app/routers/scales.py
import logging
import json # Ensure json is imported
from fastapi import APIRouter, HTTPException, Depends

# Import Pydantic models and other necessary components
from app.schemas.scale import ScaleInfo, ScaleQuestion, ScaleOption, AvailableScalesResponse, ScaleQuestionsResponse
from src.data_handler import DataHandler
from app.core.config import settings

# Get the logger instance configured in main.py
# Ensure the logger name matches the one used in main.py's setup_logging call
logger = logging.getLogger(settings.APP_NAME)

# Create an APIRouter instance
router = APIRouter()

# --- Dependency Injection ---
# def get_data_handler():
#     """Dependency function to get a DataHandler instance."""
#     try:
#         # Use the database path from the application settings
#         return DataHandler(db_path=settings.DB_PATH)
#     except Exception as e:
#         # Log the error and raise an HTTP exception if DataHandler fails to initialize
#         logger.error(f"Failed to initialize DataHandler: {e}", exc_info=True)
#         raise HTTPException(status_code=500, detail="Database handler initialization failed.")
def get_data_handler():
    try:
        # <<< FIX: 使用 DB_PATH_SQLITE >>>
        return DataHandler(db_path=settings.DB_PATH_SQLITE)
    except Exception as e:
        logger.error(f"Failed to initialize DataHandler: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"数据库处理程序初始化失败: {e}")

# --- API Endpoints ---

@router.get(
    "/scales",
    response_model=AvailableScalesResponse, # Use keyword argument for response_model
    tags=["Scales"]                          # Use keyword argument for tags
)
async def get_available_scales(dh: DataHandler = Depends(get_data_handler)):
    """
    Retrieves a list of all available scale types (code and name).
    """
    logger.info("Request received for available scales.")
    try:
        # Fetch scale types from the data handler
        scales = dh.get_all_scale_types() # This should return List[Dict] e.g. [{'code': 'SAS', 'name': '...'}, ...]

        # Convert the list of dictionaries to a list of Pydantic models
        scale_infos = [ScaleInfo(code=s["code"], name=s["name"]) for s in scales]

        # Return the response using the Pydantic response model
        return AvailableScalesResponse(scales=scale_infos)
    except Exception as e:
        # Log any unexpected errors during the process
        logger.error(f"Error fetching available scales: {e}", exc_info=True)
        # Raise a generic 500 error to the client
        raise HTTPException(status_code=500, detail="Failed to fetch scale types from database.")


@router.get(
    "/scales/{scale_code}/questions",
    response_model=ScaleQuestionsResponse, # Use keyword argument
    tags=["Scales"]                          # Use keyword argument
)
async def get_scale_questions(scale_code: str, dh: DataHandler = Depends(get_data_handler)):
    """
    Retrieves all questions for a specific scale based on its code.
    """
    logger.info(f"Request received for questions of scale: {scale_code}")
    try:
        # Load question data using the data handler
        # This method should return List[Dict] or None
        questions_data = dh.load_questions_by_type(scale_code)

        # Handle case where no questions are found for the given scale code
        if questions_data is None:
            logger.warning(f"No questions found for scale code: {scale_code}")
            raise HTTPException(status_code=404, detail=f"Scale with code '{scale_code}' not found or has no questions.")

        questions_list = []
        # Iterate through the raw question data from the database
        for q_data in questions_data:
            # --- Data Validation and Transformation ---
            # Ensure the basic structure of question data is present
            if not all(k in q_data for k in ('number', 'text', 'options')):
                logger.warning(f"Skipping question data due to missing keys: {q_data}")
                continue # Skip this malformed question data

            options_list = []
            options_data_from_db = q_data.get('options', [])

            # Ensure options data is a list before processing
            if not isinstance(options_data_from_db, list):
                logger.warning(f"Options data for Q{q_data['number']} is not a list, skipping options. Data: {options_data_from_db}")
            else:
                # Iterate through options for the current question
                for opt in options_data_from_db:
                    # Ensure option structure is correct
                    if not isinstance(opt, dict) or not all(k in opt for k in ('text', 'score')):
                        logger.warning(f"Skipping invalid option data for Q{q_data['number']}: {opt}")
                        continue # Skip malformed option data

                    text_val = opt['text']
                    score_val_raw = opt['score']
                    score_val_numeric = None

                    # Explicitly convert score to a numeric type (int or float)
                    try:
                        if isinstance(score_val_raw, (int, float)):
                            score_val_numeric = score_val_raw
                        elif isinstance(score_val_raw, str):
                            # Attempt conversion from string
                            try:
                                score_float = float(score_val_raw)
                                score_val_numeric = int(score_float) if score_float.is_integer() else score_float
                            except ValueError:
                                logger.error(f"Could not convert score string '{score_val_raw}' to number for Q{q_data['number']}, option '{text_val}'. Skipping option.")
                                continue # Skip this option if conversion fails
                        else:
                            # Handle unexpected score types
                            logger.warning(f"Unexpected type for score ({type(score_val_raw)}) for Q{q_data['number']}, option '{text_val}'. Using 0 as fallback.")
                            score_val_numeric = 0

                    except Exception as conv_err:
                         logger.error(f"Error converting score '{score_val_raw}' for Q{q_data['number']}, option '{text_val}': {conv_err}", exc_info=True)
                         continue # Skip option on unexpected conversion error

                    # Only create ScaleOption if score conversion was successful
                    if score_val_numeric is not None:
                        try:
                            # Create Pydantic model instance for the option
                            options_list.append(ScaleOption(text=str(text_val), score=score_val_numeric))
                        except Exception as pydantic_option_err:
                            # Catch potential errors during Pydantic model instantiation
                            logger.error(f"Pydantic error creating ScaleOption for Q{q_data['number']}, option '{text_val}': {pydantic_option_err}", exc_info=True)
                            continue # Skip option if it fails validation

            # Create Pydantic model instance for the question
            try:
                questions_list.append(ScaleQuestion(
                    number=int(q_data['number']), # Ensure number is integer
                    text=str(q_data['text']),    # Ensure text is string
                    options=options_list
                ))
            except Exception as pydantic_q_err:
                # Catch potential errors during Pydantic model instantiation
                logger.error(f"Pydantic error creating ScaleQuestion for Q{q_data.get('number', 'N/A')}: {pydantic_q_err}", exc_info=True)
                continue # Skip this question if it fails validation

        # Log successful processing and return the validated response
        logger.info(f"Successfully processed {len(questions_list)} questions for scale {scale_code}.")
        return ScaleQuestionsResponse(questions=questions_list)

    except HTTPException as http_exc:
        # Re-raise known HTTP exceptions (like 404)
        raise http_exc
    except Exception as e:
        # Log unexpected errors and return a generic 500 response
        logger.error(f"Unexpected error fetching/processing questions for scale {scale_code}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to fetch questions for scale {scale_code}.")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\routers\__init__.py

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\assessment.py
#评估提交相关
# app/schemas/assessment.py
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional

# --- 用于 POST /api/assessments/submit 的请求体 (部分数据将来自 Form) ---
# Pydantic 模型通常用于 JSON body，但 FastAPI 也能从 Form 字段映射
# 这里定义基础信息字段，方便验证和文档化，实际接收用 Form(...)
class BasicInfoSubmit(BaseModel):
    name: str = Field(..., description="姓名")
    gender: str = Field(..., description="性别")
    id_card: Optional[str] = Field(None, description="身份证号")
    age: int = Field(..., gt=0, description="年龄")
    occupation: Optional[str] = Field(None, description="职业")
    case_name: Optional[str] = Field(None, description="案件名称")
    case_type: Optional[str] = Field(None, description="案件类型")
    identity_type: Optional[str] = Field(None, description="人员身份")
    person_type: Optional[str] = Field(None, description="人员类型")
    marital_status: Optional[str] = Field(None, description="婚姻状况")
    children_info: Optional[str] = Field(None, description="子女情况")
    criminal_record: Optional[int] = Field(0, ge=0, le=1, description="有无犯罪前科 (0:无, 1:有)")
    health_status: Optional[str] = Field(None, description="健康情况")
    phone_number: Optional[str] = Field(None, description="手机号")
    domicile: Optional[str] = Field(None, description="归属地")
    # 注意：scale_type 和 scale_answers 会作为独立的 Form 字段传入

# --- 用于 POST /api/assessments/submit 的响应 ---
class AssessmentSubmitResponse(BaseModel):
    status: str = "success" # "success" or "error"
    message: str
    submission_id: Optional[int] = None # 成功时返回 ID
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\encyclopedia.py
# 文件: app/schemas/encyclopedia.py
from pydantic import BaseModel
from typing import List, Optional

class EncyclopediaEntry(BaseModel):
    """单个百科条目的结构"""
    category: str
    title: str
    content: str

class CategoriesResponse(BaseModel):
    """获取分类列表的响应"""
    categories: List[str]

class EntriesResponse(BaseModel):
    """获取条目列表的响应"""
    entries: List[EncyclopediaEntry]
    # 可以添加分页信息 (如果需要)
    # total: Optional[int] = None
    # page: Optional[int] = None
    # size: Optional[int] = None

# 可以复用 EncyclopediaEntry 作为随机条目的响应，或者定义一个更简单的
# class RandomTipResponse(BaseModel):
#     tip: str
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\report.py
#报告相关
# app/schemas/report.py
from pydantic import BaseModel
from typing import Optional, Dict, Any
from datetime import datetime

class ReportData(BaseModel):
    """报告详情的响应模型"""
    id: int
    image_path: Optional[str] = None
    subject_name: Optional[str] = None
    age: Optional[int] = None
    gender: Optional[str] = None
    questionnaire_type: Optional[str] = None
    questionnaire_data: Optional[Dict[str, Any]] = None # 解析后的 JSON
    report_text: Optional[str] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    id_card: Optional[str] = None
    occupation: Optional[str] = None
    case_name: Optional[str] = None
    case_type: Optional[str] = None
    identity_type: Optional[str] = None
    person_type: Optional[str] = None
    marital_status: Optional[str] = None
    children_info: Optional[str] = None
    criminal_record: Optional[int] = None
    health_status: Optional[str] = None
    phone_number: Optional[str] = None
    domicile: Optional[str] = None
    # ... 可以添加未来需要的其他报告字段 ...

class ReportResponse(BaseModel):
    """获取报告的 API 响应"""
    report: Optional[ReportData] = None
    error: Optional[str] = None # 如果报告未找到或处理中
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\scale.py
# 量表相关
# app/schemas/scale.py
from pydantic import BaseModel
from typing import List, Dict, Any, Optional

class ScaleOption(BaseModel):
    """量表问题的选项"""
    text: str
    score: int | float # 分数可能是整数或浮点数

class ScaleQuestion(BaseModel):
    """量表问题结构"""
    number: int
    text: str
    options: List[ScaleOption]

class ScaleInfo(BaseModel):
    """量表基本信息 (用于列表显示)"""
    code: str # 量表代码 (e.g., 'SAS', 'Personality')
    name: str # 量表显示名称

# --- 用于 /api/scales/{scale_code}/questions 的响应 ---
class ScaleQuestionsResponse(BaseModel):
    questions: Optional[List[ScaleQuestion]] = None
    error: Optional[str] = None # 如果找不到问题，可以返回错误信息

# --- 用于 /api/scales 的响应 ---
class AvailableScalesResponse(BaseModel):
    scales: List[ScaleInfo]
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\token.py
# app/schemas/token.py
from pydantic import BaseModel
from typing import Optional

class Token(BaseModel):
    """响应模型：登录成功后返回给客户端的令牌"""
    access_token: str
    token_type: str = "bearer" # 标准 OAuth2 类型

class TokenData(BaseModel):
    """内部模型：JWT 令牌载荷 (Payload) 的数据结构"""
    username: Optional[str] = None
    # 你未来可以在这里添加其他需要存储在令牌中的信息，
    # 例如用户 ID 或角色/权限 (scopes)
    # scopes: List[str] = []
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\user.py
# app/schemas/user.py (示例)
from pydantic import BaseModel, EmailStr, Field
from typing import Optional

class UserBase(BaseModel):
    username: str = Field(..., min_length=3, max_length=50)
    email: Optional[EmailStr] = None
    full_name: Optional[str] = Field(None, max_length=100)
    is_active: Optional[bool] = True
    is_superuser: Optional[bool] = False

class UserCreate(UserBase):
    password: str = Field(..., min_length=6) # 创建时接收明文密码

class UserUpdate(UserBase):
    password: Optional[str] = Field(None, min_length=6) # 更新时可选密码

# 用于从数据库读取用户数据的基础模式
class UserInDBBase(UserBase):
    id: int
    # Pydantic V2 使用 from_attributes 替代 orm_mode
    class Config:
        from_attributes = True

# API 返回给客户端的用户信息模式 (不包含密码)
class User(UserInDBBase):
    pass

# 内部使用的用户数据模式 (包含哈希密码)
class UserInDB(UserInDBBase):
    hashed_password: str
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\__init__.py
# app/schemas/__init__.py

# 导入用户相关的 Schemas
from .user import User, UserCreate, UserUpdate, UserInDB, UserBase

# 导入认证相关的 Schemas
from .token import Token, TokenData

# 导入量表相关的 Schemas
from .scale import ScaleOption, ScaleQuestion, ScaleInfo, ScaleQuestionsResponse, AvailableScalesResponse

# 导入评估提交相关的 Schemas
from .assessment import BasicInfoSubmit, AssessmentSubmitResponse

# 导入报告相关的 Schemas
from .report import ReportData, ReportResponse

# 导入百科相关的 Schemas
from .encyclopedia import EncyclopediaEntry, CategoriesResponse, EntriesResponse 

# (可选) __all__ 列表
__all__ = [
    "User", "UserCreate", "UserUpdate", "UserInDB", "UserBase",
    "Token", "TokenData", # <--- 添加 Token 相关
    "ScaleOption", "ScaleQuestion", "ScaleInfo", "ScaleQuestionsResponse", "AvailableScalesResponse",
    "BasicInfoSubmit", "AssessmentSubmitResponse",
    "ReportData", "ReportResponse",
    "EncyclopediaEntry", "CategoriesResponse", "EntriesResponse",
]
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\tasks\analysis.py
# app/tasks/analysis.py
import logging
import os
import sys
import asyncio

# --- 路径设置 (保持不变) ---
TASK_DIR = os.path.dirname(os.path.abspath(__file__))
APP_ROOT_FROM_TASK = os.path.dirname(TASK_DIR) # app/
PROJECT_ROOT_FROM_TASK = os.path.dirname(APP_ROOT_FROM_TASK) # PsychologyAnalysis/
if PROJECT_ROOT_FROM_TASK not in sys.path:
    sys.path.insert(0, PROJECT_ROOT_FROM_TASK)
    print(f"[Celery Task Init] 已添加 {PROJECT_ROOT_FROM_TASK} 到 sys.path 以供 worker 使用")

# --- 核心导入 ---
from app.core.celery_app import celery_app
from app.core.config import settings
# --- 导入异步 DB 会话和 CRUD ---
from app.db.session import AsyncSessionLocal
# *** 直接导入 assessment 的 CRUD 模块 ***
from app.crud import assessment as crud_assessment

# --- 导入处理逻辑 (现在是 generate_report_content) ---
try:
    # *** 导入修改后的核心处理函数 ***
    from src.ai_utils import generate_report_content
    # ... (日志设置逻辑保持不变) ...
    try:
        from src.utils import setup_logging
        WORKER_LOGGER_NAME = f"{settings.APP_NAME}_Worker"
        # 重新配置日志，以防 worker 启动时未完全初始化
        setup_logging(log_level_str=settings.LOG_LEVEL,
                      log_dir_name=os.path.basename(settings.LOGS_DIR),
                      logger_name=WORKER_LOGGER_NAME)
        logger = logging.getLogger(WORKER_LOGGER_NAME)
        print(f"[Celery Task Init] Logger '{WORKER_LOGGER_NAME}' 配置完成。")
    except Exception as log_setup_err:
        print(f"[Celery Task Init] 日志设置错误: {log_setup_err}. 使用基础日志记录器。")
        logger = logging.getLogger(__name__) # 使用默认 logger
        if not logger.hasHandlers(): # 确保至少有基本配置
            logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

except ImportError as e:
     print(f"[Celery Task Init] CRITICAL: 无法导入 src 模块: {e}", exc_info=True)
     # 定义虚拟函数以便任务优雅失败
     generate_report_content = None
     logger = logging.getLogger(__name__) # 使用基础 logger
     if not logger.hasHandlers():
         logging.basicConfig(level=logging.INFO)
     logger.critical(f"CRITICAL: 核心处理函数 (generate_report_content) 导入失败: {e}")


@celery_app.task(bind=True, name='tasks.run_ai_analysis')
def run_ai_analysis(self, assessment_id: int):
    """
    Celery 任务：异步运行 AI 分析并更新报告。
    1. 异步从数据库加载评估数据。
    2. 调用核心逻辑生成报告文本。
    3. 异步更新数据库中的报告文本。
    """
    # ---- 函数体开始 ----
    global logger
    # 再次确保 logger 可用
    if logger is None:
        print("CRITICAL ERROR: Logger 在 run_ai_analysis 任务中不可用!")
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__) # 最后手段

    task_id_str = f"[Celery Task {self.request.id}]" # 用于日志追踪
    logger.info(f"{task_id_str} 收到任务，评估 ID: {assessment_id}")

    # 检查核心函数是否已加载
    if generate_report_content is None:
         logger.error(f"{task_id_str} 核心处理函数 (generate_report_content) 未加载，中止任务 ID {assessment_id}。")
         # 返回失败状态给 Celery，让调用者知道任务失败
         # 可以通过 task.get() 获取这个结果
         return {"status": "failure", "assessment_id": assessment_id, "error": "核心处理函数导入失败"}

    async def _run_analysis_async():
        """内部异步函数，封装异步操作"""
        nonlocal assessment_id # 确保能访问外部的 assessment_id
        report_text_to_save = "处理失败：未知错误" # 默认错误信息
        final_status = "failure" # 默认状态
        error_detail = None # 存储错误摘要

        # 使用异步会话管理器
        async with AsyncSessionLocal() as session:
            try:
                # 1. 异步加载评估数据
                logger.info(f"{task_id_str} 正在异步加载评估数据 ID: {assessment_id}")
                assessment_record = await crud_assessment.get(db=session, id=assessment_id)

                if not assessment_record:
                    logger.error(f"{task_id_str} 无法找到评估记录 ID: {assessment_id}")
                    error_detail = f"评估记录 ID {assessment_id} 未找到"
                    # 不需要更新数据库，因为记录不存在
                    # 直接返回失败结果
                    return {"status": "failure", "assessment_id": assessment_id, "error": error_detail}

                # 将 SQLAlchemy 模型对象转换为字典，传递给核心逻辑
                # 注意：这是一种简化方式，可能不适用于所有复杂关系。
                # Pydantic 模型验证可能是更健壮的选择。
                submission_data = {}
                for column in assessment_record.__table__.columns:
                    submission_data[column.name] = getattr(assessment_record, column.name)

                logger.debug(f"{task_id_str} 已加载数据，准备调用核心处理函数，ID: {assessment_id}")

                # 2. 调用核心处理逻辑生成报告文本
                generated_text = generate_report_content(
                    submission_data=submission_data,
                    config=settings.model_dump(), # 传递 Pydantic Settings 的字典表示
                    task_logger=logger
                )

                # 检查报告生成结果
                if generated_text is None:
                    logger.error(f"{task_id_str} 核心处理函数 (generate_report_content) 返回 None，ID: {assessment_id}")
                    report_text_to_save = "错误：报告生成意外返回空"
                    error_detail = "报告生成返回空"
                    # final_status 保持 'failure'
                elif "错误" in generated_text or "Error" in generated_text or "失败" in generated_text:
                     logger.error(f"{task_id_str} 核心处理函数返回错误信息，ID {assessment_id}: {generated_text[:200]}...")
                     report_text_to_save = generated_text # 保存具体的错误信息
                     error_detail = f"AI 处理失败: {generated_text[:150]}" # 记录简短错误摘要
                     # final_status 保持 'failure'
                else:
                    # 报告生成成功
                    logger.info(f"{task_id_str} 报告内容生成成功，ID: {assessment_id}")
                    report_text_to_save = generated_text
                    final_status = "success"
                    error_detail = None # 清除错误详情

                # 3. 异步更新数据库 (无论成功失败，都尝试更新 report_text)
                logger.info(f"{task_id_str} 尝试异步更新报告文本 (或错误信息) 到数据库，ID: {assessment_id}")
                # 确保 report_text_to_save 是字符串
                report_to_save_str = str(report_text_to_save)

                updated_record = await crud_assessment.update_report_text(
                    db=session,
                    assessment_id=assessment_id,
                    report_text=report_to_save_str
                )

                if not updated_record:
                     # 这通常意味着 ID 找不到了，但应该在加载步骤就失败了
                     logger.error(f"{task_id_str} 尝试更新数据库时记录 ID {assessment_id} 未找到！")
                     # 如果之前的状态是 success，现在也应该标记为 failure
                     if final_status == "success": final_status = "failure"
                     # 用这个错误覆盖之前的错误，DB 更新失败更关键
                     error_detail = f"数据库更新失败 (更新时 ID: {assessment_id} 未找到)"
                else:
                    logger.info(f"{task_id_str} 数据库报告文本更新成功，ID: {assessment_id}")
                    # session.commit() 和 session.refresh() 应该由 CRUD 函数处理

            except Exception as e:
                # 捕获 _run_analysis_async 内部未处理的异常
                logger.error(f"{task_id_str} 在 _run_analysis_async 中发生意外错误，ID {assessment_id}: {e}", exc_info=True)
                error_message = f"任务执行失败: {type(e).__name__} - {str(e)}"
                report_text_to_save = error_message # 准备将此错误写入 DB
                final_status = "failure"
                error_detail = error_message[:150] # 记录摘要
                # 尝试将这个最终错误写入数据库（仍在 async with session 内）
                try:
                    await crud_assessment.update_report_text(
                         db=session,
                         assessment_id=assessment_id,
                         report_text=report_text_to_save[:2000] # 截断长错误
                     )
                    logger.info(f"{task_id_str} 已将最终错误信息写入数据库，ID {assessment_id}")
                except Exception as db_err_on_fail:
                    logger.error(f"{task_id_str} 在失败处理中写入数据库错误信息也失败了，ID {assessment_id}: {db_err_on_fail}")

        # --- async with session 结束 ---

        # 根据最终状态返回结果字典
        if final_status == "success":
             return {"status": "success", "assessment_id": assessment_id, "report_length": len(report_text_to_save)}
        else:
             # 确保 error_detail 有值
             if not error_detail: error_detail = "处理过程中发生未知错误"
             return {"status": "failure", "assessment_id": assessment_id, "error": error_detail}

    # --- 在同步的 Celery 任务中运行异步代码块 ---
    try:
        # 使用 asyncio.run() 来执行异步函数
        # 这会创建一个新的事件循环来运行 _run_analysis_async
        result = asyncio.run(_run_analysis_async())
        return result
    except RuntimeError as e:
        # 处理可能由 Celery 事件循环引起的嵌套循环错误
        logger.warning(f"{task_id_str} Asyncio 运行时错误 (可能事件循环冲突): {e}。")
        error_msg = f"任务失败: Asyncio 运行时错误 - {e}"
        # 尝试同步方式记录错误（作为最后的手段）
        try:
             # 延迟导入，仅在需要时导入同步 Handler
             from src.data_handler import DataHandler
             sync_db_path = settings.DB_PATH_SQLITE # 从配置获取同步路径
             sync_handler = DataHandler(db_path=sync_db_path)
             sync_handler.update_report_text(assessment_id, error_msg[:2000]) # 截断
             logger.info(f"{task_id_str} 已尝试同步记录 Asyncio 错误到数据库，ID {assessment_id}")
        except Exception as sync_db_err:
             logger.error(f"{task_id_str} 同步记录 Asyncio 错误到数据库失败，ID {assessment_id}: {sync_db_err}")

        return {"status": "failure", "assessment_id": assessment_id, "error": error_msg}
    except Exception as task_exec_err:
        # 捕获 asyncio.run() 本身或其他同步代码可能抛出的错误
        logger.critical(f"{task_id_str} Celery 任务执行期间发生顶层错误，ID {assessment_id}: {task_exec_err}", exc_info=True)
        error_msg = f"任务执行错误: {type(task_exec_err).__name__} - {str(task_exec_err)}"
        # 同样尝试同步记录错误
        try:
             from src.data_handler import DataHandler
             sync_db_path = settings.DB_PATH_SQLITE
             sync_handler = DataHandler(db_path=sync_db_path)
             sync_handler.update_report_text(assessment_id, error_msg[:2000]) # 截断
             logger.info(f"{task_id_str} 已尝试同步记录顶层错误到数据库，ID {assessment_id}")
        except Exception as sync_db_err:
             logger.error(f"{task_id_str} 同步记录顶层错误到数据库失败，ID {assessment_id}: {sync_db_err}")

        return {"status": "failure", "assessment_id": assessment_id, "error": error_msg}

# ---- run_ai_analysis 函数体结束 ----
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\tasks\__init__.py

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\ai_utils.py
# src/ai_utils.py
import os
import json
# 移除了 import sqlite3
from datetime import datetime
import sys
import logging

# --- 路径设置和模块导入 (保持不变) ---
SRC_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT_FROM_SRC = os.path.dirname(SRC_DIR)
if PROJECT_ROOT_FROM_SRC not in sys.path:
    sys.path.insert(0, PROJECT_ROOT_FROM_SRC)

try:
    # 移除了 from .data_handler import DataHandler 的导入，因为此文件不再直接使用它
    from .image_processor import ImageProcessor
    from .report_generator import ReportGenerator
    print("[ai_utils] 成功相对导入 ImageProcessor 和 ReportGenerator。")
except ImportError:
    try:
        # 移除了 from data_handler import DataHandler 的导入
        from image_processor import ImageProcessor
        from report_generator import ReportGenerator
        print("[ai_utils] 成功直接导入 ImageProcessor 和 ReportGenerator (后备)。")
    except ImportError as e:
        print(f"[ai_utils] CRITICAL ERROR: 无法导入必要的同级模块: {e}", file=sys.stderr)
        raise e

# --- calculate_score_and_interpret 函数 (保持不变) ---
def calculate_score_and_interpret(scale_type, scale_answers, task_logger=None):
    """
    根据量表答案计算分数，并根据量表类型提供基本解释。
    使用提供的 logger 实例进行日志记录。

    Args:
        scale_type (str): 表示量表类型的代码 (例如, 'SAS', 'SDS').
        scale_answers (dict): 问题答案字典 { 'q1': 'score_value', ... }.
        task_logger (logging.Logger, optional): 要使用的 Logger 实例. 默认为 None.

    Returns:
        tuple: (calculated_score, interpretation_string)
    """
    current_logger = task_logger or logging.getLogger(__name__)
    if task_logger is None:
        current_logger.warning("未向 calculate_score_and_interpret 提供 task_logger，使用默认日志记录器。")
        # 确保有基本的日志处理器
        if not logging.getLogger().hasHandlers():
            logging.basicConfig(level=logging.INFO)

    if not scale_answers:
        current_logger.info(f"量表类型 '{scale_type}' 的答案为空。")
        return 0, "无量表答案"

    calculated_score = 0
    try:
        # 确保值存在且可以转换为数字
        valid_scores = []
        for key, value in scale_answers.items():
            if value is not None:
                try:
                    # 尝试转换为浮点数，然后转整数（如果可能）
                    score_float = float(value)
                    valid_scores.append(int(score_float) if score_float.is_integer() else score_float)
                except (ValueError, TypeError):
                    current_logger.warning(f"无法将答案 '{key}':'{value}' 转换为数字，已忽略。")

        if not valid_scores:
             current_logger.warning(f"在类型 {scale_type} 的量表答案中未找到有效的数字分数: {scale_answers}")
             return 0, f"量表 '{scale_type}' 无有效得分项"
        calculated_score = sum(valid_scores)
        current_logger.debug(f"计算得到的分数: {calculated_score} (来自: {valid_scores})")
    except Exception as e:
        current_logger.error(f"从答案 {scale_answers} 计算分数时出错: {e}", exc_info=True)
        return "计算错误", f"分数计算出错: {e}"

    interpretation = f"量表 '{scale_type}' 总得分: {calculated_score}."
    current_logger.info(f"开始为量表 '{scale_type}' (得分: {calculated_score}) 生成解释。")

    try:
        # --- 量表解释逻辑 (保持不变) ---
        if scale_type == 'SAS': # Anxiety Self-Rating Scale
            standard_score = int(calculated_score * 1.25)
            interpretation = f"量表 '{scale_type}' 原始得分: {calculated_score}, 标准分: {standard_score}."
            if standard_score >= 70: interpretation += " (重度焦虑水平)"
            elif standard_score >= 60: interpretation += " (中度焦虑水平)"
            elif standard_score >= 50: interpretation += " (轻度焦虑水平)"
            else: interpretation += " (焦虑水平在正常范围)"
        elif scale_type == 'SDS': # Depression Self-Rating Scale
             standard_score = int(calculated_score * 1.25)
             interpretation = f"量表 '{scale_type}' 原始得分: {calculated_score}, 标准分: {standard_score}."
             if standard_score >= 73: interpretation += " (重度抑郁水平)" # 注意：Zung SDS 通常用 70, 60, 50 作为界限，这里按量表文件给出的界限
             elif standard_score >= 63: interpretation += " (中度抑郁水平)"
             elif standard_score >= 53: interpretation += " (轻度抑郁水平)"
             else: interpretation += " (抑郁水平在正常范围)"
        elif scale_type == 'ParentChild':
            if calculated_score >= 80: interpretation += " (亲子关系非常和谐)"
            elif calculated_score >= 60: interpretation += " (亲子关系良好)"
            else: interpretation += " (亲子关系可能存在挑战，建议关注)"
        elif scale_type == 'Personality':
            # 性格测试通常需要更复杂的解释，可能基于得分范围
            if calculated_score >= 101: interpretation += " (倾向：积极热情)"
            elif calculated_score >= 90: interpretation += " (倾向：领导人特质)"
            elif calculated_score >= 79: interpretation += " (倾向：感性)"
            elif calculated_score >= 60: interpretation += " (倾向：理性&淡定)"
            elif calculated_score >= 40: interpretation += " (倾向：双重&孤寂)"
            else: interpretation += " (倾向：现实&自我)"
            interpretation += " (具体解释需参考原始量表得分范围)"
        elif scale_type == 'InterpersonalRelationship':
            if calculated_score <= 8: interpretation += " (人际关系困扰较少)"
            elif calculated_score <= 14: interpretation += " (人际关系存在一定困扰)"
            else: interpretation += " (人际关系困扰较严重)"
        elif scale_type == 'EmotionalStability':
             if calculated_score <= 20: interpretation += " (情绪稳定，自信心强)"
             elif calculated_score <= 40: interpretation += " (情绪基本稳定，但可能较为深沉或消极)"
             else: interpretation += " (情绪不稳定，可能需要关注)"
        elif scale_type == 'HAMD24':
            if calculated_score >= 36: interpretation += " (重度抑郁)"
            elif calculated_score >= 21: interpretation += " (肯定有抑郁)"
            elif calculated_score >= 8: interpretation += " (可能有抑郁)"
            else: interpretation += " (无抑郁症状)"
        # EPQ85 需要分别计算 P, E, N, L 四个维度的得分，这里仅做标记
        elif scale_type == 'EPQ85':
             interpretation = f"艾森克人格问卷 (EPQ-85)，总得分无直接意义，需分析 P, E, N, L 各维度得分。"
             # 实际分析需要在 generate_report_content 中单独处理 EPQ85
        else:
            current_logger.warning(f"未找到量表类型 '{scale_type}' 的特定解释规则。")
            interpretation += " (无特定解释规则)"

    except Exception as e:
        current_logger.error(f"量表 {scale_type} 解释过程中出错: {e}", exc_info=True)
        interpretation += " (解释规则应用出错)"

    current_logger.info(f"量表 '{scale_type}' 解释完成: '{interpretation}'")
    return calculated_score, interpretation


# --- 重命名并重构核心函数 ---
def generate_report_content(submission_data: dict, config: dict, task_logger: logging.Logger) -> str:
    """
    根据传入的评估数据和配置，生成报告文本。不再直接操作数据库。

    Args:
        submission_data (dict): 从数据库异步加载的评估数据字典.
        config (dict): 应用程序配置字典 (来自 settings.model_dump()).
        task_logger (logging.Logger): 用于记录日志的 logger 实例.

    Returns:
        str: 生成的报告文本或错误信息字符串.
    """
    logger = task_logger
    submission_id = submission_data.get("id", "未知ID")
    logger.info(f"开始为评估 ID: {submission_id} 生成报告内容")

    # --- 提取数据 ---
    image_filename = submission_data.get('image_path') # 这是存储在 DB 中的相对路径或文件名
    image_full_path = None
    if image_filename:
        # 从配置中获取上传目录
        uploads_dir = config.get("UPLOADS_DIR")
        if uploads_dir and os.path.isdir(uploads_dir):
            image_full_path = os.path.join(uploads_dir, image_filename)
            logger.info(f"将使用的图片文件路径: {image_full_path}")
        elif not uploads_dir:
            logger.warning(f"配置中未找到 UPLOADS_DIR，无法定位图片文件: {image_filename}")
        else: # uploads_dir 存在但不是目录
             logger.warning(f"配置的 UPLOADS_DIR '{uploads_dir}' 不是有效目录，无法定位图片文件: {image_filename}")

    scale_type = submission_data.get('questionnaire_type')
    scale_answers_json = submission_data.get('questionnaire_data')
    basic_info = {k: submission_data.get(k) for k in [
        "subject_name", "gender", "id_card", "age", "occupation", "case_name",
        "case_type", "identity_type", "person_type", "marital_status",
        "children_info", "criminal_record", "health_status", "phone_number", "domicile"
    ]}
    # 确保 criminal_record 有默认值 0 (无)
    basic_info.setdefault('criminal_record', 0)
    # 确保 'name' 键存在，用于报告模板
    basic_info['name'] = basic_info.get('subject_name', '未知')

    logger.debug(f"用于报告生成的基础信息 (ID {submission_id}): {basic_info}")

    # --- 准备 AI 配置 ---
    ai_config = config.copy()
    if 'api_key' not in ai_config and 'DASHSCOPE_API_KEY' in ai_config:
        logger.info("复制 DASHSCOPE_API_KEY 到 'api_key' 以供 AI 处理器使用。")
        ai_config['api_key'] = ai_config['DASHSCOPE_API_KEY']
    elif 'api_key' not in ai_config:
        logger.error(f"CRITICAL: AI 处理器的 API Key 未在配置中找到! (ID: {submission_id})")
        return "错误：AI 服务配置不完整 (缺少 API Key)"

    # --- 处理图片 ---
    image_description = "未提供图片"
    if image_full_path:
        if os.path.exists(image_full_path):
            logger.info(f"开始处理图片: {image_full_path}")
            try:
                image_processor = ImageProcessor(ai_config)
                image_description = image_processor.process_image(image_full_path)
                logger.info(f"图片描述生成成功 (ID {submission_id})。描述片段: {image_description[:100]}...")
            except FileNotFoundError:
                 logger.error(f"图片文件在处理时未找到: {image_full_path}")
                 image_description = "图片文件未找到"
            except Exception as img_err:
                logger.error(f"图片处理失败 (ID {submission_id}): {img_err}", exc_info=True)
                image_description = f"图片处理错误: {img_err}"
        else:
            logger.warning(f"图片路径存在但文件在处理时未找到: {image_full_path}")
            image_description = "图片文件未找到"
    else:
        logger.info(f"评估 ID {submission_id} 未提供图片路径。")

    # --- 处理量表数据 ---
    scale_answers = None
    calculated_score = 0
    scale_interpretation = "无量表数据"
    if scale_type and scale_answers_json:
        logger.info(f"开始处理量表数据，类型: {scale_type} (ID {submission_id})")
        try:
            scale_answers = json.loads(scale_answers_json) # 期望是字典 {'q1': 'score', ...}
            if isinstance(scale_answers, dict):
                 # 特殊处理 EPQ85，因为它需要计算四个维度
                 if scale_type == 'EPQ85':
                      # TODO: 实现 EPQ85 的计分逻辑
                      # 这需要访问 EPQ85 的 JSON 文件来获取计分规则
                      # 假设有一个辅助函数 `calculate_epq85_scores(scale_answers)`
                      # 返回 {'P': score_p, 'E': score_e, 'N': score_n, 'L': score_l, 'interpretation': '...'}
                      # epq_results = calculate_epq85_scores(scale_answers, logger)
                      # calculated_score = epq_results # 或者只用某个主维度
                      # scale_interpretation = epq_results['interpretation']
                      logger.warning(f"EPQ85 量表计分逻辑尚未在此函数中完全实现 (ID: {submission_id})。")
                      calculated_score = "N/A" # 标记为不适用总分
                      scale_interpretation = "EPQ85 量表结果需单独分析各维度。"
                 else:
                     # 对于其他量表，使用通用计分函数
                     calculated_score, scale_interpretation = calculate_score_and_interpret(
                         scale_type, scale_answers, task_logger=logger
                     )
                 logger.info(f"量表处理完成: Score={calculated_score}, Interpretation='{scale_interpretation}' (ID {submission_id})")
            else:
                 logger.error(f"解析后的量表答案不是字典类型 (ID {submission_id}): {type(scale_answers)}")
                 scale_interpretation = "量表答案格式错误 (非字典)"
                 scale_answers = None # 重置为 None
        except json.JSONDecodeError as json_err:
            logger.error(f"量表答案 JSON 解析失败 (ID {submission_id}): {json_err}. JSON: {scale_answers_json[:200]}...")
            scale_interpretation = "量表答案格式错误 (JSON 解析失败)"
            scale_answers = None
        except Exception as scale_err:
            logger.error(f"量表数据处理失败 (ID {submission_id}): {scale_err}", exc_info=True)
            scale_interpretation = f"量表处理错误: {scale_err}"
            scale_answers = None
    elif scale_type:
         logger.warning(f"提供了量表类型 '{scale_type}' 但无答案数据 (ID {submission_id}).")
         scale_interpretation = f"量表 '{scale_type}' 未提供答案"
    else:
         logger.info(f"评估 ID {submission_id} 未提供量表类型.")

    # --- 调用 LLM 生成报告 ---
    logger.info(f"开始调用 LLM 生成报告 (ID {submission_id})")
    final_report_text = None
    try:
        report_generator = ReportGenerator(ai_config)
        final_report_text = report_generator.generate_report(
             description=image_description,
             questionnaire=scale_answers, # 传递解析后的字典或 None
             subject_info=basic_info,
             questionnaire_type=scale_type,
             score=calculated_score, # 可能是数字，也可能是 "N/A" (如 EPQ)
             scale_interpretation=scale_interpretation
         )
        if final_report_text is None:
             # ReportGenerator 应该返回字符串，即使是错误信息
             raise ValueError("报告生成器意外返回了 None")
        logger.info(f"LLM 报告生成成功 (ID {submission_id}, 长度: {len(final_report_text)})")

    except Exception as report_err:
        logger.error(f"LLM 报告生成失败 (ID {submission_id}): {report_err}", exc_info=True)
        # 返回具体的错误信息，而不是仅仅标记失败
        final_report_text = f"报告生成错误: {type(report_err).__name__} - {str(report_err)}"

    # --- 返回最终文本 ---
    # 注意：此函数不再负责数据库更新
    return final_report_text

# 移除旧的 process_data_and_generate_report_sync 函数定义（如果它还存在）
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\api_tester.py
# 文件路径: src/api_tester.py
import base64
import os
import sqlite3
import json
from openai import OpenAI
from src.utils import setup_logging
from src.image_processor import ImageProcessor
from src.report_generator import ReportGenerator

class APITester:
    def __init__(self, config):
        """初始化 API 测试模块"""
        self.logger = setup_logging()
        self.config = config
        
        # 统一客户端配置
        self.api_key = config["api_key"]
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
        )
        
        # 模型名称
        self.vision_model = "qwen-vl-max-latest"
        self.text_model = config["text_model"]  # 从配置中读取，例如 "qwen-plus"
        
        # 初始化数据库路径
        self.db_path = "psychology_analysis.db"
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        # 初始化测试用模块
        self.image_processor = ImageProcessor(config)
        self.report_generator = ReportGenerator(config)

    def setup_test_data(self):
        """为测试准备数据，插入测试用图片路径和量表数据"""
        self.logger.info("准备测试数据...")
        
        # 连接数据库
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # 清空现有数据（仅用于测试）
            cursor.execute("DELETE FROM analysis_data")
            
            # 插入测试数据
            test_image_path = os.path.join(self.project_root, "input", "images", "TestPic.jpg")
            subject_info = {"name": "测试用户", "age": 20, "gender": "男"}
            questionnaire_data = {"q1": "yes", "q2": "no", "q3": "sometimes"}
            
            cursor.execute('''
                INSERT INTO analysis_data (image_path, subject_name, age, gender, questionnaire_data)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                test_image_path,
                subject_info["name"],
                subject_info["age"],
                subject_info["gender"],
                json.dumps(questionnaire_data)
            ))
            
            # 插入只有量表数据的记录（无对应图片）
            cursor.execute('''
                INSERT INTO analysis_data (image_path, subject_name, age, gender, questionnaire_data)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                "no_image.jpg",  # 虚拟图片路径
                "无图片用户",
                25,
                "女",
                json.dumps({"q1": "no", "q2": "yes", "q3": "often"})
            ))
            
            conn.commit()
        self.logger.info("测试数据准备完成")

    def cleanup_test_data(self):
        """清理测试数据"""
        self.logger.info("清理测试数据...")
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("DELETE FROM analysis_data")
            conn.commit()
        self.logger.info("测试数据清理完成")

    def test_vision_api(self):
        """测试图像处理模型的 API 连通性"""
        self.logger.info("开始测试图像处理模型 API...")
        
        # 使用 input/images/TestPic.jpg 进行测试
        test_image_path = os.path.join(self.project_root, "input", "images", "TestPic.jpg")
        
        if not os.path.exists(test_image_path):
            self.logger.error(f"测试图片 {test_image_path} 不存在，请确保文件已放置在正确位置")
            return False
        
        # 将图片转为 base64 编码
        with open(test_image_path, "rb") as image_file:
            image_base64 = base64.b64encode(image_file.read()).decode("utf-8")

        # 构造消息内容
        messages = [
            {"role": "system", "content": [{"type": "text", "text": "You are a helpful assistant."}]},
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}},
                    {"type": "text", "text": "请简要描述这张图片的内容，用于API测试。"}
                ]
            }
        ]

        try:
            completion = self.client.chat.completions.create(
                model=self.vision_model,
                messages=messages,
            )
            description = completion.choices[0].message.content
            self.logger.info("图像模型 API 测试成功！")
            self.logger.info(f"测试图片描述: {description}")
            return True
        except Exception as e:
            self.logger.error(f"图像模型 API 测试失败: {str(e)}")
            return False

    def test_text_api(self):
        """测试文本生成模型的 API 连通性"""
        self.logger.info("开始测试文本生成模型 API...")
        
        # 构造测试消息
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "这是一个API连通性测试，请回复‘测试成功’。"}
        ]

        try:
            completion = self.client.chat.completions.create(
                model=self.text_model,
                messages=messages,
            )
            text_output = completion.choices[0].message.content
            self.logger.info("文本生成模型 API 测试成功！")
            self.logger.info(f"测试输出: {text_output}")
            return True
        except Exception as e:
            self.logger.error(f"文本生成模型 API 测试失败: {str(e)}")
            return False

    def test_report_with_questionnaire_only(self):
        """测试只有量表数据时是否能生成报告"""
        self.logger.info("开始测试只有量表数据时的报告生成...")
        
        # 从数据库加载只有量表数据的记录
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT subject_name, age, gender, questionnaire_data FROM analysis_data WHERE image_path = ?", ("no_image.jpg",))
            result = cursor.fetchone()
            if not result:
                self.logger.error("未找到只有量表数据的测试记录")
                return False
            
            subject_info = {"name": result[0], "age": result[1], "gender": result[2]}
            questionnaire_data = json.loads(result[3]) if result[3] else None
            
            if not questionnaire_data:
                self.logger.error("量表数据为空，无法生成报告")
                return False

        # 使用空描述生成报告
        description = "无图片输入，仅基于量表数据生成报告"
        try:
            report = self.report_generator.generate_report(description, questionnaire_data, subject_info)
            self.logger.info("仅使用量表数据生成报告成功！")
            self.logger.info(f"生成的报告片段: {report[:100]}...")  # 只打印前100个字符
            return True
        except Exception as e:
            self.logger.error(f"仅使用量表数据生成报告失败: {str(e)}")
            return False

    def test_full_pipeline(self):
        """测试完整流程：图像 + 量表数据"""
        self.logger.info("开始测试完整流程（图像 + 量表数据）...")
        
        # 使用 TestPic.jpg 进行测试
        test_image_path = os.path.join(self.project_root, "input", "images", "TestPic.jpg")
        
        if not os.path.exists(test_image_path):
            self.logger.error(f"测试图片 {test_image_path} 不存在，请确保文件已放置在正确位置")
            return False
        
        # 步骤1: 图像识别
        try:
            description = self.image_processor.process_image(test_image_path)
            self.logger.info("图像描述生成成功")
            self.logger.info(f"描述片段: {description[:100]}...")  # 只打印前100个字符
        except Exception as e:
            self.logger.error(f"图像描述生成失败: {str(e)}")
            return False
        
        # 步骤2: 从数据库加载量表数据
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT subject_name, age, gender, questionnaire_data FROM analysis_data WHERE image_path = ?", (test_image_path,))
            result = cursor.fetchone()
            if not result:
                self.logger.error(f"图片 {test_image_path} 无对应量表数据")
                return False
            
            subject_info = {"name": result[0], "age": result[1], "gender": result[2]}
            questionnaire_data = json.loads(result[3]) if result[3] else None
            
            if not questionnaire_data:
                self.logger.error("量表数据为空，无法生成报告")
                return False

        # 步骤3: 生成报告
        try:
            report = self.report_generator.generate_report(description, questionnaire_data, subject_info)
            self.logger.info("完整流程生成报告成功！")
            self.logger.info(f"生成的报告片段: {report[:100]}...")  # 只打印前100个字符
            return True
        except Exception as e:
            self.logger.error(f"完整流程生成报告失败: {str(e)}")
            return False

    def run_all_tests(self):
        """运行所有 API 测试"""
        self.logger.info("开始运行所有 API 测试...")
        
        # 准备测试数据
        self.setup_test_data()
        
        # 执行所有测试
        vision_result = self.test_vision_api()
        text_result = self.test_text_api()
        questionnaire_only_result = self.test_report_with_questionnaire_only()
        full_pipeline_result = self.test_full_pipeline()
        
        # 清理测试数据
        self.cleanup_test_data()
        
        # 汇总测试结果
        if all([vision_result, text_result, questionnaire_only_result, full_pipeline_result]):
            self.logger.info("所有 API 测试通过！")
            return True
        else:
            self.logger.error("部分或全部 API 测试未通过，请检查日志")
            self.logger.error(f"测试结果 - 图像API: {vision_result}, 文本API: {text_result}, 仅量表: {questionnaire_only_result}, 完整流程: {full_pipeline_result}")
            return False

if __name__ == "__main__":
    import yaml
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    config_path = os.path.join(project_root, "config", "config.yaml")
    with open(config_path, "r", encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    tester = APITester(config)
    tester.run_all_tests()
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\data_entry.py
# 文件路径: src/data_entry.py
import os
import sys

# 获取项目根目录（PsychologyAnalysis/）
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
# 将项目根目录添加到模块搜索路径
sys.path.append(project_root)

from src.data_handler import DataHandler

def main():
    # 初始化 DataHandler
    handler = DataHandler(db_path=os.path.join(project_root, "psychology_analysis.db"))

    # 定义测试数据
    data_entries = [
        {
            "image_path": os.path.join(project_root, "input\images\image1.jpg"),
            "subject_info": {"name": "张三", "age": 13, "gender": "男"},
            "questionnaire_data": {"q1": "yes", "q2": "no", "q3": "sometimes"}
        },
        {
            "image_path": os.path.join(project_root, "input\images\TestPic.jpg"),
            "subject_info": {"name": "李四", "age": 15, "gender": "女"},
            "questionnaire_data": {"q1": "no", "q2": "yes", "q3": "often"}
        },
        {
            "image_path": "no_image.jpg",  # 没有对应图片，仅量表数据
            "subject_info": {"name": "王五", "age": 20, "gender": "男"},
            "questionnaire_data": {"q1": "yes", "q2": "yes", "q3": "rarely"}
        }
    ]

    # 录入数据
    for entry in data_entries:
        image_path = entry["image_path"]
        subject_info = entry["subject_info"]
        questionnaire_data = entry["questionnaire_data"]

        # 检查图片文件是否存在（如果 image_path 不是虚拟路径）
        if image_path != "no_image.jpg" and not os.path.exists(image_path):
            print(f"警告: 图片文件 {image_path} 不存在，跳过录入")
            continue

        try:
            handler.save_data(image_path, subject_info, questionnaire_data)
            print(f"成功录入数据: 图片路径 {image_path}, 被测者 {subject_info['name']}")
        except Exception as e:
            print(f"录入数据失败: 图片路径 {image_path}, 错误: {str(e)}")

if __name__ == "__main__":
    main()
    from src.data_handler import check_db_content
    check_db_content(os.path.join(project_root, "psychology_analysis.db"))
    
    
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\data_handler.py
# 文件路径: src/data_handler.py
import sqlite3
import json
import os
from datetime import datetime

class DataHandler:
    def __init__(self, db_path="psychology_analysis.db"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """Initializes the database schema more robustly."""
        print(f"Initializing database schema at: {self.db_path}")
        required_columns_analysis = {
            # 列名: 列类型 (不包含约束和复杂默认值，这些在CREATE TABLE中处理)
            "image_path": "TEXT",
            "subject_name": "TEXT",
            "age": "INTEGER",
            "gender": "TEXT",
            "questionnaire_type": "TEXT",
            "questionnaire_data": "TEXT", # JSON
            "report_text": "TEXT",
            "updated_at": "TIMESTAMP", # 类型即可，默认值由触发器处理
            "id_card": "TEXT", # 类型即可，UNIQUE在CREATE TABLE中处理
            "occupation": "TEXT",
            "case_name": "TEXT",
            "case_type": "TEXT",
            "identity_type": "TEXT",
            "person_type": "TEXT",
            "marital_status": "TEXT",
            "children_info": "TEXT",
            "criminal_record": "INTEGER",
            "health_status": "TEXT",
            "phone_number": "TEXT",
            "domicile": "TEXT"
        }
        # questionnaire_questions 列
        required_columns_questions = {
            "scale_name": "TEXT"
        }


        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # --- Handle analysis_data Table ---
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='analysis_data';")
            table_exists = cursor.fetchone()

            if not table_exists:
                print("Table 'analysis_data' does not exist. Creating new table with full schema...")
                # Create table with all columns and constraints if it doesn't exist
                create_table_sql = """
                CREATE TABLE analysis_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    image_path TEXT,
                    subject_name TEXT,
                    age INTEGER,
                    gender TEXT,
                    questionnaire_type TEXT,
                    questionnaire_data TEXT,
                    report_text TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, -- Initial value
                    id_card TEXT , 
                    occupation TEXT,
                    case_name TEXT,
                    case_type TEXT,
                    identity_type TEXT,
                    person_type TEXT,
                    marital_status TEXT,
                    children_info TEXT,
                    criminal_record INTEGER DEFAULT 0,
                    health_status TEXT,
                    phone_number TEXT,
                    domicile TEXT
                );
                """
                cursor.execute(create_table_sql)
                print("Table 'analysis_data' created successfully.")
            else:
                print("Table 'analysis_data' exists. Checking for missing columns...")
                # If table exists, check and add missing columns without problematic constraints/defaults
                cursor.execute("PRAGMA table_info(analysis_data)")
                existing_columns = {info[1] for info in cursor.fetchall()}

                for col_name, col_type in required_columns_analysis.items():
                    if col_name not in existing_columns:
                        try:
                            # Add column with only the type, no complex defaults or UNIQUE constraints here
                            cursor.execute(f"ALTER TABLE analysis_data ADD COLUMN {col_name} {col_type}")
                            print(f"Added column '{col_name}' to analysis_data table.")
                        except sqlite3.OperationalError as e:
                            print(f"Warning: Could not add column '{col_name}': {e}")

            # --- Handle questionnaire_questions Table ---
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='questionnaire_questions';")
            q_table_exists = cursor.fetchone()
            if not q_table_exists:
                 print("Table 'questionnaire_questions' does not exist. Creating new table...")
                 cursor.execute('''CREATE TABLE questionnaire_questions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    questionnaire_type TEXT NOT NULL,
                    question_number INTEGER NOT NULL,
                    question_text TEXT NOT NULL,
                    options TEXT NOT NULL, -- JSON string
                    scale_name TEXT, -- User-friendly name
                    UNIQUE(questionnaire_type, question_number)
                )''')
                 print("Table 'questionnaire_questions' created successfully.")
            else:
                 print("Table 'questionnaire_questions' exists. Checking for missing columns...")
                 cursor.execute("PRAGMA table_info(questionnaire_questions)")
                 existing_q_columns = {info[1] for info in cursor.fetchall()}
                 for col_name, col_type in required_columns_questions.items():
                      if col_name not in existing_q_columns:
                           try:
                                cursor.execute(f"ALTER TABLE questionnaire_questions ADD COLUMN {col_name} {col_type}")
                                print(f"Added column '{col_name}' to questionnaire_questions table.")
                           except sqlite3.OperationalError as e:
                                print(f"Warning: Could not add column '{col_name}' to questionnaire_questions: {e}")


            # --- Ensure Triggers Exist ---
            # Trigger to update 'updated_at' timestamp (safe to run even if exists)
            try:
                 cursor.execute('''
                    CREATE TRIGGER IF NOT EXISTS update_analysis_data_updated_at
                    AFTER UPDATE ON analysis_data
                    FOR EACH ROW
                    WHEN OLD.updated_at = NEW.updated_at OR OLD.updated_at IS NULL -- Avoid infinite loops if trigger itself updates
                    BEGIN
                        UPDATE analysis_data SET updated_at = CURRENT_TIMESTAMP WHERE id = OLD.id;
                    END;
                ''')
                 print("Ensured 'updated_at' trigger exists.")
            except sqlite3.OperationalError as e:
                 print(f"Warning: Could not create/verify 'updated_at' trigger: {e}")


            conn.commit()
        print("Database schema initialization process completed.")
    def normalize_path(self, path):
        """Normalizes file path for consistency, returns None if path is None."""
        if path is None:
            return None
        return os.path.normpath(path).replace('\\', '/')

    def save_data(self, image_path, basic_info, scale_type, scale_answers_json):
        """Saves comprehensive data to the analysis_data table."""
        normalized_image_path = self.normalize_path(image_path) # Can be None

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            # Prepare column names and placeholders dynamically based on basic_info keys
            # Always include mandatory fields
            columns = ['image_path', 'questionnaire_type', 'questionnaire_data']
            placeholders = ['?', '?', '?']
            values = [normalized_image_path, scale_type, scale_answers_json]

            # Add fields from basic_info
            for key, value in basic_info.items():
                # Map form names to DB column names if necessary
                db_key = key # Assume direct mapping for now
                # Handle specific fields like subject_name, age, gender if they are part of basic_info
                if db_key == "name": db_key = "subject_name"

                # Ensure the key is a valid column name (check against required_columns keys)
                # This is a basic check; more robust validation might be needed
                valid_columns = { "subject_name", "age", "gender", "id_card", "occupation", "case_name", "case_type", "identity_type", "person_type", "marital_status", "children_info", "criminal_record", "health_status", "phone_number", "domicile"}
                if db_key in valid_columns:
                    columns.append(db_key)
                    placeholders.append('?')
                    # Special handling for criminal_record (assuming 1 for Yes, 0 for No from form)
                    if db_key == 'criminal_record':
                        values.append(1 if str(value).lower() in ['1', 'yes', 'true'] else 0)
                    else:
                        values.append(value)

            columns_str = ", ".join(columns)
            placeholders_str = ", ".join(placeholders)

            sql = f'''INSERT INTO analysis_data ({columns_str})
                      VALUES ({placeholders_str})'''

            try:
                cursor.execute(sql, values)
                submission_id = cursor.lastrowid
                conn.commit()
                print(f"Data saved successfully. Submission ID: {submission_id}")
                return submission_id
            except sqlite3.IntegrityError as e:
                 print(f"Error saving data: {e}. Possible duplicate entry (e.g., ID card)?")
                 # Depending on requirements, you might want to update instead of insert
                 # Or simply report the error
                 raise e # Re-raise the exception
            except Exception as e:
                 print(f"An unexpected error occurred during save: {e}")
                 raise e


    def load_data_by_id(self, submission_id):
        """Loads a submission's data by its ID, returning a dictionary."""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row # Return rows as dictionary-like objects
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM analysis_data WHERE id = ?", (submission_id,))
            result = cursor.fetchone()
            if result:
                return dict(result) # Convert Row object to a standard dictionary
            return None

    def update_report_text(self, submission_id, report_text):
        """Updates the report_text for a given submission ID."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("UPDATE analysis_data SET report_text = ? WHERE id = ?",
                           (report_text, submission_id))
            conn.commit()
            print(f"Report text updated for submission ID: {submission_id}")

    def load_questions_by_type(self, questionnaire_type_code):
        """Loads questions based on the questionnaire type code (e.g., 'SAS')."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''SELECT question_number, question_text, options
                            FROM questionnaire_questions
                            WHERE questionnaire_type = ?
                            ORDER BY question_number''', (questionnaire_type_code,))
            results = cursor.fetchall()
            questions = []
            if not results:
                 print(f"Warning: No questions found for type code '{questionnaire_type_code}'")
                 return None # Return None or empty list based on how you want to handle this
            for row in results:
                try:
                    options_list = json.loads(row[2])
                    # Ensure options have 'text' (or 'name') and 'score' keys
                    formatted_options = [
                        {"text": opt.get("text", opt.get("name", "N/A")), "score": opt.get("score", 0)}
                        for opt in options_list
                    ]
                    question = {
                        "number": row[0],
                        "text": row[1],
                        "options": formatted_options
                    }
                    questions.append(question)
                except json.JSONDecodeError:
                    print(f"Warning: Could not decode options for question {row[0]} of type {questionnaire_type_code}")
                except Exception as e:
                     print(f"Error processing question {row[0]} options: {e}")
            return questions


    def insert_question(self, questionnaire_type, question_number, question_text, options_json_str, scale_name=None):
        """Inserts a single question into the database."""
        # The options should already be a JSON string here if coming from import_questions
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            try:
                cursor.execute('''INSERT OR REPLACE INTO questionnaire_questions
                                (questionnaire_type, question_number, question_text, options, scale_name)
                                VALUES (?, ?, ?, ?, ?)''', (
                    questionnaire_type,
                    question_number,
                    question_text,
                    options_json_str, # Store as JSON string
                    scale_name if scale_name else questionnaire_type # Default scale_name to type if not provided
                ))
                conn.commit()
                # print(f"Inserted/Replaced question: {questionnaire_type} - Q{question_number}")
            except Exception as e:
                 print(f"Error inserting question {questionnaire_type}-Q{question_number}: {e}")

    def get_all_scale_types(self):
        """Retrieves distinct scale types (code and name) from the database."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            # Use COALESCE to provide the type code if name is NULL
            cursor.execute('''SELECT DISTINCT questionnaire_type, COALESCE(scale_name, questionnaire_type) as display_name
                            FROM questionnaire_questions
                            ORDER BY questionnaire_type''')
            results = cursor.fetchall()
            # Return as a list of dictionaries
            return [{"code": row[0], "name": row[1]} for row in results]


def check_db_content(db_path="psychology_analysis.db"):
    """Utility function to print the content of the analysis_data table."""
    print(f"\n--- Checking content of {db_path} ---")
    if not os.path.exists(db_path):
        print("Database file does not exist.")
        return

    try:
        with sqlite3.connect(db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            print("\n[analysis_data Table Content]")
            try:
                cursor.execute("SELECT * FROM analysis_data LIMIT 10") # Limit output for brevity
                rows = cursor.fetchall()
                if not rows:
                    print("Table is empty.")
                else:
                    # Print header
                    print(" | ".join(rows[0].keys()))
                    print("-" * (len(" | ".join(rows[0].keys())) + 10))
                    # Print rows
                    for row in rows:
                        print(" | ".join(map(str, row)))
            except sqlite3.OperationalError as e:
                print(f"Error querying analysis_data: {e}")


            print("\n[questionnaire_questions Table Content]")
            try:
                cursor.execute("SELECT DISTINCT questionnaire_type, scale_name FROM questionnaire_questions")
                scales = cursor.fetchall()
                if not scales:
                    print("Table is empty or contains no distinct scales.")
                else:
                    print("Available Scales (Type | Name):")
                    for scale in scales:
                        print(f"{scale['questionnaire_type']} | {scale['scale_name']}")

                # Optionally print a few questions per scale
                if scales:
                     print("\nSample Questions:")
                     for scale in scales[:2]: # Limit to first 2 scales for brevity
                         print(f"--- Scale: {scale['questionnaire_type']} ---")
                         cursor.execute("SELECT question_number, question_text FROM questionnaire_questions WHERE questionnaire_type = ? ORDER BY question_number LIMIT 3", (scale['questionnaire_type'],))
                         questions = cursor.fetchall()
                         for q in questions:
                             print(f"  Q{q['question_number']}: {q['question_text'][:50]}...") # Truncate long text

            except sqlite3.OperationalError as e:
                print(f"Error querying questionnaire_questions: {e}")

    except sqlite3.Error as e:
        print(f"An error occurred connecting to or reading the database: {e}")
    print("--- End of content check ---\n")


# Example usage (can be run directly to check DB)
if __name__ == "__main__":
     # Assume db is in the project root relative to this file's location (src/)
     project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
     db_file_path = os.path.join(project_root, "psychology_analysis.db")
     # Initialize schema first (important if DB or tables don't exist)
     print("Initializing DataHandler to ensure schema exists...")
     try:
         handler = DataHandler(db_path=db_file_path)
         print("DataHandler initialized.")
         # Now check content
         check_db_content(db_file_path)
         print("\nAvailable scale types:")
         print(handler.get_all_scale_types())
     except Exception as e:
          print(f"Error during DataHandler initialization or check: {e}")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\image_processor.py
# src/image_processor.py
import os
import base64
from openai import OpenAI
import logging # Use logging

# Get the logger instance setup in app.py or utils.py
logger = logging.getLogger("PsychologyAnalysis")

class ImageProcessor:
    def __init__(self, config):
        """Initializes ImageProcessor with configuration and explicit safe headers."""
        self.api_key = config.get("api_key")
        if not self.api_key:
             logger.warning("API Key not found in config for ImageProcessor.")
             # Attempt to get from environment as a fallback
             self.api_key = os.environ.get('DASHSCOPE_API_KEY')
             if not self.api_key:
                  raise ValueError("API Key missing for ImageProcessor.")

        self.model = config.get("vision_model", "qwen-vl-plus") # Use config
        self.base_url = config.get("base_url", "https://dashscope.aliyuncs.com/compatible-mode/v1")

        # --- Explicitly set safe default headers ---
        safe_headers = {
            "User-Agent": "MyPsychologyApp-ImageProcessor/1.0", # Example ASCII User-Agent
            "Accept": "application/json",
            # Add other standard headers if needed, but keep values ASCII safe
            # Avoid adding headers derived from potentially non-ASCII user input here
        }
        # --- End of safe headers ---

        try:
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                default_headers=safe_headers # <--- Add explicit safe headers
            )
            logger.info(f"ImageProcessor OpenAI client initialized. Base URL: {self.base_url}. Using explicit safe headers.")
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client in ImageProcessor: {e}", exc_info=True)
            raise

    def process_image(self, image_path):
        """Processes an image using the configured vision model."""
        logger.info(f"Processing image: {image_path}")
        try:
            # 将图片转为 base64 编码
            with open(image_path, "rb") as image_file:
                image_base64 = base64.b64encode(image_file.read()).decode("utf-8")
        except FileNotFoundError:
             logger.error(f"Image file not found: {image_path}")
             raise FileNotFoundError(f"图片文件未找到: {image_path}")
        except Exception as e:
             logger.error(f"Error reading or encoding image {image_path}: {e}", exc_info=True)
             raise Exception(f"读取或编码图片时出错: {e}") from e

        # 构造消息内容
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": "You are a helpful assistant focused on image description."}] # System prompt
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"} # Assume JPEG, adjust if needed
                    },
                    # Keep the prompt focused on description
                    {"type": "text", "text": "请详细描述这张图片的内容，包括物体、人物（如有）、场景氛围、颜色和构图等。"}
                ]
            }
        ]

        try:
            # 调用 API
            logger.debug(f"Calling vision model '{self.model}' for image {os.path.basename(image_path)}")
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
            )
            description = completion.choices[0].message.content
            logger.info(f"Image description received successfully for {os.path.basename(image_path)}.")
            return description
        except Exception as e:
            # Log the specific error during the API call
            logger.error(f"Error calling vision API for {os.path.basename(image_path)}: {type(e).__name__} - {e}", exc_info=True)
            # Re-raise a user-friendly exception (the original traceback still points here)
            raise Exception(f"图像识别失败: {str(e)}") from e
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\import_questions.py
# 文件路径: src/import_questions.py
import os
import json
import sqlite3
# Correct import assuming data_handler.py is in the same directory (src)
try:
    from data_handler import DataHandler
except ImportError:
    print("Error: data_handler.py not found in the same directory.")
    # Fallback for running directly from PsychologyAnalysis root
    try:
        from data_handler import DataHandler
    except ImportError:
         print("Error: Could not import DataHandler. Make sure you run this script correctly.")
         exit(1)


def import_questions_from_json():
    """Loads scale questions from JSON files in input/questionnaires/ into the SQLite DB."""
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    db_path = os.path.join(project_root, "psychology_analysis.db")
    handler = DataHandler(db_path=db_path)
    questionnaire_dir = os.path.join(project_root, "input", "questionnaires")

    print(f"Scanning for JSON questionnaires in: {questionnaire_dir}")

    json_files = [f for f in os.listdir(questionnaire_dir) if f.endswith('.json')]
    if not json_files:
        print(f"No .json files found in {questionnaire_dir}")
        return

    imported_count = 0
    error_count = 0

    # Define mapping from filename (or title) to scale type code and name
    # Prioritize title if available, otherwise use filename mapping
    scale_mapping = {
        "1测你性格最真实的一面.json": {"code": "Personality", "name": "测你性格最真实的一面"},
        "2亲子关系问卷量表.json": {"code": "ParentChild", "name": "亲子关系问卷量表"},
        "3焦虑症自评量表 (SAS).json": {"code": "SAS", "name": "焦虑症自评量表 (SAS)"},
        "4标准量表：抑郁症自测量表 (SDS).json": {"code": "SDS", "name": "抑郁症自测量表 (SDS)"},
        "5人际关系综合诊断量表.json": {"code": "InterpersonalRelationship", "name": "人际关系综合诊断量表"},
        "6情绪稳定性测验量表.json": {"code": "EmotionalStability", "name": "情绪稳定性测验量表"},
        "7汉密尔顿抑郁量表HAMD24.json": {"code": "HAMD24", "name": "汉密尔顿抑郁量表 (HAMD-24)"},
        "8艾森克人格问卷EPQ85成人版.json": {"code": "EPQ85", "name": "艾森克人格问卷 (EPQ-85成人版)"}
        # Add more mappings as needed
    }
    title_mapping = {
        "测你性格最真实的一面": {"code": "Personality", "name": "测你性格最真实的一面"},
        "亲子关系问卷量表": {"code": "ParentChild", "name": "亲子关系问卷量表"},
        "焦虑症自评量表 (SAS)": {"code": "SAS", "name": "焦虑症自评量表 (SAS)"},
        "标准量表：抑郁症自测量表 (SDS)": {"code": "SDS", "name": "抑郁症自测量表 (SDS)"},
        "人际关系综合诊断量表.json": {"code": "InterpersonalRelationship", "name": "人际关系综合诊断量表"},
        "情绪稳定性测验量表.json": {"code": "EmotionalStability", "name": "情绪稳定性测验量表"},
        "汉密尔顿抑郁量表HAMD24.json": {"code": "HAMD24", "name": "汉密尔顿抑郁量表 (HAMD-24)"},
        "艾森克人格问卷EPQ85成人版.json": {"code": "EPQ85", "name": "艾森克人格问卷 (EPQ-85成人版)"}
    }


    for json_file in json_files:
        file_path = os.path.join(questionnaire_dir, json_file)
        print(f"\nProcessing file: {json_file}")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Determine scale type code and name
            title = data.get("title")
            scale_info = None
            if title and title in title_mapping:
                scale_info = title_mapping[title]
            elif json_file in scale_mapping:
                 scale_info = scale_mapping[json_file]
            else:
                # Fallback: use filename without extension as code and title as name
                scale_code = os.path.splitext(json_file)[0]
                scale_name = title if title else scale_code
                scale_info = {"code": scale_code, "name": scale_name}
                print(f"  Warning: No specific mapping found for '{json_file}' or title '{title}'. Using fallback code='{scale_code}', name='{scale_name}'.")

            questionnaire_type = scale_info["code"]
            scale_display_name = scale_info["name"]

            print(f"  Identified as: Code='{questionnaire_type}', Name='{scale_display_name}'")

            # Clear old questions for this type before inserting new ones (optional but recommended)
            with sqlite3.connect(handler.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("DELETE FROM questionnaire_questions WHERE questionnaire_type = ?", (questionnaire_type,))
                conn.commit()
            print(f"  Cleared existing questions for type '{questionnaire_type}'.")


            # Insert questions
            questions_in_file = data.get("questions", [])
            if not questions_in_file:
                 print(f"  Warning: No 'questions' array found in {json_file}")
                 error_count += 1
                 continue

            for question in questions_in_file:
                try:
                    q_num = question["number"]
                    q_text = question["text"]
                    # Prepare options as JSON string, ensuring 'text' and 'score' keys exist
                    options_list = [
                        {"text": opt.get("text", "N/A"), "score": opt.get("score", 0)}
                        for opt in question.get("options", [])
                    ]
                    options_json = json.dumps(options_list, ensure_ascii=False)

                    # Use the enhanced insert_question method
                    handler.insert_question(
                        questionnaire_type,
                        q_num,
                        q_text,
                        options_json, # Pass JSON string directly
                        scale_display_name # Pass the scale name
                        )
                    imported_count += 1
                except KeyError as ke:
                     print(f"    Error processing question in {json_file}: Missing key {ke}")
                     error_count += 1
                except Exception as e_inner:
                     print(f"    Error processing question {question.get('number', 'N/A')} in {json_file}: {e_inner}")
                     error_count += 1

        except json.JSONDecodeError as jde:
            print(f"  Error decoding JSON from {json_file}: {jde}")
            error_count += 1
        except Exception as e_outer:
            print(f"  Error processing file {json_file}: {e_outer}")
            error_count += 1

    print(f"\nImport finished. Successfully imported {imported_count} questions.")
    if error_count > 0:
        print(f"Encountered {error_count} errors during import.")

if __name__ == "__main__":
    import_questions_from_json()
    # Optional: Check DB content after import
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    db_file_path = os.path.join(project_root, "psychology_analysis.db")
    try:
        from data_handler import check_db_content
        check_db_content(db_file_path)
    except ImportError:
         print("\nRun `python src/data_handler.py` to check database content.")
    except Exception as e:
        print(f"\nError checking DB content after import: {e}")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\report_generator.py
# src/report_generator.py
from openai import OpenAI
import json
import os
import logging # Use logging

# Get the logger instance setup in app.py or utils.py
logger = logging.getLogger("PsychologyAnalysis")

class ReportGenerator:
    # --- 确保 __init__ 方法定义正确，包含 self ---
    def __init__(self, config):
        """Initializes ReportGenerator with configuration and explicit safe headers."""
        # --- 使用 self. 引用实例变量 ---
        self.config = config # Store config if needed elsewhere
        self.api_key = config.get("api_key")
        if not self.api_key:
             logger.warning("API Key not found in config for ReportGenerator.")
             self.api_key = os.environ.get('DASHSCOPE_API_KEY')
             if not self.api_key:
                  raise ValueError("API Key missing for ReportGenerator.")

        self.model = config.get("text_model", "qwen-plus")
        self.base_url = config.get("base_url", "https://dashscope.aliyuncs.com/compatible-mode/v1")

        # --- Explicitly set safe default headers ---
        safe_headers = {
            "User-Agent": "MyPsychologyApp-ReportGenerator/1.0", # Example ASCII User-Agent
            "Accept": "application/json",
        }
        # --- End of safe headers ---

        try:
             # --- 使用 self. 引用实例变量 ---
             self.client = OpenAI(
                 api_key=self.api_key,
                 base_url=self.base_url,
                 default_headers=safe_headers # <--- Add explicit safe headers
             )
             logger.info(f"ReportGenerator OpenAI client initialized. Base URL: {self.base_url}. Using explicit safe headers.")
        except Exception as e:
             logger.error(f"Failed to initialize OpenAI client in ReportGenerator: {e}", exc_info=True)
             raise

        # --- Default prompt template definition (moved inside init for clarity) ---
        self.default_prompt_template = """
请根据以下信息生成一份详细的心理分析报告，服务于警务工作场景。

**I. 被测者基础信息:**
姓名: {subject_info[name]}
性别: {subject_info[gender]}
身份证号: {subject_info[id_card]}
年龄: {subject_info[age]}
职业: {subject_info[occupation]}
案件名称: {subject_info[case_name]}
案件类型: {subject_info[case_type]}
人员身份: {subject_info[identity_type]}
人员类型: {subject_info[person_type]}
婚姻状况: {subject_info[marital_status]}
子女情况: {subject_info[children_info]}
有无犯罪前科: {criminal_record_text}
健康情况: {subject_info[health_status]}
手机号: {subject_info[phone_number]}
归属地: {subject_info[domicile]}

**II. 绘画分析 (基于AI对图片的描述):**
{description}

**III. 量表分析:**
量表类型: {questionnaire_type}
量表得分: {score}
量表答案详情 (JSON):
{questionnaire}
初步解释: {scale_interpretation}

**IV. 综合心理状态分析与建议:**
请结合以上所有信息（基础信息、绘画分析、量表结果），进行深入的心理状态评估，提取关键人格特征，并针对警务工作场景（如未成年人犯罪预防、在押人员管理、上访户调解、民辅警关怀等，根据人员类型判断侧重点）给出具体的风险评估、干预建议或沟通策略。分析需专业、客观、有条理。报告应直接开始分析内容，无需重复引言。

--- 分析报告正文 ---
"""
        # --- 使用 self. 引用实例变量 ---
        # Use template from config if provided and is a string, otherwise use the default
        config_template = config.get("REPORT_PROMPT_TEMPLATE")
        if isinstance(config_template, str) and config_template.strip():
             self.prompt_template = config_template
             logger.debug("Using prompt template from config.")
        else:
             if config_template is not None: # Log if it existed but wasn't valid
                  logger.warning("REPORT_PROMPT_TEMPLATE in config is not a valid string. Using default.")
             else:
                  logger.debug("REPORT_PROMPT_TEMPLATE not found in config. Using default.")
             self.prompt_template = self.default_prompt_template


    # 确保 generate_report 方法也正确包含 self 和所有需要的参数
    def generate_report(self, description, questionnaire, subject_info, questionnaire_type, score, scale_interpretation):
        """Generates the report by formatting the prompt and calling the LLM API."""
        logger.info(f"Generating report for subject: {subject_info.get('name', 'N/A')}")

        # --- 在这里计算 criminal_record_text ---
        criminal_record_text = '是' if subject_info.get('criminal_record', 0) == 1 else '否'
        # ------------------------------------

        # Safely format questionnaire data (which should be a dictionary passed from ai_utils)
        questionnaire_str = "N/A"
        if questionnaire and isinstance(questionnaire, dict): # Check if it's a dict
            try:
                # Dump the dictionary to a pretty JSON string for the prompt
                questionnaire_str = json.dumps(questionnaire, ensure_ascii=False, indent=2)
            except Exception as json_err:
                 logger.warning(f"Could not dump questionnaire dict to JSON: {json_err}. Using raw dict string.")
                 questionnaire_str = str(questionnaire) # Fallback
        elif isinstance(questionnaire, str): # If it's already a string (e.g., JSON string)
            questionnaire_str = questionnaire
        elif questionnaire:
            logger.warning(f"Unexpected type for questionnaire data: {type(questionnaire)}. Using raw string.")
            questionnaire_str = str(questionnaire)


        # --- 构建 prompt_context，包含所有模板需要的键 ---
        prompt_context = {
            'description': description if description else "无",
            'questionnaire': questionnaire_str, # Use formatted string
            'subject_info': subject_info if subject_info else {}, # Ensure it's a dict
            'questionnaire_type': questionnaire_type if questionnaire_type else "未知",
            'score': score if score is not None else "N/A",
            'scale_interpretation': scale_interpretation if scale_interpretation else "无",
            'criminal_record_text': criminal_record_text # <--- 添加计算出的文本
        }
        # ---------------------------------------------

        # Ensure all required keys for the template exist in subject_info context
        default_keys = ["name", "gender", "id_card", "age", "occupation", "case_name", "case_type", "identity_type", "person_type", "marital_status", "children_info", "criminal_record", "health_status", "phone_number", "domicile"]
        # Ensure subject_info itself is a dict before iterating
        if isinstance(prompt_context['subject_info'], dict):
             for key in default_keys:
                 prompt_context['subject_info'].setdefault(key, '未提供') # Set default if key missing
        else: # If subject_info is somehow not a dict, create a default one
             logger.warning(f"subject_info was not a dictionary (type: {type(prompt_context['subject_info'])}). Creating default context.")
             prompt_context['subject_info'] = {key: '未提供' for key in default_keys}


        try:
            # --- 使用 self.prompt_template 和构建好的 context 格式化 ---
            final_prompt = self.prompt_template.format(**prompt_context)
            logger.debug(f"Formatted Prompt (first 500 chars): {final_prompt[:500]}...")
        except KeyError as e:
             logger.error(f"Prompt template formatting error: Missing key {e}. Context keys available: {list(prompt_context.keys())}", exc_info=True)
             # Check if the missing key is expected in subject_info
             if str(e).strip("'") in default_keys:
                  logger.error(f"Missing key '{e}' likely expected within subject_info dictionary: {prompt_context.get('subject_info')}")
             raise KeyError(f"Prompt template formatting error: Missing key {e}") from e
        except Exception as e_fmt:
             logger.error(f"Prompt template formatting error: {e_fmt}", exc_info=True)
             raise Exception(f"Prompt template formatting error: {e_fmt}") from e_fmt


        messages = [
            # Refined system prompt
            {"role": "system", "content": "你是一位专业的心理分析师。请根据用户提供的多维度信息（基础信息、绘画描述、量表结果与解释），结合心理学知识和警务场景，生成一份结构清晰、分析深入、建议具体的综合心理评估报告。"},
            {"role": "user", "content": final_prompt}
        ]

        try:
            logger.debug(f"Calling text model '{self.model}'...")
             # --- 使用 self.client 和 self.model 调用 API ---
            completion = self.client.chat.completions.create(
                model=self.model, # 使用 self.model
                messages=messages,
            )
            report_content = completion.choices[0].message.content
            logger.info("Report content received successfully.")
            return report_content
        except Exception as e:
            logger.error(f"Error calling text generation API: {type(e).__name__} - {e}", exc_info=True)
            # Re-raise the exception so ai_utils can catch it
            raise Exception(f"调用大模型 API 时出错 - {str(e)}") from e
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\utils.py
# src/utils.py
import logging
import os
import sys
from logging.handlers import RotatingFileHandler # (可选) 使用轮转日志

# --------------------------------------------------------------------------
# 重要: 这个文件现在属于 'src' 包，
# 它将被 'app/main.py' 导入。
# 'app/main.py' 已经将项目根目录添加到了 sys.path,
# 所以这里理论上可以直接访问 'app' 包，但最好避免循环导入。
# 因此，日志配置的参数（如 level, dir）应该由调用者传入。
# --------------------------------------------------------------------------

# 获取项目根目录 (PsychologyAnalysis/)
# __file__ 指向当前文件 (utils.py)
# os.path.dirname(__file__) 指向 src 目录
# os.path.dirname(os.path.dirname(__file__)) 指向 PsychologyAnalysis 目录
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

def setup_logging(log_level_str: str = "INFO", log_dir_name: str = "logs", logger_name: str = "QingtingzheApp"):
    """
    配置应用程序的日志记录。

    Args:
        log_level_str (str): 日志级别字符串 (e.g., "DEBUG", "INFO", "WARNING").
        log_dir_name (str): 相对于项目根目录的日志文件夹名称.
        logger_name (str): 要配置的日志记录器的名称.
    """
    # --- 1. 获取日志级别 ---
    log_level = getattr(logging, log_level_str.upper(), logging.INFO)
    print(f"[Logging Setup] Setting log level to: {logging.getLevelName(log_level)} ({log_level_str})")

    # --- 2. 计算日志文件路径 ---
    log_directory = os.path.join(PROJECT_ROOT, log_dir_name)
    try:
        os.makedirs(log_directory, exist_ok=True)
        print(f"[Logging Setup] Ensured log directory exists: {log_directory}")
    except OSError as e:
        print(f"[Logging Setup] Error creating log directory {log_directory}: {e}", file=sys.stderr)
        # 如果目录创建失败，可能无法写入文件日志，但控制台日志仍应工作
        log_directory = None # 标记目录不可用

    log_file_path = os.path.join(log_directory, "app.log") if log_directory else None
    print(f"[Logging Setup] Log file path set to: {log_file_path}")


    # --- 3. 获取或创建 Logger 实例 ---
    # 使用传入的 logger_name，而不是固定的 "PsychologyAnalysis"
    # 这样可以更容易地区分来自不同模块的日志（如果需要的话）
    # 但对于简单应用，使用根 logger 或一个统一的 app logger 也可以
    logger = logging.getLogger(logger_name)
    logger.setLevel(log_level) # 设置 Logger 的基础级别

    # --- 4. 清除旧的 Handlers (防止重复添加) ---
    # 如果多次调用 setup_logging，这可以防止日志重复输出
    if logger.hasHandlers():
        print("[Logging Setup] Clearing existing handlers for logger:", logger_name)
        logger.handlers.clear()

    # --- 5. 创建 Formatter ---
    log_format = "%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s"
    formatter = logging.Formatter(log_format)

    # --- 6. 创建并添加 Handlers ---

    # a) 控制台 Handler (总是添加)
    console_handler = logging.StreamHandler(sys.stdout) # 输出到标准输出
    console_handler.setLevel(log_level) # 控制台 Handler 也遵循设定的级别
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    print(f"[Logging Setup] Added Console Handler (Level: {logging.getLevelName(console_handler.level)})")

    # b) 文件 Handler (如果路径有效)
    if log_file_path:
        try:
            # 可选：使用 RotatingFileHandler 实现日志轮转
            # maxBytes=10MB, backupCount=5 (保留5个旧日志文件)
            file_handler = RotatingFileHandler(log_file_path, maxBytes=10*1024*1024, backupCount=5, encoding='utf-8')
            # 或者使用你原来的 FileHandler:
            # file_handler = logging.FileHandler(log_file_path, encoding='utf-8')

            file_handler.setLevel(log_level) # 文件 Handler 也遵循设定的级别
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
            print(f"[Logging Setup] Added Rotating File Handler (Level: {logging.getLevelName(file_handler.level)})")
        except Exception as e:
            print(f"[Logging Setup] Failed to create/add file handler for {log_file_path}: {e}", file=sys.stderr)
            logger.error(f"Failed to set up file logging to {log_file_path}: {e}")

    # --- 7. (可选) 配置特定库的日志级别 ---
    # 减少某些库的冗余输出
    logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING) # httpx 日志可能很多
    # logging.getLogger("sqlalchemy.engine").setLevel(logging.INFO) # 查看 SQL

    print(f"[Logging Setup] Configuration for logger '{logger_name}' complete.")
    # setup_logging 不再返回 logger 实例，因为它配置的是指定名称的 logger
    # 在其他模块中，通过 logging.getLogger(logger_name) 获取即可
    # 或者如果配置的是根 logger (logging.basicConfig)，则直接使用 logging.info() 等


# --- 使用方式说明 ---
# 在你的 app/main.py 中:
#
# import logging
# from app.core.config import settings
# from src.utils import setup_logging
#
# # 在创建 FastAPI app 实例之前或之后调用
# setup_logging(log_level_str=settings.LOG_LEVEL,
#               log_dir_name=os.path.basename(settings.LOGS_DIR), # 从完整路径获取目录名
#               logger_name=settings.APP_NAME) # 使用 App 名称作为 Logger 名称
#
# # 获取 logger 实例以在 main.py 中使用
# logger = logging.getLogger(settings.APP_NAME)
# logger.info("FastAPI application starting...")
#
# # 在其他模块 (e.g., app/routers/some_router.py or src/data_handler.py) 中:
# import logging
# from app.core.config import settings # 如果需要配置中的 logger 名称
#
# # 获取在 main.py 中配置好的同名 logger
# logger = logging.getLogger(settings.APP_NAME)
# # 或者，如果决定所有模块都用同一个名字:
# # logger = logging.getLogger("QingtingzheApp") # 使用 setup_logging 时传入的固定名字
#
# logger.info("This log message comes from another module.")
-----


--------------------------------------------------
文件路径: README.md
--------------------------------------------------
# conda
conda activate listener

# 依赖
pip install -r requirements.txt 来安装所有依赖
pip install --force-reinstall -r requirements.txt。这会确保即使包已存在，也会尝试重新安装，有时能解决损坏的安装。

# 进度
核心的 FastAPI 框架、API 端点（用于量表、提交、报告获取）以及 Celery 异步 AI 处理流程都已经成功搭建并初步验证

# 进程1
在执行前，打开redis windows为：redis-server.exe
python run_celery_worker.py #运行该代码来 启动celery worker(这个就行)
celery -A app.core.celery_app worker --loglevel=info -P solo # Windows 使用 solo 进程池

# 进程2

uvicorn app.main:app --reload
uvicorn app.main:app --reload --host 127.0.0.1 --port 8000 --log-level info 启动 FastAPI 服务器 (Uvicorn)

# 进程0:
## 数据库初始化
从终端运行 python -m app.db.init_db。这会连接到 config.py 中 DATABASE_URL 指定的 SQLite 文件，并创建 users 表（如果它还不存在）。

## 数据库迁移 使用 Alembic（目前仅仅用来将数据库备份下，当作数据库版本管理）
安装 Alembic: pip install alembic
初始化 Alembic: 在项目根目录运行 alembic init alembic (会创建一个 alembic 文件夹和 alembic.ini 文件)。
配置 Alembic: 修改 alembic/env.py 文件，让它能连接到你的数据库并识别你的 SQLAlchemy Base。你需要：
导入你的 Base: from app.db.base_class import Base
设置 target_metadata = Base.metadata
配置数据库 URL (可以从 settings 导入)。
自动生成迁移脚本: 运行 alembic revision --autogenerate -m "Add submitter_id to analysis_data"。Alembic 会比较你的模型和数据库，并在 alembic/versions/ 目录下生成一个 Python 文件，包含添加 submitter_id 列的 SQL 操作。
检查迁移脚本: 打开生成的脚本文件，确认它包含了类似 op.add_column('analysis_data', sa.Column('submitter_id', sa.Integer(), nullable=True)) 和 op.create_foreign_key(...)、op.create_index(...) 的操作。
应用迁移: 运行 *alembic upgrade head* 这会执行迁移脚本，更新你的数据库表结构。

## 量表导入：
导入input\questionnaires里面的量表
python src/import_questions.py

使用总结：
1.
alembic revision --autogenerate -m "Create users table and add submitter_id fk"
2.
alembic upgrade head


# 运行应用: 确保你的 FastAPI 应用和 Celery worker 都在运行。（两个进程：1 2）
uvicorn app.main:app --reload --host 127.0.0.1 --port 8000
python run_celery_worker.py (或 celery -A ...)

# 注意！！！！

1. pages.py
下面内容注销了，所以这里没有依赖，等有机会再修复。先能跑

     添加依赖项，确保只有激活的用户才能访问此页面
     dependencies=[Depends(get_current_active_user)]

--------------------------------------------------
文件路径: requirements.txt
--------------------------------------------------
# requirements.txt

# FastAPI Core
fastapi>=0.100.0
uvicorn[standard]>=0.22.0

# Configuration & Environment
pydantic-settings>=2.0.0
python-dotenv>=1.0.0
pyyaml>=6.0

# Database
sqlalchemy>=2.0.0
aiosqlite >= 0.17.0 # For async SQLite access
alembic>=1.7.0

# Security
python-jose[cryptography]>=3.3.0
passlib[bcrypt]>=1.7.4

# HTTP Client & File Uploads
requests>=2.28.1
httpx>=0.24.1
python-multipart>=0.0.7
Werkzeug>=2.0.0 # Secure filename utility

# AI Integration
openai>=1.30.1

# Celery (Background Tasks)
celery>=5.0.0
# 使用 redis[hiredis] >= 5.0.0 来确保包含 redis.asyncio 并获得性能提升
redis[hiredis]>=5.0.0 # Using hiredis for performance, requires redis-py >= 5.0 for Celery 5+ compatibility

# Server-Sent Events (SSE)
sse-starlette>=1.0.0 # 确保这一行存在

# Pydantic (Base for Settings and Schemas)
# 确保 pydantic 版本与 FastAPI 和 Pydantic-Settings 兼容
pydantic[email]>=2.0.0

# Jinja2 (If needed for any template rendering)
jinja2>=3.0.0

# --- 注意：移除了文件末尾重复的 sse-starlette 和 redis>=4.2.0 行 ---
# --- 确保上面列出的 redis[hiredis] 和 sse-starlette 是你需要的唯一条目 ---

--------------------------------------------------
文件路径: run_celery_worker.py
--------------------------------------------------
# run_celery_worker.py
import os
import sys

# --- 1. 定位项目根目录 ---
# __file__ 指向这个脚本文件 (run_celery_worker.py)
# os.path.dirname(__file__) 指向项目根目录 (PsychologyAnalysis/)
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
print(f"[Worker Start Script] Project Root detected: {PROJECT_ROOT}")

# --- 2. 将项目根目录添加到 sys.path ---
# 这样 Celery 启动时就能找到 app 和 src 包
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
    print(f"[Worker Start Script] Added {PROJECT_ROOT} to sys.path")
else:
    print(f"[Worker Start Script] {PROJECT_ROOT} already in sys.path")

# --- 3. 导入 Celery App 实例 ---
# 现在可以安全地导入了
try:
    from app.core.celery_app import celery_app
    print("[Worker Start Script] Successfully imported celery_app")
except ImportError as e:
    print(f"[Worker Start Script] CRITICAL ERROR: Could not import celery_app: {e}")
    print("Please ensure app/core/celery_app.py exists and dependencies are installed.")
    sys.exit(1)
except Exception as e:
     print(f"[Worker Start Script] CRITICAL ERROR during celery_app import: {e}")
     sys.exit(1)


# --- 4. (可选) 加载 .env 文件 (如果 Celery 任务需要直接访问环境变量) ---
# Celery worker 默认不一定加载 .env。如果你的任务代码 (如 ai_utils)
# 需要读取 .env 中的变量 (除了 Pydantic Settings 已经加载的)，
# 在这里加载它。
# from dotenv import load_dotenv
# dotenv_path = os.path.join(PROJECT_ROOT, '.env')
# if os.path.exists(dotenv_path):
#     load_dotenv(dotenv_path=dotenv_path)
#     print(f"[Worker Start Script] Loaded environment variables from: {dotenv_path}")
# else:
#     print("[Worker Start Script] .env file not found, skipping dotenv load.")


# --- 5. 准备 Celery Worker 的命令行参数 ---
# 你可以在这里定义参数，或者从命令行读取
worker_args = [
    'worker',             # 命令
    '--loglevel=info',    # 日志级别
    # '-P', 'solo',       # 在 Windows 上需要添加这个参数
    # '-c', '4',          # (可选) 并发数 (Linux/macOS)
    # '--pool=prefork',   # (可选) 进程池类型 (Linux/macOS 默认)
]

# --- 针对 Windows 添加 solo 进程池 ---
if sys.platform == "win32":
     if '-P' not in worker_args and '--pool' not in worker_args:
         print("[Worker Start Script] Windows detected, adding '-P solo' argument.")
         worker_args.extend(['-P', 'solo'])

# --- 6. 执行 Celery Worker 命令 ---
print(f"[Worker Start Script] Starting Celery worker with args: {worker_args}")
# 使用 celery_app.worker_main 来启动 worker，它会处理命令行参数
celery_app.worker_main(argv=worker_args)

--------------------------------------------------
文件路径: alembic\env.py
--------------------------------------------------
# FILE: alembic/env.py (修改后，启用 batch mode)
import os
import sys
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool
# +++ 确保导入了 sqlalchemy +++
import sqlalchemy

from alembic import context

# +++ 添加这部分来设置路径和导入你的应用模块 +++
# 1. 计算项目根目录 (PROJECT_ROOT)
#    确保你的项目结构是 PsychologyAnalysis/alembic/env.py
try:
    # This assumes the env.py file is in PsychologyAnalysis/alembic/
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
except NameError: # Fallback if __file__ is not defined
    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '.'))
    print(f"警告: __file__ 未定义, 假设项目根目录是当前工作目录: {PROJECT_ROOT}")

# 2. 将项目根目录添加到 sys.path，这样 Python 就能找到 'app' 包
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
    print(f"[Alembic env.py] 已添加项目根目录到 sys.path: {PROJECT_ROOT}")
else:
    print(f"[Alembic env.py] 项目根目录已在 sys.path 中: {PROJECT_ROOT}")


# 3. 从你的应用导入 settings, Base, 和所有模型
try:
    # 导入配置
    from app.core.config import settings
    # 导入 SQLAlchemy Base
    from app.db.base_class import Base
    # +++ 显式导入所有需要被 Alembic 管理的模型 +++
    from app.models.user import User           # 导入 User 模型
    from app.models.assessment import Assessment # 导入 Assessment 模型
    from app.models.interrogation import InterrogationRecord # 导入审讯记录模型
    # 如果还有其他模型，也在这里导入:
    # from app.models.questionnaire import QuestionnaireQuestion # <--- 如果你决定保留并为其创建模型
    print("[Alembic env.py] 成功导入 settings, Base, 和模型 (User, Assessment, InterrogationRecord).") # 更新日志
except ImportError as e:
    print(f"[Alembic env.py] 导入应用模块时出错: {e}")
    print(f"请确认项目根目录 ({PROJECT_ROOT}) 是否正确，并且包含 'app' 包及其子模块 "
          f"(core.config, db.base_class, models.user, models.assessment, models.interrogation)。")
    sys.exit(1)
except Exception as e_import:
     print(f"[Alembic env.py] 在导入期间发生意外错误: {e_import}")
     sys.exit(1)
# --- 添加结束 ---


# 这是 Alembic 配置对象，提供对 .ini 文件值的访问
config = context.config

# 解析配置文件以进行 Python 日志记录。
# 这行代码基本上是设置日志记录器。
if config.config_file_name is not None:
    try:
        fileConfig(config.config_file_name)
        print(f"[Alembic env.py] 已从以下文件配置日志记录: {config.config_file_name}")
    except Exception as e_log:
        # 如果日志配置失败则避免崩溃，仅发出警告
        print(f"[Alembic env.py] 警告: 无法从 {config.config_file_name} 配置日志记录: {e_log}")


# 在这里添加你的模型的 MetaData 对象
# 以支持 'autogenerate'
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata

# +++ 设置 Alembic 需要知道的模型元数据 (现在包含了导入的所有模型) +++
print(f"[Alembic env.py] 从 {Base.__module__}.Base 设置 target_metadata")
target_metadata = Base.metadata
# --- 设置结束 ---

# +++ (可选) 调试代码，打印已注册的表名 +++
try:
    known_tables = list(target_metadata.tables.keys())
    print(f"[Alembic env.py] DEBUG: 在比较前，Base.metadata 中注册的表: {known_tables}")
except Exception as e_debug:
    print(f"[Alembic env.py] DEBUG: 获取元数据中的表名时出错: {e_debug}")
# +++ 调试代码结束 +++

# 从配置中获取的其他值，由 env.py 的需求定义，
# 可以这样获取:
# my_important_option = config.get_main_option("my_important_option")
# ... 等等。


def get_sync_database_url() -> str:
    """从设置中获取数据库 URL，并确保其对于 Alembic 是同步的。"""
    url = settings.DATABASE_URL
    # 将异步 sqlite 驱动替换为同步驱动
    if url and url.startswith("sqlite+aiosqlite"):
        sync_url = url.replace("sqlite+aiosqlite", "sqlite", 1)
        # print(f"[Alembic env.py] 为 Alembic 转换的数据库 URL: {sync_url}") # 减少冗余输出
        return sync_url
    # 如果需要，为其他异步驱动添加类似的替换
    # elif url and url.startswith("postgresql+asyncpg"):
    #     return url.replace("postgresql+asyncpg", "postgresql", 1)
    # print(f"[Alembic env.py] 为 Alembic 按原样使用数据库 URL: {url}") # 减少冗余输出
    return url # 如果未找到已知的异步驱动，则返回原始 URL


def run_migrations_offline() -> None:
    """在 'offline' 模式下运行迁移。"""
    sync_url = get_sync_database_url() # 获取可能修改过的同步 URL
    print(f"[Alembic env.py] 使用 URL 配置离线模式: {sync_url}")

    context.configure(
        url=sync_url,
        target_metadata=target_metadata,
        literal_binds=True, # 推荐用于脚本生成
        dialect_opts={"paramstyle": "named"},
        compare_type=True, # 启用类型比较
        compare_server_default=True, # 启用服务器默认值比较
        render_as_batch=True # <--- *** 在离线模式下也启用 Batch Mode ***
    )

    with context.begin_transaction():
        context.run_migrations()
    print("[Alembic env.py] 离线模式迁移完成。")


def run_migrations_online() -> None:
    """在 'online' 模式下运行迁移。"""
    # 获取同步 URL
    sync_url = get_sync_database_url()
    # 在 Alembic 配置对象中设置同步 URL，覆盖 alembic.ini
    config.set_main_option("sqlalchemy.url", sync_url)
    print(f"[Alembic env.py] 已将在线模式的 sqlalchemy.url 设置为: {sync_url}")

    try:
        # 使用配置中的同步 URL 创建引擎
        connectable = engine_from_config(
            config.get_section(config.config_ini_section, {}),
            prefix="sqlalchemy.",
            poolclass=pool.NullPool, # 迁移时使用 NullPool
        )
        # print("[Alembic env.py] 在线模式的同步引擎创建成功。") # 减少冗余输出
    except Exception as e_engine:
         print(f"[Alembic env.py] 从配置创建同步引擎时出错: {e_engine}")
         sys.exit(1)

    # 使用同步引擎连接
    with connectable.connect() as connection:
        # print("[Alembic env.py] 在线模式的同步连接已建立。") # 减少冗余输出
        # 配置上下文，使用连接和元数据
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,            # 比较列类型
            compare_server_default=True,  # 比较服务器默认值
            # include_schemas=True, # 如果使用 PG schemas，取消注释
            render_as_batch=True # <--- *** 确保在线模式也启用 Batch Mode ***
        )
        # print("[Alembic env.py] 在线模式的上下文已配置。") # 减少冗余输出

        # 在事务中运行迁移
        try:
            print("[Alembic env.py] 开始事务并运行迁移...")
            with context.begin_transaction():
                context.run_migrations()
            print("[Alembic env.py] 迁移在事务中成功运行。")
        except Exception as e_migrate:
             print(f"[Alembic env.py] 运行迁移时出错: {e_migrate}")
             # 如果你希望命令明确失败，可以考虑重新抛出异常
             # raise e_migrate
        finally:
            # 连接会在 'with' 块结束时自动关闭
            pass


# --- 判断模式并运行 ---
if context.is_offline_mode():
    print("[Alembic env.py] 在离线模式下运行迁移。")
    run_migrations_offline()
else:
    print("[Alembic env.py] 在在线模式下运行迁移。")
    run_migrations_online()

print("[Alembic env.py] 脚本执行完毕。")

--------------------------------------------------
文件路径: alembic\versions\269625ee49fe_add_status_column_to_assessment_table.py
--------------------------------------------------
"""Add status column to Assessment table

Revision ID: 269625ee49fe
Revises: 72143d5034d8
Create Date: 2025-04-25 23:48:19.299356

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '269625ee49fe'
down_revision: Union[str, None] = '72143d5034d8'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    # op.drop_table('questionnaire_questions')
    op.add_column('analysis_data', sa.Column('status', sa.String(length=30), server_default='pending', nullable=False))
    op.create_index(op.f('ix_analysis_data_status'), 'analysis_data', ['status'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_analysis_data_status'), table_name='analysis_data')
    op.drop_column('analysis_data', 'status')
    op.create_table('questionnaire_questions',
    sa.Column('id', sa.INTEGER(), nullable=True),
    sa.Column('questionnaire_type', sa.TEXT(), nullable=False),
    sa.Column('question_number', sa.INTEGER(), nullable=False),
    sa.Column('question_text', sa.TEXT(), nullable=False),
    sa.Column('options', sa.TEXT(), nullable=False),
    sa.Column('scale_name', sa.TEXT(), nullable=True),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('questionnaire_type', 'question_number')
    )
    # ### end Alembic commands ###


--------------------------------------------------
文件路径: alembic\versions\5976b03ee0fc_create_interrogation_records_table.py
--------------------------------------------------
"""Create interrogation_records table

Revision ID: 5976b03ee0fc
Revises: 269625ee49fe
Create Date: 2025-05-08 18:39:41.402219

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '5976b03ee0fc'
down_revision: Union[str, None] = '269625ee49fe'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('interrogation_records',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('interrogator_id', sa.Integer(), nullable=False),
    sa.Column('basic_info', sa.JSON(), nullable=True),
    sa.Column('qas', sa.JSON(), nullable=True),
    sa.Column('status', sa.String(length=50), nullable=False),
    sa.Column('full_text', sa.Text(), nullable=True),
    sa.Column('created_at', sa.TIMESTAMP(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),
    sa.Column('updated_at', sa.TIMESTAMP(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),
    sa.ForeignKeyConstraint(['interrogator_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_interrogation_records_id'), 'interrogation_records', ['id'], unique=False)
    op.create_index(op.f('ix_interrogation_records_interrogator_id'), 'interrogation_records', ['interrogator_id'], unique=False)
    op.create_index(op.f('ix_interrogation_records_status'), 'interrogation_records', ['status'], unique=False)
    op.drop_table('questionnaire_questions')
    op.alter_column('analysis_data', 'created_at',
               existing_type=sa.TIMESTAMP(),
               nullable=False,
               existing_server_default=sa.text('(CURRENT_TIMESTAMP)'))
    op.alter_column('analysis_data', 'updated_at',
               existing_type=sa.TIMESTAMP(),
               nullable=False,
               existing_server_default=sa.text('(CURRENT_TIMESTAMP)'))
    op.alter_column('analysis_data', 'criminal_record',
               existing_type=sa.INTEGER(),
               nullable=False)
    op.create_index(op.f('ix_analysis_data_submitter_id'), 'analysis_data', ['submitter_id'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_analysis_data_submitter_id'), table_name='analysis_data')
    op.alter_column('analysis_data', 'criminal_record',
               existing_type=sa.INTEGER(),
               nullable=True)
    op.alter_column('analysis_data', 'updated_at',
               existing_type=sa.TIMESTAMP(),
               nullable=True,
               existing_server_default=sa.text('(CURRENT_TIMESTAMP)'))
    op.alter_column('analysis_data', 'created_at',
               existing_type=sa.TIMESTAMP(),
               nullable=True,
               existing_server_default=sa.text('(CURRENT_TIMESTAMP)'))
    op.create_table('questionnaire_questions',
    sa.Column('id', sa.INTEGER(), nullable=True),
    sa.Column('questionnaire_type', sa.TEXT(), nullable=False),
    sa.Column('question_number', sa.INTEGER(), nullable=False),
    sa.Column('question_text', sa.TEXT(), nullable=False),
    sa.Column('options', sa.TEXT(), nullable=False),
    sa.Column('scale_name', sa.TEXT(), nullable=True),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('questionnaire_type', 'question_number')
    )
    op.drop_index(op.f('ix_interrogation_records_status'), table_name='interrogation_records')
    op.drop_index(op.f('ix_interrogation_records_interrogator_id'), table_name='interrogation_records')
    op.drop_index(op.f('ix_interrogation_records_id'), table_name='interrogation_records')
    op.drop_table('interrogation_records')
    # ### end Alembic commands ###


--------------------------------------------------
文件路径: alembic\versions\72143d5034d8_create_users_table.py
--------------------------------------------------
"""Create users table

Revision ID: 72143d5034d8
Revises: 
Create Date: 2025-04-24 15:06:06.038114

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '72143d5034d8'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    pass


def downgrade() -> None:
    """Downgrade schema."""
    pass


--------------------------------------------------
文件路径: alembic\versions\a43553c2237e_add_attributes_and_assessment_.py
--------------------------------------------------
"""Add attributes and assessment_attributes tables

Revision ID: a43553c2237e
Revises: 5976b03ee0fc
Create Date: 2025-05-08 19:19:08.492623

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'a43553c2237e'
down_revision: Union[str, None] = '5976b03ee0fc'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('attributes',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('name', sa.String(length=100), nullable=False),
    sa.Column('description', sa.Text(), nullable=True),
    sa.Column('category', sa.String(length=50), nullable=True),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('attributes', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('ix_attributes_category'), ['category'], unique=False)
        batch_op.create_index(batch_op.f('ix_attributes_id'), ['id'], unique=False)
        batch_op.create_index(batch_op.f('ix_attributes_name'), ['name'], unique=True)

    op.create_table('assessment_attributes',
    sa.Column('assessment_id', sa.Integer(), nullable=False),
    sa.Column('attribute_id', sa.Integer(), nullable=False),
    sa.ForeignKeyConstraint(['assessment_id'], ['analysis_data.id'], ),
    sa.ForeignKeyConstraint(['attribute_id'], ['attributes.id'], ),
    sa.PrimaryKeyConstraint('assessment_id', 'attribute_id')
    )
    with op.batch_alter_table('analysis_data', schema=None) as batch_op:
        batch_op.alter_column('created_at',
               existing_type=sa.TIMESTAMP(),
               nullable=False,
               existing_server_default=sa.text('(CURRENT_TIMESTAMP)'))
        batch_op.alter_column('updated_at',
               existing_type=sa.TIMESTAMP(),
               nullable=False,
               existing_server_default=sa.text('(CURRENT_TIMESTAMP)'))
        batch_op.alter_column('criminal_record',
               existing_type=sa.INTEGER(),
               nullable=False)
        batch_op.create_index(batch_op.f('ix_analysis_data_submitter_id'), ['submitter_id'], unique=False)

    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table('analysis_data', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_analysis_data_submitter_id'))
        batch_op.alter_column('criminal_record',
               existing_type=sa.INTEGER(),
               nullable=True)
        batch_op.alter_column('updated_at',
               existing_type=sa.TIMESTAMP(),
               nullable=True,
               existing_server_default=sa.text('(CURRENT_TIMESTAMP)'))
        batch_op.alter_column('created_at',
               existing_type=sa.TIMESTAMP(),
               nullable=True,
               existing_server_default=sa.text('(CURRENT_TIMESTAMP)'))

    op.drop_table('assessment_attributes')
    with op.batch_alter_table('attributes', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_attributes_name'))
        batch_op.drop_index(batch_op.f('ix_attributes_id'))
        batch_op.drop_index(batch_op.f('ix_attributes_category'))

    op.drop_table('attributes')
    # ### end Alembic commands ###


--------------------------------------------------
文件路径: app\main.py
--------------------------------------------------
# FILE: app/main.py (修改后，包含后台管理 Admin 路由)
import sys
import os
import logging
import traceback

# --- 添加项目根目录到 sys.path ---
# 获取当前文件 (main.py) 所在的目录 (app)
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# 获取项目根目录 (app 目录的上级目录)
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
    print(f"[Main App] 已添加项目根目录 {PROJECT_ROOT} 到 sys.path")
# ------------------------------------

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

# --- 导入配置、解析的 origins 和日志设置 ---
try:
    from app.core.config import settings, parsed_cors_origins
    # 确保 src 目录在 sys.path 中，或者使用相对导入 (如果结构允许)
    # 假设 setup_logging 在 src/utils.py 中
    from src.utils import setup_logging
except ImportError as e:
    print(f"[Main App] 严重错误: 无法导入核心配置或日志设置: {e}")
    print(f"[Main App] 当前 sys.path: {sys.path}") # 打印 sys.path 帮助调试
    traceback.print_exc()
    sys.exit(1)
except Exception as e:
    print(f"[Main App] 严重错误: 在初始导入/配置处理期间发生错误: {e}")
    traceback.print_exc()
    sys.exit(1)
# ---------------------------------------------------------

# --- 设置集中式日志 ---
APP_LOGGER_NAME = settings.APP_NAME or "QingtingzheApp"
try:
    # 确保日志目录存在
    log_dir = settings.LOGS_DIR
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
        print(f"[Main App] 已创建日志目录: {log_dir}")

    setup_logging(log_level_str=settings.LOG_LEVEL,
                  log_dir_name=os.path.basename(log_dir), # 传递目录名，而不是完整路径
                  log_dir_base_path=os.path.dirname(log_dir), # 传递目录的基础路径
                  logger_name=APP_LOGGER_NAME)
    logger = logging.getLogger(APP_LOGGER_NAME)
    logger.info("--- 启动 FastAPI 应用 ---")
    logger.info(f"日志记录器 '{APP_LOGGER_NAME}' 配置成功。日志级别: {settings.LOG_LEVEL}, 日志目录: {settings.LOGS_DIR}")
    logger.info(f"应用名称: {settings.APP_NAME}, 环境: {settings.ENVIRONMENT}")
except Exception as e:
    print(f"[Main App] 严重错误: 无法设置日志: {e}")
    traceback.print_exc()
    # 提供一个基础的回退日志记录器
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(APP_LOGGER_NAME)
    logger.error("日志设置失败，使用基础配置。请检查日志目录权限和配置。")
# ---------------------------------

# --- 导入 API 和 SSE 路由 ---
try:
    # 导入所有需要的路由模块
    from app.routers import scales, assessments, reports, auth, encyclopedia, sse
    # +++ 导入后台管理路由 +++
    from app.routers import admin
    logger.info("成功导入 API、SSE 和 Admin 路由。") # <--- 更新日志信息
except ImportError as e:
    # 提供更详细的导入错误信息
    logger.critical(f"严重错误: 无法导入路由模块: {e}", exc_info=True)
    # 尝试找出哪个模块导入失败 (如果 Python 版本 >= 3.6)
    if hasattr(e, 'name') and e.name:
        logger.critical(f"无法导入的模块名: {e.name}")
    if hasattr(e, 'path') and e.path:
        logger.critical(f"尝试导入的路径: {e.path}")
    sys.exit(1)
except Exception as e:
    logger.critical(f"严重错误: 在路由导入期间发生未知错误: {e}", exc_info=True)
    sys.exit(1)
# -------------------------

# --- 创建 FastAPI 应用实例 ---
app = FastAPI(
    title=settings.APP_NAME,
    description="倾听者 AI 智能警务分析评估应用系统 API",
    version="0.4.0", # <--- 更新版本号
    openapi_url=f"{settings.API_V1_STR}/openapi.json", # API 文档的 JSON 描述路径
    docs_url="/docs", # Swagger UI 文档路径
    redoc_url="/redoc", # ReDoc 文档路径
)
logger.info("FastAPI 应用实例已创建。")
logger.info(f"API 文档可在 /docs (Swagger UI) 或 /redoc (ReDoc) 访问。")
logger.info(f"OpenAPI 规范位于 {settings.API_V1_STR}/openapi.json。")
# ---------------------------------

# --- CORS 中间件配置 ---
# 跨源资源共享 (CORS) 配置，允许指定来源的前端访问 API
if parsed_cors_origins:
    # +++ 确保 CORS 配置允许后台前端源 (如果与主前端不同) +++
    # 例如，如果后台前端在 localhost:8081
    # parsed_cors_origins.append("http://localhost:8081")
    # parsed_cors_origins.append("http://127.0.0.1:8081")
    logger.info(f"最终应用的 CORS 来源: {parsed_cors_origins}")

    app.add_middleware(
        CORSMiddleware,
        allow_origins=parsed_cors_origins, # 允许的来源列表
        allow_credentials=True,          # 允许携带 Cookie
        allow_methods=["*"],             # 允许所有 HTTP 方法 (GET, POST, PUT, DELETE 等)
        allow_headers=["*"],             # 允许所有请求头
    )
    logger.info(f"CORS 中间件已启用，允许的来源: {parsed_cors_origins}")
else:
    logger.warning("CORS 源列表为空或解析失败 (parsed_cors_origins 为空)。")
    logger.warning("如果前端应用需要访问 API，请检查 .env 或配置中的 BACKEND_CORS_ORIGINS 设置。")
# ----------------------------------

# --- 挂载静态文件 (已移除) ---
# logger.info("静态文件挂载功能已移除。此应用现在专注于 API。")

# --- 提供根前端页面 (已移除) ---
# logger.info("根路径 '/' 的 HTML 响应功能已移除。")

# --- 包含 API 路由 ---
logger.info(f"开始包含公共 API 路由，统一前缀: {settings.API_V1_STR}")
app.include_router(auth.router, prefix=settings.API_V1_STR, tags=["认证 (Auth)"]) # Public Auth
app.include_router(scales.router, prefix=settings.API_V1_STR, tags=["量表 (Scales)"])
app.include_router(assessments.router, prefix=settings.API_V1_STR, tags=["评估 (Assessments)"])
app.include_router(reports.router, prefix=f"{settings.API_V1_STR}/reports", tags=["报告 (Reports)"])
app.include_router(encyclopedia.router, prefix=settings.API_V1_STR, tags=["心理百科 (Encyclopedia)"])
logger.info(f"已成功包含公共 API 路由，挂载于 {settings.API_V1_STR} 下。")
# -------------------------

# +++ 包含后台管理 API 路由 +++
# admin router 内部已设置了 /admin 前缀和权限依赖
logger.info(f"开始包含后台管理 API 路由，统一前缀: {settings.API_V1_STR}{admin.router.prefix}")
app.include_router(admin.router, prefix=settings.API_V1_STR) # 将 admin router 添加到 /api/v1 下
logger.info(f"已成功包含后台管理 API 路由，挂载于 {settings.API_V1_STR}{admin.router.prefix} 下。")
# +++++++++++++++++++++++++++++++

# --- 包含 SSE 路由 ---
logger.info("开始包含 SSE (Server-Sent Events) 路由。")
app.include_router(sse.router, tags=["SSE 事件流"]) # 保持在根路径
logger.info("已成功包含 SSE 路由。")
# -------------------------

# --- 包含页面路由 (已移除) ---
# logger.info("页面 (Pages) 路由包含功能已移除。")

# --- 最终启动消息 ---
logger.info("=" * 50)
logger.info(f"FastAPI 应用 '{settings.APP_NAME}' 已完成配置并准备就绪。")
logger.info(f"环境: {settings.ENVIRONMENT}")
logger.info("服务已启动，等待接收请求...")
logger.info("=" * 50)
# ---------------------------

# --- Uvicorn 运行部分 (通常在生产环境中由 Gunicorn + Uvicorn worker 管理) ---
# 以下代码块主要用于开发时直接运行此文件 (python app/main.py)
if __name__ == "__main__":
    import uvicorn
    logger.info("检测到直接运行 main.py，启动 Uvicorn 开发服务器...")
    # 尝试从配置中读取 HOST 和 PORT，提供默认值
    run_host = getattr(settings, 'HOST', '0.0.0.0') # 允许所有 IP 连接
    # 使用 settings.PORT 或从 .env 加载 PORT，默认 8000
    run_port_str = os.environ.get('PORT', '8000')
    try:
        run_port = int(run_port_str)
    except ValueError:
        logger.warning(f"无效的 PORT 环境变量值 '{run_port_str}'，使用默认端口 8000。")
        run_port = 8000

    reload_flag = settings.ENVIRONMENT == "development" # 只在开发环境启用自动重载

    logger.info(f"服务器将在 http://{run_host}:{run_port} 上运行")
    if reload_flag:
        logger.info("开发模式：已启用代码自动重载 (reload=True)。")
    else:
        logger.info("生产/其他模式：未启用代码自动重载 (reload=False)。")

    try:
        uvicorn.run(
            "app.main:app",         # 指向 FastAPI 应用实例
            host=run_host,          # 监听地址
            port=run_port,          # 监听端口
            reload=reload_flag,     # 是否启用自动重载
            log_level=settings.LOG_LEVEL.lower() # 将日志级别传递给 Uvicorn
        )
    except Exception as e:
        logger.critical(f"启动 Uvicorn 服务器失败: {e}", exc_info=True)
        sys.exit(1)

--------------------------------------------------
文件路径: app\__init__.py
--------------------------------------------------
# # FILE: app/crud/__init__.py (Corrected)
# from . import user      # 导入 user.py 中的内容
# from . import assessment # +++ 添加这一行，导入 assessment.py 中的内容 +++


--------------------------------------------------
文件路径: app\core\celery_app.py
--------------------------------------------------
# app/core/celery_app.py
from celery import Celery
from app.core.config import settings # 导入你的设置

# 配置 Redis 作为 Broker 和 Backend
# 使用 settings 中的配置或默认值
REDIS_URL = "redis://localhost:6379/0" # 默认 Redis 地址和数据库 0
# 你可以在 .env 中添加 REDIS_URL 并从 settings 加载

# 创建 Celery 实例
# main 参数通常是 Celery 应用的入口点名称，这里用 'app' 或项目名
celery_app = Celery(
    "QingtingzheApp", # 与 FastAPI app name 保持一致或自定义
    broker=REDIS_URL,
    backend=REDIS_URL, # 使用 Redis 作为结果存储后端
    include=['app.tasks.analysis'] # 指定包含任务定义的模块列表
)

# 可选：Celery 配置项 (可以放在 settings 或这里)
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],  # Allow json content
    result_serializer='json',
    timezone='Asia/Shanghai', # 设置时区
    enable_utc=True,
    # task_track_started=True, # 如果需要追踪任务开始状态
    # broker_connection_retry_on_startup=True, # 启动时自动重试连接 broker
)

# 可选: 打印确认信息
print(f"[Celery Setup] Celery app configured. Broker: {REDIS_URL}, Backend: {REDIS_URL}")
print(f"[Celery Setup] Included task modules: {celery_app.conf.include}")

# 如果你需要在任务中使用 FastAPI 的依赖项或设置，
# 可以考虑更复杂的设置，但现在保持简单。

--------------------------------------------------
文件路径: app\core\config.py
--------------------------------------------------
# app/core/config.py
import os
import sys
import json
from typing import List, Union, Optional, Dict, Any
from pydantic_settings import BaseSettings, SettingsConfigDict
import yaml
import logging
import traceback

# --- Basic Logging Setup (用于配置加载本身) ---
# 获取一个专门用于配置加载问题的基本日志记录器实例
# 这避免了主应用程序记录器尚未配置时可能出现的问题
config_logger = logging.getLogger("ConfigLoader")
# 如果主设置尚未运行，则设置默认级别
config_logger.setLevel(logging.INFO)
# 如果没有处理程序存在 (例如，直接运行脚本)，则添加一个处理程序
if not config_logger.hasHandlers():
    ch = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    config_logger.addHandler(ch)
# ---------------------------------------------------------

# --- 项目根目录计算 ---
# __file__ 指向此文件 (config.py)
# os.path.dirname(__file__) 指向 app/core
# os.path.join(os.path.dirname(__file__), "..", "..") 指向项目根目录 PsychologyAnalysis/
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
CONFIG_YAML_PATH = os.path.join(PROJECT_ROOT, "config/config.yaml") # YAML 配置文件路径
DOTENV_PATH = os.path.join(PROJECT_ROOT, ".env")                     # .env 文件路径
ENCYCLOPEDIA_JSON_PATH = os.path.join(PROJECT_ROOT, "config/psychology_encyclopedia.json") # 百科文件路径

class Settings(BaseSettings):
    """
    应用程序设置，从 .env 文件和 YAML 加载。
    对于重叠的变量，.env 文件具有优先权。
    """
    # --- 基本应用设置 ---
    APP_NAME: str = "Qingtingzhe AI Analysis"       # 应用名称
    ENVIRONMENT: str = "development"                # 环境 (development, staging, production)
    API_V1_STR: str = "/api/v1"                     # API V1 端点的基本路径
    LOG_LEVEL: str = "INFO"                         # 默认日志级别 (DEBUG, INFO, WARNING, ERROR)

    # --- 安全设置 (JWT 认证) ---
    SECRET_KEY: str = "a_very_unsafe_default_secret_key_please_change_in_dotenv" # JWT 密钥，务必在 .env 中修改
    ALGORITHM: str = "HS256"                        # JWT 签名算法
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24      # 访问令牌过期时间 (分钟)，默认为 1 天

    # --- CORS (跨源资源共享) ---
    # 允许访问后端的源列表 (逗号分隔的字符串)
    BACKEND_CORS_ORIGINS_STR: str = "http://localhost:8080,http://127.0.0.1:8080,http://localhost:5173,http://127.0.0.1:5173" # 添加 Vite 默认开发端口

    # --- 数据库 ---
    # SQLite 数据库文件路径
    DB_PATH_SQLITE: str = os.path.join(PROJECT_ROOT, "psychology_analysis.db")
    # 数据库连接 URL (用于 SQLAlchemy AsyncIO)
    DATABASE_URL: str = f"sqlite+aiosqlite:///{DB_PATH_SQLITE}"

    # --- 文件存储 ---
    UPLOADS_DIR: str = os.path.join(PROJECT_ROOT, "uploads") # 上传文件存储目录
    LOGS_DIR: str = os.path.join(PROJECT_ROOT, "logs")       # 日志文件存储目录

    # --- Dashscope (阿里云灵积) API ---
    # API 密钥，优先从 .env 读取，否则尝试从 YAML 读取
    DASHSCOPE_API_KEY: Optional[str] = None

    # --- Redis 配置 --- # <--- 添加 Redis 配置部分
    # Redis 连接 URL，格式: redis://:[password@]host:port/db
    # 示例: redis://localhost:6379/0 (本地无密码)
    # 示例: redis://:mypassword@myredishost:6379/1 (带密码，使用数据库 1)
    REDIS_URL: str = "redis://localhost:6379/0"  # <-- 确保此行存在且具有合理的默认值

    # --- 可能从 config.yaml 加载的设置 (作为 .env 未设置时的备选或默认值) ---
    TEXT_MODEL: str = "qwen-plus"                       # 默认使用的文本生成模型
    VISION_MODEL: str = "qwen-vl-plus"                  # 默认使用的视觉理解模型
    REPORT_PROMPT_TEMPLATE: Optional[str] = None        # 报告生成的提示模板 (可选，可从 YAML 加载)
    YAML_CONFIG: Dict[str, Any] = {}                    # 用于存储从 YAML 加载的原始配置数据

    # --- 心理学百科全书 ---
    # 百科全书 JSON 文件路径
    PSYCHOLOGY_ENCYCLOPEDIA_FILE: str = ENCYCLOPEDIA_JSON_PATH
    # 加载后的百科条目列表 (每个条目是字典)
    PSYCHOLOGY_ENTRIES: List[Dict[str, str]] = []

    # Pydantic Settings 配置
    model_config = SettingsConfigDict(
        env_file=DOTENV_PATH,  # 明确指定 .env 文件路径
        extra='ignore'         # 忽略 .env 或环境变量中未在 Settings 类中定义的额外字段
    )

# --- 辅助函数：加载 YAML 配置 ---
def load_yaml_config(yaml_path: str) -> Dict[str, Any]:
    """从 YAML 文件加载配置。"""
    if not os.path.exists(yaml_path):
        config_logger.warning(f"在 {yaml_path} 未找到 YAML 配置文件。返回空配置。")
        return {}
    try:
        with open(yaml_path, 'r', encoding='utf-8') as f:
            config_data = yaml.safe_load(f)
            if config_data is None:
                config_logger.warning(f"位于 {yaml_path} 的 YAML 配置文件为空。返回空配置。")
                return {}
            config_logger.info(f"成功从 {yaml_path} 加载配置。")
            return config_data
    except Exception as e:
        config_logger.error(f"从 {yaml_path} 加载 YAML 配置时出错: {e}", exc_info=True)
        return {}

# --- 辅助函数：从 JSON 加载百科全书 ---
def load_encyclopedia_from_json(encyclopedia_path: str) -> List[Dict[str, str]]:
    """从 JSON 文件加载百科条目列表，并进行基本验证。"""
    if not os.path.exists(encyclopedia_path):
        config_logger.warning(f"百科文件未找到: {encyclopedia_path}。返回空列表。")
        return []
    try:
        with open(encyclopedia_path, 'r', encoding='utf-8') as f:
            entries_list = json.load(f)
            # 验证根元素是否为列表
            if not isinstance(entries_list, list):
                config_logger.error(f"百科文件 {encyclopedia_path} 格式错误，根元素不是列表。")
                return []

            valid_entries = []
            required_keys = {"category", "title", "content"} # 每个条目必需的键
            for i, entry in enumerate(entries_list):
                # 验证条目是否为字典且包含所有必需的键
                if isinstance(entry, dict) and required_keys.issubset(entry.keys()):
                    # 验证必需键的值是否为非空字符串
                    if all(isinstance(entry[key], str) and entry[key].strip() for key in required_keys):
                        # 添加清理过的条目
                        valid_entries.append({
                            "category": entry["category"].strip(),
                            "title": entry["title"].strip(),
                            "content": entry["content"].strip()
                        })
                    else:
                        config_logger.warning(f"百科文件 {encyclopedia_path} 中第 {i+1} 条记录的值无效 (非字符串或为空)，已跳过。")
                else:
                    config_logger.warning(f"百科文件 {encyclopedia_path} 中第 {i+1} 条记录格式错误或缺少必需的键 ({required_keys})，已跳过。")

            config_logger.info(f"成功从 {encyclopedia_path} 加载 {len(valid_entries)} 条有效的百科条目。")
            return valid_entries
    except json.JSONDecodeError as e:
        config_logger.error(f"解析百科 JSON 文件 {encyclopedia_path} 时出错: {e}", exc_info=True)
        return []
    except Exception as e:
        config_logger.error(f"加载百科文件 {encyclopedia_path} 时发生意外错误: {e}", exc_info=True)
        return []
# ---------------------------------------------------------

# --- 实例化设置 (首先从 .env 加载) ---
try:
    settings = Settings()
except Exception as e:
    # 如果在 Pydantic 模型验证或 .env 文件读取期间发生严重错误
    config_logger.critical(f"在 Settings 初始化期间发生严重错误: {e}")
    traceback.print_exc() # 打印详细的错误堆栈
    sys.exit(1) # 关键配置失败，退出程序

# --- 关键安全检查：默认 SECRET_KEY ---
if settings.SECRET_KEY == "a_very_unsafe_default_secret_key_please_change_in_dotenv":
    config_logger.critical("="*30 + " 关键安全警告 " + "="*30)
    config_logger.critical("SECRET_KEY 正在使用默认的不安全值！")
    config_logger.critical("请生成一个强密钥并将其设置在 .env 文件中。")
    config_logger.critical("示例生成命令: openssl rand -hex 32")
    config_logger.critical("="*80 + "\n")
    # 生产环境中建议取消注释下一行，强制要求设置密钥
    # sys.exit(1)

# --- 加载并合并 YAML 配置 ---
yaml_config_data = load_yaml_config(CONFIG_YAML_PATH)
settings.YAML_CONFIG = yaml_config_data # 存储原始 YAML 数据

# --- 如果 .env 中未设置，则从 YAML 加载特定设置作为备选 ---
# 检查 DASHSCOPE_API_KEY 是否已从 .env 加载，如果没有，则尝试从 YAML 加载
if settings.DASHSCOPE_API_KEY is None:
    yaml_api_key = yaml_config_data.get("api_key") # 假设 YAML 中的键是 'api_key'
    if yaml_api_key:
        settings.DASHSCOPE_API_KEY = yaml_api_key
        config_logger.warning("从 YAML 加载了 DASHSCOPE_API_KEY (推荐使用 .env)。")

# 如果模型设置仍为默认值，则尝试从 YAML 加载
if settings.TEXT_MODEL == "qwen-plus": # 检查是否仍是类定义中的默认值
    settings.TEXT_MODEL = yaml_config_data.get("text_model", settings.TEXT_MODEL)
if settings.VISION_MODEL == "qwen-vl-plus": # 检查是否仍是类定义中的默认值
    settings.VISION_MODEL = yaml_config_data.get("vision_model", settings.VISION_MODEL)

# 如果报告模板未从 .env 加载，则尝试从 YAML 加载
if settings.REPORT_PROMPT_TEMPLATE is None:
    settings.REPORT_PROMPT_TEMPLATE = yaml_config_data.get("REPORT_PROMPT_TEMPLATE", settings.REPORT_PROMPT_TEMPLATE)


# --- 加载心理学百科全书 ---
settings.PSYCHOLOGY_ENTRIES = load_encyclopedia_from_json(settings.PSYCHOLOGY_ENCYCLOPEDIA_FILE)
if not settings.PSYCHOLOGY_ENTRIES:
    config_logger.warning("未能加载任何心理百科条目，相关功能可能受影响。")

# --- 构造数据库 URL (主要用于日志记录和确认) ---
# 实际的 URL 在 Settings 类中已基于 DB_PATH_SQLITE 构建
config_logger.info(f"使用的数据库 URL: {settings.DATABASE_URL}")

# --- 确保必要的目录存在 ---
try:
    # exist_ok=True 表示如果目录已存在则不引发错误
    os.makedirs(settings.UPLOADS_DIR, exist_ok=True)
    os.makedirs(settings.LOGS_DIR, exist_ok=True)
    config_logger.info(f"确保目录存在: 上传='{settings.UPLOADS_DIR}', 日志='{settings.LOGS_DIR}'")
except OSError as e:
     # 如果创建目录时出错 (例如权限问题)
     config_logger.error(f"创建必要目录时出错: {e}", exc_info=True)

# --- 检查关键的 API 密钥是否已加载 ---
if not settings.DASHSCOPE_API_KEY:
    config_logger.critical("="*30 + " 关键警告 " + "="*30)
    config_logger.critical("DASHSCOPE_API_KEY 未在 .env 或 config.yaml 中设置！")
    config_logger.critical("AI 相关功能很可能会失败。")
    config_logger.critical("请在 .env 文件 (推荐) 或 config/config.yaml 中设置它。")
    config_logger.critical("="*80 + "\n")

# --- 手动解析 CORS 源字符串 ---
def parse_cors_origins(origins_str: str) -> List[str]:
    """将逗号分隔的源字符串解析为字符串列表。"""
    if not origins_str or not isinstance(origins_str, str):
        # 如果字符串为空或无效，使用包含 Vite 端口的更新后默认值
        default_origins = ["http://localhost:8080", "http://127.0.0.1:8080", "http://localhost:5173", "http://127.0.0.1:5173"]
        config_logger.warning(f"BACKEND_CORS_ORIGINS_STR 为空或无效 ('{origins_str}')。使用默认值: {default_origins}")
        return default_origins
    try:
        # 分割字符串，去除空白，并过滤掉空字符串
        parsed = [origin.strip() for origin in origins_str.split(",") if origin.strip()]
        if not parsed:
             # 如果解析后列表为空 (例如，字符串只包含逗号或空格)
             default_origins = ["http://localhost:8080", "http://127.0.0.1:8080", "http://localhost:5173", "http://127.0.0.1:5173"]
             config_logger.warning(f"解析 BACKEND_CORS_ORIGINS_STR '{origins_str}' 得到空列表。使用默认值: {default_origins}")
             return default_origins
        config_logger.info(f"成功解析 CORS 源: {parsed}")
        return parsed
    except Exception as e:
        # 捕获解析过程中可能出现的其他异常
        default_origins = ["http://localhost:8080", "http://127.0.0.1:8080", "http://localhost:5173", "http://127.0.0.1:5173"]
        config_logger.error(f"解析 BACKEND_CORS_ORIGINS_STR '{origins_str}' 时出错: {e}。使用默认值: {default_origins}", exc_info=True)
        return default_origins

# 解析 CORS 源并存储结果
parsed_cors_origins: List[str] = parse_cors_origins(settings.BACKEND_CORS_ORIGINS_STR)

# --- 记录最终生效的设置 ---
config_logger.info("--- 生效的应用程序设置 ---")
config_logger.info(f"应用名称: {settings.APP_NAME}")
config_logger.info(f"运行环境: {settings.ENVIRONMENT}")
config_logger.info(f"日志级别: {settings.LOG_LEVEL}")
config_logger.info(f"API V1 前缀: {settings.API_V1_STR}")
config_logger.info(f"数据库 URL: {settings.DATABASE_URL}")
config_logger.info(f"Redis URL: {settings.REDIS_URL}") # <--- 添加 Redis URL 日志记录
config_logger.info(f"上传目录: {settings.UPLOADS_DIR}")
config_logger.info(f"日志目录: {settings.LOGS_DIR}")
config_logger.info(f"CORS 源 (已解析): {parsed_cors_origins}")
config_logger.info(f"JWT 算法: {settings.ALGORITHM}")
config_logger.info(f"令牌过期时间 (分钟): {settings.ACCESS_TOKEN_EXPIRE_MINUTES}")
# 提示密钥是否已从默认值更改
config_logger.info(f"SECRET_KEY 已加载: {'是' if settings.SECRET_KEY != 'a_very_unsafe_default_secret_key_please_change_in_dotenv' else '否 (使用默认值 - 不安全!)'}")
# 提示 API 密钥是否已加载
config_logger.info(f"DASHSCOPE_API_KEY 已加载: {'是' if settings.DASHSCOPE_API_KEY else '否 - 关键警告!'}")
config_logger.info(f"文本模型: {settings.TEXT_MODEL}")
config_logger.info(f"视觉模型: {settings.VISION_MODEL}")
config_logger.info(f"已加载百科条目数: {len(settings.PSYCHOLOGY_ENTRIES)}")
config_logger.info("------------------------------------")

# +++ 添加: 打印最终确定的数据库绝对路径 (用于调试) +++
try:
    abs_db_path = os.path.abspath(settings.DB_PATH_SQLITE)
    config_logger.info(f"!!! Settings 使用的绝对数据库路径: {abs_db_path}")
except Exception as e_path:
    config_logger.error(f"!!! 无法确定绝对数据库路径: {e_path}")
# ++++++++++++++++++++++++++++++++++++++++++

# `settings` 实例现在可以被其他模块导入使用:
# from app.core.config import settings

--------------------------------------------------
文件路径: app\core\deps.py
--------------------------------------------------
# FILE: app/core/deps.py (修改后，添加 get_current_active_superuser)
import logging
from typing import AsyncGenerator, Optional # Use AsyncGenerator for async yield
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from sqlalchemy.ext.asyncio import AsyncSession # Import AsyncSession

# --- Core App Imports ---
from app.core.config import settings
from app.core.security import decode_access_token # Your JWT decoding function

# --- Database and CRUD Imports ---
from app.db.session import AsyncSessionLocal # Import the async session maker
from app import crud, models, schemas # Adjust imports based on your project structure

logger = logging.getLogger(settings.APP_NAME) # Use the main app logger

# --- OAuth2 Scheme Definition ---
# Define the URL where clients will send username/password to get a token.
# This should match the path operation of your login endpoint.
oauth2_scheme = OAuth2PasswordBearer(tokenUrl=f"{settings.API_V1_STR}/auth/token")

# --- Asynchronous Database Session Dependency ---
async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """
    Dependency that provides an asynchronous database session per request.
    It ensures the session is properly closed afterwards.
    """
    async with AsyncSessionLocal() as session:
        try:
            yield session
            # Optional: You could uncomment the next line if you want commits
            # to happen automatically at the end of successful requests,
            # but manual commits in CRUD functions are generally preferred
            # for better control.
            # await session.commit()
        except Exception as e:
            # Rollback in case of exceptions during the request handling
            logger.error(f"数据库会话错误: {e}", exc_info=True)
            await session.rollback()
            # Re-raise the exception so FastAPI can handle it
            raise
        # Session is automatically closed when exiting the 'async with' block

# --- Asynchronous Current User Dependency ---
async def get_current_user(
    db: AsyncSession = Depends(get_db),          # Depend on the async get_db
    token: str = Depends(oauth2_scheme)          # Get token from Authorization header
) -> models.User:                                # Return the SQLAlchemy User model
    """
    Decodes the JWT token, validates it, and retrieves the current user
    from the database asynchronously. Raises HTTPException if invalid.
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="无法验证凭据", # "Could not validate credentials"
        headers={"WWW-Authenticate": "Bearer"},
    )

    # Decode the token using your security utility
    token_data = decode_access_token(token)
    if token_data is None or token_data.username is None:
        logger.warning("Token 解码失败或 Token 负载中缺少用户名。")
        raise credentials_exception

    # Retrieve the user from the database asynchronously using the async CRUD function
    try:
        # Ensure crud.user.get_user_by_username is an async function
        user = await crud.user.get_user_by_username(db, username=token_data.username)
    except Exception as e:
        logger.error(f"获取用户 '{token_data.username}' 时发生数据库错误: {e}", exc_info=True)
        # Don't expose internal DB errors directly, raise the standard credentials exception
        raise credentials_exception

    if user is None:
        logger.warning(f"用户 '{token_data.username}' 在 Token 中找到但在数据库中未找到。")
        raise credentials_exception

    # Return the validated user object
    return user

# --- Asynchronous Active User Dependency ---
async def get_current_active_user(
    current_user: models.User = Depends(get_current_user), # Depend on get_current_user
) -> models.User:
    """
    Ensures the user retrieved from the token is marked as active.
    """
    if not current_user.is_active:
        logger.warning(f"非活动用户尝试认证: {current_user.username}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="非活动用户") # "Inactive user"
    return current_user

# +++ 新增：超级用户依赖项 +++
async def get_current_active_superuser(
    current_user: models.User = Depends(get_current_active_user), # 首先确保用户是活动的
) -> models.User:
    """
    确保当前用户是活动的 **并且** 是超级用户。
    如果不是超级用户，则引发 HTTPException。
    此依赖项应用于所有需要管理员权限的接口。
    """
    if not current_user.is_superuser:
        logger.warning(f"非超级用户拒绝访问管理接口: {current_user.username}")
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN, # 403 Forbidden 表示权限不足
            detail="用户权限不足", # "The user doesn't have enough privileges"
        )
    # 如果检查通过，记录调试信息并返回用户对象
    logger.debug(f"超级用户访问已授权: {current_user.username}")
    return current_user

--------------------------------------------------
文件路径: app\core\redis_client.py
--------------------------------------------------


--------------------------------------------------
文件路径: app\core\security.py
--------------------------------------------------
# app/core/security.py
from datetime import datetime, timedelta, timezone
from typing import Any, Union, Optional
from jose import jwt, JWTError
from passlib.context import CryptContext
from app.core.config import settings
from app.schemas.token import TokenData # 导入令牌数据模式

# 使用 bcrypt 哈希算法
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

ALGORITHM = settings.ALGORITHM
SECRET_KEY = settings.SECRET_KEY
ACCESS_TOKEN_EXPIRE_MINUTES = settings.ACCESS_TOKEN_EXPIRE_MINUTES

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """验证明文密码与哈希密码是否匹配"""
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    """生成密码的哈希值"""
    return pwd_context.hash(password)

def create_access_token(
    subject: Union[str, Any], expires_delta: Optional[timedelta] = None
) -> str:
    """创建 JWT 访问令牌"""
    if expires_delta:
        expire = datetime.now(timezone.utc) + expires_delta
    else:
        # 使用配置中的过期时间
        expire = datetime.now(timezone.utc) + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    # 令牌中至少包含过期时间和主题 (subject, 通常是用户名)
    to_encode = {"exp": expire, "sub": str(subject)}
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

def decode_access_token(token: str) -> Optional[TokenData]:
    """解码访问令牌，验证其有效性"""
    try:
        # 解码 JWT
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        # 提取用户名
        username: str | None = payload.get("sub")
        if username is None:
            # 令牌中没有 'sub' 字段
            return None
        # 你可以在这里添加更多的载荷验证逻辑 (例如：检查 scopes)
        # 返回包含用户名的 TokenData 对象
        return TokenData(username=username)
    except JWTError:
        # 令牌无效 (格式错误、签名不匹配、已过期等)
        return None

--------------------------------------------------
文件路径: app\core\__init__.py
--------------------------------------------------


--------------------------------------------------
文件路径: app\crud\assessment.py
--------------------------------------------------
# -*- coding: utf-8 -*-
# FILE: app/crud/assessment.py (修改后，包含属性关联操作)
import logging
import traceback
from typing import Optional, Dict, Any, List
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from sqlalchemy import desc
from sqlalchemy.exc import SQLAlchemyError, IntegrityError
import sqlite3

# --- 模型和配置导入 ---
from app.models.assessment import Assessment, STATUS_COMPLETE, STATUS_PENDING, STATUS_PROCESSING, STATUS_FAILED
# +++ 导入 Attribute 模型 +++
from app.models.attribute import Attribute
# ++++++++++++++++++++++++
from app.core.config import settings

logger = logging.getLogger(settings.APP_NAME)

# --- 评估记录 (Assessment) 的 CRUD 操作 ---

async def get(db: AsyncSession, id: int) -> Optional[Assessment]:
    """
    异步根据 ID 获取评估记录。
    """
    logger.debug(f"CRUD GET: 尝试查找 ID 为 {id} 的评估记录")
    try:
        # 使用 SQLAlchemy 2.0 风格的 select
        result = await db.execute(select(Assessment).filter(Assessment.id == id))
        found_obj = result.scalar_one_or_none()
        if found_obj:
            logger.debug(f"CRUD GET: 已找到 ID 为 {id} 的评估记录")
        else:
            logger.debug(f"CRUD GET: 在数据库中未找到 ID 为 {id} 的评估记录")
        return found_obj
    except SQLAlchemyError as e:
        logger.error(f"CRUD GET: 获取评估记录 ID {id} 时发生数据库错误: {e}", exc_info=True)
        # 不在 CRUD 层抛出 HTTPException，让上层调用者处理
        raise e # 或者根据调用者期望返回 None

async def create(db: AsyncSession, **kwargs: Any) -> Assessment:
    """
    异步创建一条新的评估记录。
    kwargs 应包含 Assessment 模型所需的所有字段（除了自动生成的 id, created_at, updated_at）。
    确保在成功时返回带有 ID 的对象，否则引发异常。
    """
    logger.info(f"CRUD CREATE: 准备为 '{kwargs.get('subject_name', '未知主题')}' 创建评估记录")
    try:
        # 移除不允许由用户指定的字段，或由数据库自动处理的字段
        kwargs.pop('id', None)
        kwargs.pop('created_at', None)
        kwargs.pop('updated_at', None)
        # 确保 status 使用默认值 'pending'
        if 'status' in kwargs:
            logger.warning("CRUD CREATE: 在参数中提供了 'status' 字段，将忽略并使用模型默认值。")
            kwargs.pop('status', None)
        # --- 移除 attributes 字段，关联应单独处理 ---
        if 'attributes' in kwargs:
             logger.warning("CRUD CREATE: 在参数中提供了 'attributes' 字段，应通过关联函数处理，已忽略。")
             kwargs.pop('attributes', None)
        # -----------------------------------------

        db_obj = Assessment(**kwargs)
        logger.debug("CRUD CREATE: 评估对象已在内存中创建。")
    except TypeError as te:
        logger.error(f"CRUD CREATE: 创建 Assessment 实例时发生 TypeError。提供的参数: {kwargs}", exc_info=True)
        raise TypeError(f"模型初始化字段不匹配或类型错误: {te}") from te

    db.add(db_obj)
    logger.debug("CRUD CREATE: 评估对象已添加到 SQLAlchemy 会话中。")

    try:
        logger.info("CRUD CREATE: 尝试提交数据库事务以保存新的评估记录...")
        await db.commit()
        logger.info("CRUD CREATE: 数据库提交成功。")

        # 验证对象是否仍在会话中（健全性检查）
        if db_obj not in db and hasattr(db, 'is_active') and db.is_active:
             logger.error("CRUD CREATE: 严重错误 - 对象在成功提交后意外地从会话中移除！")
             await db.rollback() # 尽管提交成功，但状态异常，回滚可能意义不大，但记录错误
             raise SQLAlchemyError("对象在提交后从会话中丢失")

        logger.debug(f"CRUD CREATE: 尝试刷新对象状态。刷新前的对象 ID (如果内存中存在): {getattr(db_obj, 'id', 'N/A')}")
        await db.refresh(db_obj)
        logger.info(f"CRUD CREATE: 评估对象已成功刷新。")

        # 验证 ID 是否已分配
        if db_obj.id is None:
            logger.critical(f"CRUD CREATE: 严重错误 - 对象已刷新但其 ID 仍为 None！回滚可能无效，需检查数据库。")
            # 回滚可能无法撤销已提交的更改，但尝试一下
            try: await db.rollback()
            except: pass
            raise SQLAlchemyError("数据库在提交和刷新操作后未能成功分配 ID")

        logger.info(f"CRUD CREATE: 成功创建并刷新评估记录，最终 ID: {db_obj.id}")
        return db_obj

    except (IntegrityError, sqlite3.IntegrityError) as ie:
        error_msg = f"数据库完整性错误: {ie}"
        logger.error(f"CRUD CREATE: {error_msg}", exc_info=False) # 通常不需要完整堆栈
        logger.info("CRUD CREATE: 由于发生完整性错误，正在回滚会话...")
        await db.rollback()
        # 抛出更具体的错误给上层
        raise IntegrityError(f"数据保存冲突或违反约束: {ie}", orig=ie, params=kwargs) from ie
    except (SQLAlchemyError, sqlite3.OperationalError) as db_err: # 捕获更广泛的 DB 错误
        error_msg = f"数据库操作错误: {type(db_err).__name__} - {db_err}"
        logger.error(f"CRUD CREATE: {error_msg}", exc_info=True)
        logger.info("CRUD CREATE: 由于发生数据库错误，正在回滚会话...")
        await db.rollback()
        raise SQLAlchemyError(f"数据库操作失败: {db_err}") from db_err
    except Exception as e: # 捕获其他意外错误
        error_msg = f"创建评估记录期间发生意外错误: {e}"
        logger.error(f"CRUD CREATE: {error_msg}", exc_info=True)
        logger.info("CRUD CREATE: 由于发生一般错误，正在回滚会话...")
        try:
            await db.rollback()
        except Exception as rollback_err:
            logger.error(f"CRUD CREATE: 在错误处理中尝试回滚会话时再次发生错误: {rollback_err}", exc_info=True)
        raise e # 重新抛出原始错误

async def update_status(db: AsyncSession, assessment_id: int, new_status: str) -> Optional[Assessment]:
    """仅更新指定评估记录的状态。"""
    logger.info(f"CRUD UPDATE STATUS: 尝试将评估记录 ID {assessment_id} 的状态更新为 '{new_status}'")
    db_obj = await get(db, id=assessment_id)
    if not db_obj:
        logger.warning(f"CRUD UPDATE STATUS: 未找到评估记录 ID {assessment_id}。无法更新状态。")
        return None

    if db_obj.status == new_status:
        logger.info(f"CRUD UPDATE STATUS: 评估记录 ID {assessment_id} 的状态已经是 '{new_status}'，无需更新。")
        return db_obj

    logger.debug(f"CRUD UPDATE STATUS: 找到评估记录 ID {assessment_id}。当前状态: '{db_obj.status}'。正在更新为 '{new_status}'。")
    db_obj.status = new_status
    db.add(db_obj) # 将更改添加到会话

    try:
        logger.info(f"CRUD UPDATE STATUS: 尝试提交数据库事务以更新状态 (ID: {assessment_id}, 新状态: {new_status})...")
        await db.commit()
        logger.info(f"CRUD UPDATE STATUS: 数据库提交成功，状态已更新 (ID: {assessment_id})。")
        await db.refresh(db_obj)
        logger.info(f"CRUD UPDATE STATUS: 评估记录对象 ID {assessment_id} 在状态更新后已刷新。")
        return db_obj
    except (sqlite3.OperationalError, sqlite3.IntegrityError, SQLAlchemyError) as db_err: # 捕获可能的数据库错误
        error_msg = f"更新状态时数据库错误 (ID: {assessment_id}): {type(db_err).__name__} - {db_err}"
        logger.error(f"CRUD UPDATE STATUS: {error_msg}", exc_info=True)
        logger.info(f"CRUD UPDATE STATUS: 由于数据库错误，正在回滚会话 (ID: {assessment_id})...")
        await db.rollback()
        raise db_err # 重新抛出错误
    except Exception as e:
        error_msg = f"更新状态期间发生一般错误 (ID: {assessment_id}): {e}"
        logger.error(f"CRUD UPDATE STATUS: {error_msg}", exc_info=True)
        logger.info(f"CRUD UPDATE STATUS: 由于一般错误，正在回滚会话 (ID: {assessment_id})...")
        await db.rollback()
        raise e

async def update_report_text(db: AsyncSession, assessment_id: int, report_text: str) -> Optional[Assessment]:
    """仅更新指定评估记录的 report_text 字段。"""
    logger.info(f"CRUD UPDATE REPORT TEXT: 尝试更新评估记录 ID: {assessment_id} 的报告文本")
    db_obj = await get(db, id=assessment_id)
    if not db_obj:
        logger.warning(f"CRUD UPDATE REPORT TEXT: 未找到评估记录 ID {assessment_id}。无法更新报告。")
        return None

    # 比较之前，确保两个值都是字符串类型（如果 report_text 可能为 None）
    current_report = db_obj.report_text if db_obj.report_text is not None else ""
    new_report = report_text if report_text is not None else ""

    if current_report == new_report:
         logger.info(f"CRUD UPDATE REPORT TEXT: 评估记录 ID {assessment_id} 的报告文本未更改，无需更新。")
         return db_obj

    logger.debug(f"CRUD UPDATE REPORT TEXT: 找到评估记录 ID {assessment_id}。正在更新 report_text。")
    db_obj.report_text = report_text # 更新为传入的值，可以是 None
    db.add(db_obj) # 将更改添加到会话

    try:
        logger.info(f"CRUD UPDATE REPORT TEXT: 尝试提交数据库事务以更新报告文本 (ID: {assessment_id})...")
        await db.commit()
        logger.info(f"CRUD UPDATE REPORT TEXT: 数据库提交成功，报告文本已更新 (ID: {assessment_id})。")
        await db.refresh(db_obj)
        logger.info(f"CRUD UPDATE REPORT TEXT: 评估记录对象 ID {assessment_id} 在报告更新后已刷新。")
        return db_obj
    except (sqlite3.OperationalError, sqlite3.IntegrityError, SQLAlchemyError) as db_err:
        error_msg = f"更新报告文本时数据库错误 (ID: {assessment_id}): {type(db_err).__name__} - {db_err}"
        logger.error(f"CRUD UPDATE REPORT TEXT: {error_msg}", exc_info=True)
        logger.info(f"CRUD UPDATE REPORT TEXT: 由于数据库错误，正在回滚会话 (ID: {assessment_id})...")
        await db.rollback()
        raise db_err
    except Exception as e:
        error_msg = f"更新报告文本期间发生一般错误 (ID: {assessment_id}): {e}"
        logger.error(f"CRUD UPDATE REPORT TEXT: {error_msg}", exc_info=True)
        logger.info(f"CRUD UPDATE REPORT TEXT: 由于一般错误，正在回滚会话 (ID: {assessment_id})...")
        await db.rollback()
        raise e

# --- 后台管理查询函数 ---

async def get_assessments_by_id_card(db: AsyncSession, id_card: str) -> List[Assessment]:
    """
    异步根据身份证号获取所有相关的评估记录列表，按创建时间降序排列。
    """
    logger.info(f"CRUD: 尝试按身份证号 '{id_card}' 查找评估记录")
    if not id_card:
        logger.warning("CRUD: 尝试按空身份证号查询，返回空列表。")
        return []
    try:
        stmt = select(Assessment).filter(Assessment.id_card == id_card).order_by(desc(Assessment.created_at))
        result = await db.execute(stmt)
        assessments = result.scalars().all()
        logger.info(f"CRUD: 找到 {len(assessments)} 条身份证号为 '{id_card}' 的评估记录")
        return list(assessments) # 确保返回列表
    except SQLAlchemyError as e:
        logger.error(f"CRUD: 按身份证号 '{id_card}' 获取评估记录时发生数据库错误: {e}", exc_info=True)
        raise e

async def get_latest_completed_by_id_card(db: AsyncSession, id_card: str) -> Optional[Assessment]:
    """
    异步根据身份证号获取最新一条状态为 'complete' 的评估记录。
    """
    logger.info(f"CRUD: 尝试按身份证号 '{id_card}' 查找最新的已完成评估记录")
    if not id_card:
         logger.warning("CRUD: 尝试按空身份证号查询最新完成记录，返回 None。")
         return None
    try:
        stmt = (
            select(Assessment)
            .filter(Assessment.id_card == id_card)
            .filter(Assessment.status == STATUS_COMPLETE) # 使用导入的状态常量
            .order_by(desc(Assessment.created_at)) # 按创建时间降序排列
            .limit(1) # 只取最新的一条
        )
        result = await db.execute(stmt)
        assessment = result.scalar_one_or_none()
        if assessment:
            logger.info(f"CRUD: 找到身份证号 '{id_card}' 的最新已完成评估记录 ID: {assessment.id}")
        else:
            logger.info(f"CRUD: 未找到身份证号 '{id_card}' 的已完成评估记录")
        return assessment
    except SQLAlchemyError as e:
        logger.error(f"CRUD: 按身份证号 '{id_card}' 获取最新已完成评估时发生数据库错误: {e}", exc_info=True)
        raise e

# --- +++ 新增：处理评估与属性关联的 CRUD 函数 +++ ---

async def add_attribute_to_assessment(
    db: AsyncSession, *, assessment_id: int, attribute_id: int
) -> Optional[Assessment]:
    """
    将一个属性关联到一个评估记录（通过 ID）。

    Args:
        db: 数据库会话。
        assessment_id: 评估记录的 ID。
        attribute_id: 要关联的属性的 ID。

    Returns:
        更新后的 Assessment 对象 (如果成功) 或 None (如果任一 ID 未找到)。
    """
    logger.info(f"CRUD Assoc: 尝试将属性 ID {attribute_id} 添加到评估 ID {assessment_id}")
    # 1. 获取评估对象
    assessment = await get(db, id=assessment_id)
    if not assessment:
        logger.warning(f"CRUD Assoc: 未找到评估 ID {assessment_id}，无法添加属性。")
        return None

    # 2. 获取属性对象 (需要导入 crud.attribute)
    try:
        from app.crud import attribute as crud_attribute # 在函数内部导入以避免循环依赖问题
    except ImportError:
         logger.error("CRUD Assoc: 无法导入 crud.attribute 模块！")
         raise RuntimeError("Attribute CRUD module is not available.")

    attribute = await crud_attribute.get_attribute(db, attribute_id=attribute_id)
    if not attribute:
        logger.warning(f"CRUD Assoc: 未找到属性 ID {attribute_id}，无法添加到评估 {assessment_id}。")
        return None # 返回 None 表示属性未找到，或者可以返回 assessment 让调用者知道

    # 3. 检查是否已关联
    #    注意：直接检查 assessment.attributes 需要 ORM 加载关联数据，
    #    对于仅添加操作，可以直接尝试添加，让数据库处理唯一性约束（如果有）。
    #    或者，先查询关联表是否存在记录 (更安全但多一次查询)。
    #    这里我们直接尝试添加，依赖于数据库的复合主键或唯一约束。
    if attribute in assessment.attributes: # 如果已加载，可以检查
        logger.debug(f"CRUD Assoc: 属性 ID {attribute.id} 已关联到评估 ID {assessment.id}，无需重复添加。")
        return assessment

    # 4. 添加关联
    assessment.attributes.append(attribute) # SQLAlchemy 会在 commit 时处理关联表的插入
    db.add(assessment) # 标记 assessment 对象已更改（虽然 append 通常会自动标记）

    try:
        await db.commit()
        # 提交后，assessment.attributes 应该会包含新添加的属性 (如果 lazy loading 策略允许或 refresh)
        # await db.refresh(assessment) # 刷新以确保看到最新的关联列表
        logger.info(f"CRUD Assoc: 成功将属性 ID {attribute.id} 添加到评估 ID {assessment.id}")
        return assessment
    except (IntegrityError, sqlite3.IntegrityError) as e: # 捕获可能的复合主键冲突
        await db.rollback()
        logger.error(f"CRUD Assoc: 添加属性关联时发生完整性错误 (评估ID: {assessment_id}, 属性ID: {attribute.id}): {e}", exc_info=False)
        # 这种情况通常意味着关联已存在，即使上面的 `if attribute in assessment.attributes` 检查没发现（可能因为未加载）
        logger.warning(f"CRUD Assoc: 属性 ID {attribute_id} 可能已关联到评估 ID {assessment.id} (数据库层面)。")
        # 可以选择返回 assessment，表示操作幂等完成
        return assessment # 或者抛出异常 raise ValueError(...)
    except SQLAlchemyError as e:
        await db.rollback()
        logger.error(f"CRUD Assoc: 添加属性关联时发生数据库错误 (评估ID: {assessment_id}, 属性ID: {attribute.id}): {e}", exc_info=True)
        raise e

async def remove_attribute_from_assessment(
    db: AsyncSession, *, assessment_id: int, attribute_id: int
) -> Optional[Assessment]:
    """
    从评估记录解除一个属性的关联（通过 ID）。

    Args:
        db: 数据库会话。
        assessment_id: 评估记录的 ID。
        attribute_id: 要解除关联的属性的 ID。

    Returns:
        更新后的 Assessment 对象 (如果成功) 或 None (如果任一 ID 未找到)。
    """
    logger.info(f"CRUD Assoc: 尝试从评估 ID {assessment_id} 移除属性 ID {attribute_id}")
    # 1. 获取评估对象
    assessment = await get(db, id=assessment_id)
    if not assessment:
        logger.warning(f"CRUD Assoc: 未找到评估 ID {assessment_id}，无法移除属性。")
        return None

    # 2. 获取属性对象 (确保它存在，虽然移除时理论上可以不获取，但获取更安全)
    try:
        from app.crud import attribute as crud_attribute
    except ImportError:
        logger.error("CRUD Assoc: 无法导入 crud.attribute 模块！")
        raise RuntimeError("Attribute CRUD module is not available.")

    attribute = await crud_attribute.get_attribute(db, attribute_id=attribute_id)
    if not attribute:
        logger.warning(f"CRUD Assoc: 未找到属性 ID {attribute_id}，无法从评估 {assessment_id} 移除。")
        return None

    # 3. 检查关联是否存在并移除
    #    同样，依赖于 assessment.attributes 是否已加载
    if attribute in assessment.attributes:
        assessment.attributes.remove(attribute) # SQLAlchemy 处理关联表的删除
        db.add(assessment) # 标记对象已更改
        try:
            await db.commit()
            # 移除后通常不需要 refresh
            logger.info(f"CRUD Assoc: 成功从评估 ID {assessment_id} 移除属性 ID {attribute_id}")
            return assessment
        except SQLAlchemyError as e:
            await db.rollback()
            logger.error(f"CRUD Assoc: 移除属性关联时发生数据库错误 (评估ID: {assessment_id}, 属性ID: {attribute.id}): {e}", exc_info=True)
            raise e
    else:
        # 如果关系未加载，或者属性确实未关联
        # 可以尝试直接查询关联表判断是否存在，但通常直接返回表示“已完成”或“无操作”即可
        logger.warning(f"CRUD Assoc: 属性 ID {attribute.id} 未关联到评估 ID {assessment.id} 或关联未加载，无法移除。")
        return assessment # 返回未修改的对象

async def set_assessment_attributes(
    db: AsyncSession, *, assessment_id: int, attribute_ids: List[int]
) -> Optional[Assessment]:
    """
    设置评估记录的属性列表，移除不再包含的属性，添加新增的属性。

    Args:
        db: 数据库会话。
        assessment_id: 评估记录的 ID。
        attribute_ids: 要设置的属性 ID 列表。

    Returns:
        更新后的 Assessment 对象或 None。
    """
    logger.info(f"CRUD Assoc: 正在设置评估 ID {assessment_id} 的属性列表为: {attribute_ids}")
    assessment = await get(db, id=assessment_id)
    if not assessment:
        logger.warning(f"CRUD Assoc: 未找到评估 ID {assessment_id}，无法设置属性。")
        return None

    try:
        from app.crud import attribute as crud_attribute
    except ImportError:
        logger.error("CRUD Assoc: 无法导入 crud.attribute 模块！")
        raise RuntimeError("Attribute CRUD module is not available.")

    # 1. 获取目标属性对象列表
    target_attributes = []
    invalid_ids = []
    if attribute_ids: # 只有当列表不为空时才查询
        stmt = select(Attribute).filter(Attribute.id.in_(attribute_ids))
        result = await db.execute(stmt)
        target_attributes = list(result.scalars().all()) # 获取所有有效的属性对象
        found_ids = {attr.id for attr in target_attributes}
        invalid_ids = [id for id in attribute_ids if id not in found_ids]
        if invalid_ids:
             logger.warning(f"CRUD Assoc: 尝试关联到评估 {assessment_id} 时，发现无效/不存在的属性 IDs: {invalid_ids}")

    # 2. 使用集合操作计算差异 (需要确保 assessment.attributes 已加载)
    #    为确保加载，可以重新查询评估并指定加载策略
    #    或者直接覆盖关系列表 (SQLAlchemy 通常能处理好)
    logger.debug(f"CRUD Assoc: 将评估 {assessment_id} 的属性更新为 ID 列表对应的对象 (找到 {len(target_attributes)} 个)")
    assessment.attributes = target_attributes # 直接将关系列表设置为新的对象列表

    db.add(assessment) # 标记对象已更改
    try:
        await db.commit()
        # await db.refresh(assessment) # 刷新以获取最新状态
        logger.info(f"CRUD Assoc: 成功设置评估 ID {assessment_id} 的属性列表。")
        if invalid_ids:
             logger.warning(f"CRUD Assoc: 在设置评估 {assessment_id} 属性时，以下无效 ID 已被忽略: {invalid_ids}")
        return assessment
    except SQLAlchemyError as e:
        await db.rollback()
        logger.error(f"CRUD Assoc: 设置评估 {assessment_id} 属性列表时发生数据库错误: {e}", exc_info=True)
        raise e

# --- 结束文件 ---

--------------------------------------------------
文件路径: app\crud\attribute.py
--------------------------------------------------
# FILE: app/crud/attribute.py
import logging
from typing import List, Optional, Sequence # 导入 Sequence
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from sqlalchemy import func
from sqlalchemy.exc import SQLAlchemyError, IntegrityError
import sqlite3

from app.models.attribute import Attribute # 导入 Attribute 模型
from app.schemas.attribute import AttributeCreate, AttributeUpdate # 导入 Pydantic Schemas
from app.core.config import settings

logger = logging.getLogger(settings.APP_NAME)

async def get_attribute(db: AsyncSession, attribute_id: int) -> Optional[Attribute]:
    """根据 ID 异步获取单个属性。"""
    logger.debug(f"CRUD: 尝试获取属性 ID: {attribute_id}")
    result = await db.execute(select(Attribute).filter(Attribute.id == attribute_id))
    attribute = result.scalar_one_or_none()
    if attribute:
        logger.debug(f"CRUD: 找到属性 ID: {attribute_id}, Name: {attribute.name}")
    else:
        logger.warning(f"CRUD: 未找到属性 ID: {attribute_id}")
    return attribute

async def get_attribute_by_name(db: AsyncSession, name: str) -> Optional[Attribute]:
    """根据名称异步获取单个属性 (大小写敏感)。"""
    logger.debug(f"CRUD: 尝试按名称获取属性: {name}")
    # 可以考虑使用 func.lower() 实现不区分大小写查找，但需确保数据库支持且有索引
    # result = await db.execute(select(Attribute).filter(func.lower(Attribute.name) == name.lower()))
    result = await db.execute(select(Attribute).filter(Attribute.name == name))
    attribute = result.scalar_one_or_none()
    if attribute:
        logger.debug(f"CRUD: 找到属性 Name: {name}, ID: {attribute.id}")
    else:
        logger.debug(f"CRUD: 未找到属性 Name: {name}")
    return attribute

async def get_attributes(
    db: AsyncSession, skip: int = 0, limit: int = 100, category: Optional[str] = None
) -> Sequence[Attribute]: # 返回模型序列
    """
    异步获取属性列表，支持分页和按分类过滤。
    返回 Attribute ORM 对象列表。
    """
    logger.info(f"CRUD: 获取属性列表, skip={skip}, limit={limit}, category={category}")
    stmt = select(Attribute).order_by(Attribute.name).offset(skip).limit(limit)
    if category:
        stmt = stmt.filter(Attribute.category == category)

    result = await db.execute(stmt)
    attributes = result.scalars().all()
    logger.info(f"CRUD: 获取到 {len(attributes)} 个属性")
    return attributes # 直接返回 ORM 对象列表

async def create_attribute(db: AsyncSession, *, attribute_in: AttributeCreate) -> Attribute:
    """异步创建新属性。"""
    logger.info(f"CRUD: 尝试创建属性: Name='{attribute_in.name}', Category='{attribute_in.category}'")
    # 检查名称是否已存在
    existing_attribute = await get_attribute_by_name(db, name=attribute_in.name)
    if existing_attribute:
        logger.warning(f"CRUD: 属性名称 '{attribute_in.name}' 已存在，无法创建。")
        raise ValueError(f"属性名称 '{attribute_in.name}' 已存在。") # 抛出 ValueError

    db_attribute = Attribute(**attribute_in.model_dump()) # 从 Pydantic 创建模型实例
    db.add(db_attribute)
    try:
        await db.commit()
        await db.refresh(db_attribute)
        logger.info(f"CRUD: 成功创建属性 ID: {db_attribute.id}, Name: {db_attribute.name}")
        return db_attribute
    except (IntegrityError, sqlite3.IntegrityError) as e: # 捕获唯一约束错误
        await db.rollback()
        logger.error(f"CRUD: 创建属性 '{attribute_in.name}' 时发生数据库完整性错误: {e}", exc_info=False)
        raise ValueError(f"属性名称 '{attribute_in.name}' 可能已存在或违反数据库约束。") from e
    except SQLAlchemyError as e:
        await db.rollback()
        logger.error(f"CRUD: 创建属性 '{attribute_in.name}' 时发生数据库错误: {e}", exc_info=True)
        raise e # 重新抛出原始 SQLAlchemy 错误

async def update_attribute(
    db: AsyncSession, *, db_attribute: Attribute, attribute_in: AttributeUpdate
) -> Attribute:
    """异步更新属性。"""
    logger.info(f"CRUD: 尝试更新属性 ID: {db_attribute.id}, Name: {db_attribute.name}")
    update_data = attribute_in.model_dump(exclude_unset=True)

    if not update_data:
        logger.info(f"CRUD: 没有提供更新数据，属性 ID {db_attribute.id} 未更改。")
        return db_attribute

    # 检查新名称是否与现有其他属性冲突
    new_name = update_data.get("name")
    if new_name and new_name != db_attribute.name:
        existing_attribute = await get_attribute_by_name(db, name=new_name)
        if existing_attribute:
            logger.warning(f"CRUD: 更新失败，新的属性名称 '{new_name}' 已被 ID {existing_attribute.id} 使用。")
            raise ValueError(f"属性名称 '{new_name}' 已存在。")

    for field, value in update_data.items():
        setattr(db_attribute, field, value)

    db.add(db_attribute)
    try:
        await db.commit()
        await db.refresh(db_attribute)
        logger.info(f"CRUD: 成功更新属性 ID: {db_attribute.id}")
        return db_attribute
    except (IntegrityError, sqlite3.IntegrityError) as e:
        await db.rollback()
        logger.error(f"CRUD: 更新属性 ID {db_attribute.id} 时发生数据库完整性错误: {e}", exc_info=False)
        raise ValueError(f"更新属性时名称 '{update_data.get('name')}' 可能已存在或违反数据库约束。") from e
    except SQLAlchemyError as e:
        await db.rollback()
        logger.error(f"CRUD: 更新属性 ID {db_attribute.id} 时发生数据库错误: {e}", exc_info=True)
        raise e

async def delete_attribute(db: AsyncSession, attribute_id: int) -> bool:
    """异步删除属性 (需要注意关联关系)。"""
    logger.warning(f"CRUD: 尝试删除属性 ID: {attribute_id}")
    db_attribute = await get_attribute(db, attribute_id=attribute_id)
    if not db_attribute:
        logger.error(f"CRUD: 无法删除，未找到属性 ID: {attribute_id}")
        return False

    # 警告：直接删除可能导致 assessment_attributes 表中的外键约束失败（如果数据库强制执行）
    # 或留下孤立的关联记录。最佳实践是先解除关联。
    # 在这里，我们先尝试直接删除，如果失败则提示需要先解除关联。
    # 更安全的做法是先查询是否有评估关联了此属性。
    # stmt_count = select(func.count()).select_from(assessment_attributes_table).where(assessment_attributes_table.c.attribute_id == attribute_id)
    # count_result = await db.execute(stmt_count)
    # associated_count = count_result.scalar_one_or_none()
    # if associated_count and associated_count > 0:
    #     logger.error(f"CRUD: 无法删除属性 ID {attribute_id}，因为它仍关联着 {associated_count} 个评估记录。")
    #     raise ValueError(f"无法删除属性，因为它仍被 {associated_count} 个评估使用。请先解除关联。")

    try:
        await db.delete(db_attribute)
        await db.commit()
        logger.info(f"CRUD: 成功删除属性 ID: {attribute_id}")
        return True
    except (IntegrityError, sqlite3.IntegrityError) as e: # 特别是外键约束错误
         await db.rollback()
         logger.error(f"CRUD: 删除属性 ID {attribute_id} 时发生完整性错误（可能仍被评估使用）: {e}", exc_info=False)
         raise ValueError(f"无法删除属性，可能它仍被某些评估使用。错误: {e}") from e
    except SQLAlchemyError as e:
        await db.rollback()
        logger.error(f"CRUD: 删除属性 ID {attribute_id} 时发生数据库错误: {e}", exc_info=True)
        raise e

--------------------------------------------------
文件路径: app\crud\interrogation.py
--------------------------------------------------
# FILE: app/crud/interrogation.py (新建)
import logging
from typing import Optional, Dict, Any, List
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from sqlalchemy.exc import SQLAlchemyError

from app.models.interrogation import InterrogationRecord # 导入审讯记录模型
from app.schemas.interrogation import InterrogationRecordUpdate # 导入更新 Schema
from app.core.config import settings

logger = logging.getLogger(settings.APP_NAME)

async def create_interrogation(
    db: AsyncSession, *, interrogator_id: int, basic_info: Dict[str, Any], qas: List[Dict[str, str]]
) -> InterrogationRecord:
    """
    异步创建一条新的审讯记录。
    """
    logger.info(f"CRUD: 准备为审讯员 ID {interrogator_id} 创建新的审讯记录")
    try:
        db_obj = InterrogationRecord(
            interrogator_id=interrogator_id,
            basic_info=basic_info, # 存储 JSON
            qas=qas,               # 存储 JSON 列表
            status="ongoing"       # 初始状态
        )
        db.add(db_obj)
        await db.commit()
        await db.refresh(db_obj)
        logger.info(f"CRUD: 成功创建审讯记录 ID: {db_obj.id}")
        return db_obj
    except SQLAlchemyError as e:
        await db.rollback()
        logger.error(f"CRUD: 创建审讯记录时发生数据库错误: {e}", exc_info=True)
        raise e
    except Exception as e: # 捕获其他可能的错误
        await db.rollback()
        logger.error(f"CRUD: 创建审讯记录时发生意外错误: {e}", exc_info=True)
        raise e

async def get_interrogation(db: AsyncSession, record_id: int) -> Optional[InterrogationRecord]:
    """
    异步根据 ID 获取审讯记录。
    """
    logger.debug(f"CRUD: 尝试查找审讯记录 ID: {record_id}")
    try:
        result = await db.execute(select(InterrogationRecord).filter(InterrogationRecord.id == record_id))
        found_obj = result.scalar_one_or_none()
        if found_obj:
            logger.debug(f"CRUD: 已找到审讯记录 ID: {record_id}")
        else:
            logger.debug(f"CRUD: 未找到审讯记录 ID: {record_id}")
        return found_obj
    except SQLAlchemyError as e:
        logger.error(f"CRUD: 获取审讯记录 ID {record_id} 时发生数据库错误: {e}", exc_info=True)
        raise e

async def update_interrogation(
    db: AsyncSession, *, record_id: int, update_data: InterrogationRecordUpdate
) -> Optional[InterrogationRecord]:
    """
    异步更新指定的审讯记录。
    """
    logger.info(f"CRUD: 准备更新审讯记录 ID: {record_id}")
    db_obj = await get_interrogation(db, record_id=record_id)
    if not db_obj:
        logger.warning(f"CRUD: 更新审讯记录失败，未找到 ID: {record_id}")
        return None

    # 使用 Pydantic 模型的 model_dump 来获取需要更新的字段
    # exclude_unset=True 确保只更新传入的字段
    update_dict = update_data.model_dump(exclude_unset=True)
    if not update_dict:
         logger.info(f"CRUD: 没有提供需要更新的字段，审讯记录 ID: {record_id} 未更改。")
         return db_obj # 没有需要更新的字段

    logger.debug(f"CRUD: 将要更新的字段 (ID: {record_id}): {list(update_dict.keys())}")

    # 更新模型实例的属性
    for field, value in update_dict.items():
        setattr(db_obj, field, value)

    db.add(db_obj) # 将更改添加到会话
    try:
        await db.commit()
        await db.refresh(db_obj)
        logger.info(f"CRUD: 成功更新审讯记录 ID: {record_id}")
        return db_obj
    except SQLAlchemyError as e:
        await db.rollback()
        logger.error(f"CRUD: 更新审讯记录 ID {record_id} 时发生数据库错误: {e}", exc_info=True)
        raise e
    except Exception as e:
        await db.rollback()
        logger.error(f"CRUD: 更新审讯记录 ID {record_id} 时发生意外错误: {e}", exc_info=True)
        raise e

# (可以添加删除函数 delete_interrogation 如果需要)

--------------------------------------------------
文件路径: app\crud\stats.py
--------------------------------------------------
# FILE: app/crud/stats.py (新建)
import logging
from typing import Dict, List, Any
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from sqlalchemy import func # 导入 SQL 函数

from app.models.user import User # 导入 User 模型 (如果按性别统计需要 User 表)
from app.models.assessment import Assessment # 导入 Assessment 模型 (如果按年龄统计需要 Assessment 表)
from app.core.config import settings

logger = logging.getLogger(settings.APP_NAME)

async def get_age_distribution(db: AsyncSession) -> Dict[str, List[Any]]:
    """
    异步获取评估记录中的年龄分布统计数据。
    注意: 这依赖于 Assessment 模型中 'age' 列有数据。
    """
    logger.info("CRUD Stats: 计算年龄分布")
    # 定义年龄段
    age_bins = {
        '<18': (0, 17),
        '18-25': (18, 25),
        '26-35': (26, 35),
        '36-45': (36, 45),
        '46-55': (46, 55),
        '56+': (56, 999), # 使用一个较大的上限
        '未知': (None, None) # 处理空值
    }
    labels = list(age_bins.keys())
    values = [0] * len(labels)

    try:
        # 查询所有有效的年龄
        stmt = select(Assessment.age)
        result = await db.execute(stmt)
        ages = result.scalars().all()

        # 在 Python 中进行分组计数
        for age in ages:
            found = False
            for i, (label, (min_age, max_age)) in enumerate(age_bins.items()):
                 if label == '未知': continue # 跳过未知标签的 bin 定义
                 if age is None:
                     if label == '未知': # 应该不会执行到这里，但作为后备
                         values[labels.index('未知')] += 1
                         found = True
                         break
                     continue # 非未知标签不匹配 None age

                 # 确保 age 是数字类型
                 if isinstance(age, (int, float)):
                     if min_age is not None and max_age is not None and min_age <= age <= max_age:
                         values[i] += 1
                         found = True
                         break
                 else:
                      logger.warning(f"CRUD Stats: 在年龄分布计算中遇到非数字年龄值: {age} (类型: {type(age)})，已忽略。")

            if not found: # 处理 None 和未匹配的年龄
                 if age is None:
                    values[labels.index('未知')] += 1
                 else:
                    logger.warning(f"CRUD Stats: 年龄 {age} 未落入任何定义的区间，已忽略。")

        logger.info(f"CRUD Stats: 年龄分布计算完成 - Labels: {labels}, Values: {values}")
        return {"labels": labels, "values": values}

    except Exception as e:
        logger.error(f"CRUD Stats: 计算年龄分布时出错: {e}", exc_info=True)
        # 返回空或默认值，避免 API 失败
        return {"labels": labels, "values": [0] * len(labels)}


async def get_gender_distribution(db: AsyncSession) -> Dict[str, List[Any]]:
    """
    异步获取评估记录中的性别分布统计数据。
    注意: 这依赖于 Assessment 模型中 'gender' 列有数据。
    """
    logger.info("CRUD Stats: 计算性别分布")
    labels = []
    values = []
    try:
        # 使用 SQLAlchemy 的 func.count 和 group_by 进行聚合查询
        stmt = (
            select(Assessment.gender, func.count(Assessment.id).label('count'))
            .group_by(Assessment.gender)
            .order_by(Assessment.gender) # 按性别排序以保持一致
        )
        result = await db.execute(stmt)
        rows = result.all() # 获取所有 (gender, count) 对

        # 处理结果
        if not rows:
            logger.info("CRUD Stats: 性别分布数据为空。")
            return {"labels": ["无数据"], "values": [0]}

        for row in rows:
            gender = row.gender if row.gender else "未知" # 处理 NULL 值
            count = row.count
            labels.append(gender)
            values.append(count)

        logger.info(f"CRUD Stats: 性别分布计算完成 - Labels: {labels}, Values: {values}")
        return {"labels": labels, "values": values}

    except Exception as e:
        logger.error(f"CRUD Stats: 计算性别分布时出错: {e}", exc_info=True)
        return {"labels": ["错误"], "values": [0]}

# --- (可选) 其他统计函数 ---
# async def get_scale_usage(db: AsyncSession) -> Dict[str, List[Any]]: ...
# async def get_assessment_status_distribution(db: AsyncSession) -> Dict[str, List[Any]]: ...

--------------------------------------------------
文件路径: app\crud\user.py
--------------------------------------------------
# FILE: app/crud/user.py (修改后，添加 admin 操作)
import logging
from typing import Tuple, List, Any # <--- 添加 Tuple, List, Any 导入
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select # For SQLAlchemy 2.0 style selects
from sqlalchemy import func # <--- 添加 func 导入用于 count

# --- Core App Imports ---
from app.core.security import get_password_hash, verify_password # Your security utils
from app.models.user import User # Your SQLAlchemy User model
from app.schemas.user import UserCreate, UserUpdate, UserUpdateAdmin # <--- 添加 UserUpdateAdmin
from app.core.config import settings

logger = logging.getLogger(settings.APP_NAME)

# --- Async User CRUD Operations ---

async def get_user(db: AsyncSession, user_id: int) -> User | None:
    """
    Asynchronously retrieves a user by their ID.
    """
    logger.debug(f"Attempting to get user by ID: {user_id}")
    result = await db.execute(select(User).filter(User.id == user_id))
    user = result.scalar_one_or_none()
    if user:
        logger.debug(f"User found by ID: {user_id}")
    else:
        logger.debug(f"User not found by ID: {user_id}")
    return user

async def get_user_by_username(db: AsyncSession, username: str) -> User | None:
    """
    Asynchronously retrieves a user by their username (case-insensitive search recommended).
    """
    logger.debug(f"Attempting to get user by username: {username}")
    # result = await db.execute(select(User).filter(func.lower(User.username) == username.lower())) # Case-insensitive
    result = await db.execute(select(User).filter(User.username == username)) # Case-sensitive
    user = result.scalar_one_or_none()
    if user:
        logger.debug(f"User found by username: {username}")
    else:
        logger.debug(f"User not found by username: {username}")
    return user


async def get_user_by_email(db: AsyncSession, email: str) -> User | None:
    """
    Asynchronously retrieves a user by their email (case-insensitive search recommended).
    """
    logger.debug(f"Attempting to get user by email: {email}")
    # result = await db.execute(select(User).filter(func.lower(User.email) == email.lower())) # Case-insensitive
    result = await db.execute(select(User).filter(User.email == email)) # Case-sensitive
    user = result.scalar_one_or_none()
    if user:
        logger.debug(f"User found by email: {email}")
    else:
        logger.debug(f"User not found by email: {email}")
    return user

async def create_user(db: AsyncSession, *, user_in: UserCreate) -> User:
    """
    Asynchronously creates a new user in the database.
    """
    logger.info(f"Attempting to create user with username: {user_in.username}")
    existing_user_by_username = await get_user_by_username(db, username=user_in.username)
    if existing_user_by_username:
        logger.warning(f"Username '{user_in.username}' already exists.")
        raise ValueError(f"用户名 '{user_in.username}' 已被注册。") # Use Chinese error
    if user_in.email:
        existing_user_by_email = await get_user_by_email(db, email=user_in.email)
        if existing_user_by_email:
            logger.warning(f"Email '{user_in.email}' already exists.")
            raise ValueError(f"邮箱 '{user_in.email}' 已被注册。") # Use Chinese error

    hashed_password = get_password_hash(user_in.password)
    db_user = User(
        username=user_in.username,
        email=user_in.email,
        full_name=user_in.full_name,
        hashed_password=hashed_password,
        is_active=user_in.is_active if user_in.is_active is not None else True,
        is_superuser=user_in.is_superuser if user_in.is_superuser is not None else False
    )
    db.add(db_user)
    try:
        await db.commit()
        logger.info(f"User '{user_in.username}' committed to database.")
    except Exception as e:
        await db.rollback()
        logger.error(f"Database commit failed while creating user '{user_in.username}': {e}", exc_info=True)
        raise
    await db.refresh(db_user)
    logger.info(f"User '{db_user.username}' created successfully with ID: {db_user.id}")
    return db_user


async def update_user(db: AsyncSession, *, db_user: User, user_in: UserUpdate) -> User:
    """
    Asynchronously updates an existing user's information (for the user themselves).
    """
    logger.info(f"Attempting to update user ID: {db_user.id} (self-update)")
    update_data = user_in.model_dump(exclude_unset=True)

    if "password" in update_data and update_data["password"]:
        hashed_password = get_password_hash(update_data["password"])
        update_data["hashed_password"] = hashed_password
        logger.info(f"Password updated for user ID: {db_user.id}")
    # Remove password from dict whether it was hashed or not provided
    if "password" in update_data:
         del update_data["password"]

    for field, value in update_data.items():
        # Prevent users from updating their own is_active or is_superuser status
        if field not in ['is_active', 'is_superuser']:
            setattr(db_user, field, value)

    db.add(db_user)
    try:
        await db.commit()
        logger.info(f"User ID {db_user.id} self-update changes committed.")
    except Exception as e:
        await db.rollback()
        logger.error(f"Database commit failed while self-updating user ID {db_user.id}: {e}", exc_info=True)
        raise
    await db.refresh(db_user)
    logger.info(f"User ID {db_user.id} updated successfully (self-update).")
    return db_user

# --- +++ 新增后台管理操作 +++ ---

async def get_users(db: AsyncSession, skip: int = 0, limit: int = 100) -> Tuple[List[User], int]:
    """
    Asynchronously retrieves a list of users with pagination and total count.
    """
    logger.info(f"CRUD: Fetching user list with skip={skip}, limit={limit}")
    try:
        # Get the list of users for the current page
        stmt_users = select(User).offset(skip).limit(limit).order_by(User.id)
        result_users = await db.execute(stmt_users)
        users = list(result_users.scalars().all()) # Ensure it's a list

        # Get the total count of users
        stmt_count = select(func.count()).select_from(User)
        result_count = await db.execute(stmt_count)
        total_count = result_count.scalar_one()

        logger.info(f"CRUD: Fetched {len(users)} users (Total: {total_count})")
        return users, total_count
    except Exception as e:
        logger.error(f"CRUD: Error fetching user list: {e}", exc_info=True)
        raise # Re-raise the exception

async def update_user_admin(db: AsyncSession, *, db_user: User, user_in: UserUpdateAdmin) -> User:
    """
    Asynchronously updates an existing user's information (by an admin).
    Allows updating is_active and is_superuser status.
    """
    logger.info(f"CRUD Admin: Attempting to update user ID: {db_user.id}")
    # Get fields to update from the admin schema
    update_data = user_in.model_dump(exclude_unset=True)

    if not update_data:
         logger.info(f"CRUD Admin: No fields provided for update for user ID: {db_user.id}")
         return db_user # Return unchanged user if no data provided

    logger.debug(f"CRUD Admin: Fields to update for user ID {db_user.id}: {list(update_data.keys())}")

    # If password is being updated, hash the new one
    if "password" in update_data and update_data["password"]:
        hashed_password = get_password_hash(update_data["password"])
        update_data["hashed_password"] = hashed_password
        logger.info(f"CRUD Admin: Password updated for user ID: {db_user.id}")
    # Remove password from dict whether hashed or not provided
    if "password" in update_data:
        del update_data["password"]

    # Update the user object's attributes based on admin input
    for field, value in update_data.items():
        if hasattr(db_user, field):
             setattr(db_user, field, value)
        else:
             logger.warning(f"CRUD Admin: Field '{field}' not found in User model, skipping update for user ID {db_user.id}")


    db.add(db_user)
    try:
        await db.commit()
        logger.info(f"CRUD Admin: User ID {db_user.id} admin changes committed.")
    except Exception as e:
        await db.rollback()
        logger.error(f"CRUD Admin: Database commit failed while admin-updating user ID {db_user.id}: {e}", exc_info=True)
        raise
    await db.refresh(db_user)
    logger.info(f"CRUD Admin: User ID {db_user.id} updated successfully by admin.")
    return db_user

# (可以添加删除用户的函数 delete_user 如果需要)
# async def delete_user(db: AsyncSession, user_id: int) -> bool: ...

--------------------------------------------------
文件路径: app\crud\__init__.py
--------------------------------------------------
# FILE: app/crud/__init__.py (更新后)
from . import user          # 用户相关 CRUD
from . import assessment    # 评估相关 CRUD
from . import interrogation # 审讯记录相关 CRUD
from . import stats         # 统计相关 CRUD
from . import attribute     # +++ 属性相关 CRUD +++

# (可选) 可以在这里定义 __all__
__all__ = [
    "user",
    "assessment",
    "interrogation",
    "stats",
    "attribute", # <--- 添加 attribute
]

--------------------------------------------------
文件路径: app\db\base_class.py
--------------------------------------------------
from sqlalchemy.orm import declarative_base

# 创建所有 ORM 模型将继承的基类
Base = declarative_base()

--------------------------------------------------
文件路径: app\db\init_db.py
--------------------------------------------------
# app/db/init_db.py (使用 run_sync 修正)
import logging
import asyncio  # 1. 导入 asyncio 库
from app.db.session import async_engine # 确保你导入的是 async_engine
# 导入所有需要创建表的模型，以及 Base
from app.db.base_class import Base
from app.models.user import User # 导入 User 模型
# from app.models.assessment import Assessment # 如果有其他模型，也导入

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 2. 将 init_db 函数改为异步函数 (async def)
async def init_db() -> None:
    """
    根据 SQLAlchemy 模型异步地创建数据库表。
    """
    logger.info("Attempting to create database tables asynchronously...")
    try:
        # 3. 使用异步上下文管理器获取连接
        async with async_engine.begin() as conn:
            logger.info("Acquired async connection. Running create_all synchronously...")
            # 4. 在异步连接上，使用 run_sync 来执行同步的 create_all 方法
            # 这会将 create_all 的执行委托给事件循环的线程池
            await conn.run_sync(Base.metadata.create_all)
            logger.info("Base.metadata.create_all executed via run_sync.")

        logger.info("Database tables created successfully (if they didn't exist).")
    except Exception as e:
        logger.error(f"Error creating database tables: {e}", exc_info=True)
        raise e
    # finally:
        # 通常不需要手动 dispose，async with 会处理好连接释放
        # 如果需要确保引擎完全关闭（比如脚本结束时），可以取消注释下面两行
        # await async_engine.dispose()
        # logger.info("Async engine disposed.")

if __name__ == "__main__":
    print("Running database initialization...")
    # 5. 使用 asyncio.run() 来运行顶层的异步函数 init_db
    try:
        asyncio.run(init_db())
        print("Database initialization finished successfully.")
    except Exception as e:
        # 捕获在 init_db 中可能重新抛出的异常
        print(f"Database initialization failed: {e}")

--------------------------------------------------
文件路径: app\db\session.py
--------------------------------------------------
# app/db/session.py
import logging
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession
from app.core.config import settings # 导入你的设置

logger = logging.getLogger(settings.APP_NAME) # Or use a specific logger

# --- Asynchronous Database Engine ---
# Create an asynchronous engine using the DATABASE_URL from settings.
# Ensure settings.DATABASE_URL is like "sqlite+aiosqlite:///path/to/your.db"
logger.info(f"Creating async engine for database: {settings.DATABASE_URL}")
try:
    async_engine = create_async_engine(
        settings.DATABASE_URL,
        # echo=True,  # Uncomment for debugging SQL statements
        future=True  # Enables SQLAlchemy 2.0 style features
        # connect_args can be added here if needed, but typically not for aiosqlite
    )
    logger.info("Async engine created successfully.")
except Exception as e:
    logger.critical(f"Failed to create async engine: {e}", exc_info=True)
    raise e # Re-raise the exception to stop application startup if engine fails

# --- Asynchronous Database Session Maker ---
# Create an asynchronous session factory configured to use the async engine.
# expire_on_commit=False is recommended for FastAPI dependency usage,
# preventing attributes from being expired after commit within a request.
AsyncSessionLocal = async_sessionmaker(
    bind=async_engine,
    class_=AsyncSession,       # Specify the use of AsyncSession
    expire_on_commit=False,    # Keep objects accessible after commit within the session scope
    autocommit=False,          # Standard setting, commits are manual
    autoflush=False            # Standard setting, flushing is manual or on commit
)
logger.info("AsyncSessionLocal (async session maker) configured.")

# Note: You will typically use this AsyncSessionLocal in your dependency
# injection function (e.g., get_db in deps.py) to get session instances.

--------------------------------------------------
文件路径: app\db\__init__.py
--------------------------------------------------
# 可以为空，或者导入 Base 和 SessionLocal 以便更容易访问
from .base_class import Base
#from .session import SessionLocal, engine

--------------------------------------------------
文件路径: app\models\assessment.py
--------------------------------------------------
# FILE: app/models/assessment.py (修改后，添加与 Attribute 的多对多关系)
from sqlalchemy import ( # 按字母顺序或分组导入，更清晰
    Table, # <--- 新增: 用于定义关联表
    Column,
    Integer,
    String,
    Boolean,
    Text,
    TIMESTAMP,
    ForeignKey,
    Index
)
from sqlalchemy.orm import relationship # <--- 新增: 用于定义 ORM 关系
from sqlalchemy.sql import func
from .association_tables import assessment_attributes_table
from app.db.base_class import Base # 确保从正确的路径导入 Base
# 注意: Attribute 模型将在 app/models/attribute.py 中定义

# --- 关联表定义 (多对多: Assessment <-> Attribute) ---
# 这个表不需要自己的模型类，SQLAlchemy 会通过 relationship 处理它
# assessment_attributes_table = Table(
#     "assessment_attributes", # 数据库中的关联表名
#     Base.metadata, # 关联到 Base 的元数据
#     Column("assessment_id", Integer, ForeignKey("analysis_data.id"), primary_key=True), # 外键指向 Assessment 表，是复合主键的一部分
#     Column("attribute_id", Integer, ForeignKey("attributes.id"), primary_key=True)   # 外键指向 Attribute 表，是复合主键的一部分
#     # 注意: ForeignKey("attributes.id") 假设你将在 attribute.py 中创建名为 "attributes" 的表
# )
# --- 关联表定义结束 ---

# 定义状态常量 (可选，但推荐)
STATUS_PENDING = "pending"
STATUS_PROCESSING = "processing"
STATUS_COMPLETE = "complete"
STATUS_FAILED = "failed"

class Assessment(Base):
    __tablename__ = "analysis_data" # 数据库中的表名

    id = Column(Integer, primary_key=True, index=True) # 主键，自动索引
    image_path = Column(Text, nullable=True) # 图片文件相对路径或标识符
    subject_name = Column(String(200), index=True) # 被测者姓名，添加索引便于查询
    age = Column(Integer) # 年龄
    gender = Column(String(10)) # 性别
    questionnaire_type = Column(String(100), nullable=True) # 使用的量表类型代码
    questionnaire_data = Column(Text, nullable=True) # 存储量表答案的 JSON 字符串
    report_text = Column(Text, nullable=True) # 存储生成的报告文本

    # 时间戳
    created_at = Column(TIMESTAMP, server_default=func.now(), nullable=False) # 创建时间，数据库自动设置
    updated_at = Column(TIMESTAMP, server_default=func.now(), onupdate=func.now(), nullable=False) # 更新时间

    # 详细信息字段
    id_card = Column(String(50), unique=True, index=True, nullable=True) # 身份证号，唯一且有索引
    occupation = Column(String(100), nullable=True) # 职业
    case_name = Column(String(200), nullable=True) # 案件名称
    case_type = Column(String(100), nullable=True) # 案件类型
    identity_type = Column(String(100), nullable=True) # 人员身份
    person_type = Column(String(100), nullable=True) # 人员类型
    marital_status = Column(String(50), nullable=True) # 婚姻状况
    children_info = Column(Text, nullable=True) # 子女情况 (文本描述)
    criminal_record = Column(Integer, default=0, nullable=False) # 有无犯罪前科 (0:无, 1:有)
    health_status = Column(Text, nullable=True) # 健康情况 (文本描述)
    phone_number = Column(String(50), nullable=True) # 手机号
    domicile = Column(String(200), nullable=True) # 归属地

    # 外键，链接到提交此评估的用户
    submitter_id = Column(Integer, ForeignKey("users.id"), nullable=True, index=True) # 提交者用户ID，允许为空，添加索引

    # 评估状态字段
    status = Column(
        String(30),
        nullable=False,
        default=STATUS_PENDING,
        server_default=STATUS_PENDING,
        index=True
    )

    # --- 新增的多对多关系 ---
    # 定义与 Attribute 模型的关系
    # secondary=assessment_attributes_table 指定了用于连接的关联表
    # back_populates="assessments" 用于在 Attribute 模型中建立反向关系，
    #   假设 Attribute 模型中会有一个名为 'assessments' 的关系指向 Assessment
    # lazy="selectin" 是一种推荐的加载策略，可以在访问 assessment.attributes 时高效加载关联的属性
    attributes = relationship(
        "Attribute", # 指向关联的模型类名 (字符串形式，避免循环导入)
        secondary=assessment_attributes_table, # 指定关联表
        back_populates="assessments", # 指定对方模型中的反向关系属性名
        lazy="selectin" # 推荐的加载策略
    )
    # --- 关系定义结束 ---

    # --- (可选) 与 User 的关系 ---
    # 如果 User 模型中定义了 back_populates="assessments"
    # submitter = relationship("User", back_populates="assessments", lazy="selectin")
    # --- 关系定义结束 ---

    # (可选) 定义表级索引，如果需要复合索引
    # __table_args__ = (
    #     Index('ix_assessment_id_card_status', 'id_card', 'status'), # 示例复合索引
    # )

    def __repr__(self):
        """提供一个方便调试的对象表示"""
        return (f"<Assessment(id={self.id}, subject='{self.subject_name}', "
                f"type='{self.questionnaire_type}', status='{self.status}')>")

--------------------------------------------------
文件路径: app\models\association_tables.py
--------------------------------------------------
# FILE: app/models/association_tables.py
from sqlalchemy import Table, Column, Integer, ForeignKey
from app.db.base_class import Base # 导入 Base 以获取 metadata

# 定义评估与属性的关联表
assessment_attributes_table = Table(
    "assessment_attributes", # 表名
    Base.metadata,           # 关联到 Base 的元数据
    Column("assessment_id", Integer, ForeignKey("analysis_data.id"), primary_key=True), # 外键关联 analysis_data 表的 id
    Column("attribute_id", Integer, ForeignKey("attributes.id"), primary_key=True)     # 外键关联 attributes 表的 id
)

--------------------------------------------------
文件路径: app\models\attribute.py
--------------------------------------------------
# FILE: app/models/attribute.py
from sqlalchemy import Column, Integer, String, Text, Index
from sqlalchemy.orm import relationship
from app.db.base_class import Base
from .association_tables import assessment_attributes_table # 稍后创建这个文件

class Attribute(Base):
    __tablename__ = "attributes"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String(100), unique=True, index=True, nullable=False) # 属性名称，唯一且索引
    description = Column(Text, nullable=True)                          # 属性描述
    category = Column(String(50), index=True, nullable=True)           # 属性分类（可选）

    # 定义与 Assessment 的多对多关系
    # 'assessments' 是在 Attribute 实例上访问关联 Assessment 对象的属性名
    # secondary 指向我们即将定义的关联表
    # back_populates 用于双向关系，需要在 Assessment 模型中定义 'attributes'
    assessments = relationship(
        "Assessment",
        secondary=assessment_attributes_table,
        back_populates="attributes" # 与 Assessment 模型中的关系名对应
    )

    def __repr__(self):
        return f"<Attribute(id={self.id}, name='{self.name}', category='{self.category}')>"

# 可以添加表级索引
# __table_args__ = (
#     Index('ix_attribute_category_name', 'category', 'name'),
# )

--------------------------------------------------
文件路径: app\models\interrogation.py
--------------------------------------------------
# FILE: app/models/interrogation.py (新建)
from sqlalchemy import Column, Integer, String, JSON, TIMESTAMP, ForeignKey, Text
from sqlalchemy.sql import func
from app.db.base_class import Base

class InterrogationRecord(Base):
    __tablename__ = "interrogation_records"

    id = Column(Integer, primary_key=True, index=True)
    # 关联进行审讯的管理员 (假设 User 模型已存在)
    interrogator_id = Column(Integer, ForeignKey("users.id"), nullable=False, index=True)
    # 存储 JSON 格式的基础信息
    basic_info = Column(JSON, nullable=True)
    # 存储 JSON 格式的问答对列表 [{'q': '...', 'a': '...'}, ...]
    qas = Column(JSON, nullable=True)
    # 记录状态: ongoing, completed, cancelled
    status = Column(String(50), nullable=False, default="ongoing", index=True)
    # 完整的审讯文本 (可选，如果需要存储最终格式化文本)
    full_text = Column(Text, nullable=True)

    created_at = Column(TIMESTAMP, server_default=func.now(), nullable=False)
    updated_at = Column(TIMESTAMP, server_default=func.now(), onupdate=func.now(), nullable=False)

    def __repr__(self):
        person_name = self.basic_info.get("person_name", "未知") if isinstance(self.basic_info, dict) else "未知"
        return f"<InterrogationRecord(id={self.id}, person='{person_name}', status='{self.status}')>"

# --- 如果使用 SQLite，可能需要为 JSON 列添加索引（取决于具体查询需求） ---
# from sqlalchemy import Index
# Index('ix_interrogation_basic_info_name', InterrogationRecord.basic_info['person_name']) # 示例

--------------------------------------------------
文件路径: app\models\user.py
--------------------------------------------------
from sqlalchemy import Column, Integer, String, Boolean
from app.db.base_class import Base # 从我们创建的基类导入

class User(Base):
    __tablename__ = "users" # 数据库中的表名

    id = Column(Integer, primary_key=True, index=True)
    # 用户名，唯一且加索引，不允许为空
    username = Column(String(100), unique=True, index=True, nullable=False)
    # 邮箱，唯一且加索引，可以为空
    email = Column(String(255), unique=True, index=True, nullable=True)
    # 存储哈希后的密码，不允许为空
    hashed_password = Column(String(255), nullable=False)
    # 全名，可以为空
    full_name = Column(String(100), nullable=True)
    # 是否激活，默认为 True
    is_active = Column(Boolean(), default=True, nullable=False)
    # 是否为超级管理员，默认为 False
    is_superuser = Column(Boolean(), default=False, nullable=False)

    # __repr__ 方法用于方便调试时打印对象信息
    def __repr__(self):
        return f"<User(id={self.id}, username='{self.username}', email='{self.email}')>"

# 你可以在这里定义其他模型，如 Assessment, Report 等
# 它们都需要继承自 Base

--------------------------------------------------
文件路径: app\models\__init__.py
--------------------------------------------------
from .user import User
from .assessment import Assessment
from .interrogation import InterrogationRecord
from .attribute import Attribute # <--- 新增导入
# 关联表通常不需要在这里导出，除非你直接使用它

__all__ = [
    "User",
    "Assessment",
    "InterrogationRecord",
    "Attribute", # <--- 添加到列表
]

--------------------------------------------------
文件路径: app\routers\admin.py
--------------------------------------------------
import logging
from typing import List, Optional, Dict, Any
import copy
import json
import os

from fastapi import APIRouter, Depends, HTTPException, status, Query, Body, Request, Response
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.exc import IntegrityError, SQLAlchemyError
from pydantic import BaseModel, Field

# --- Core App Imports ---
from app.core.config import settings
from app.core.deps import get_db, get_current_active_superuser
from app import crud, models, schemas

# --- AI & Utils Imports ---
from src.ai_utils import generate_report_content
try:
    from src.interrogation_ai import suggest_next_question
except ImportError:
    suggest_next_question = None
    logging.getLogger(settings.APP_NAME).warning("未能导入 src.interrogation_ai.suggest_next_question")
try:
    from src.guidance_generator import generate_guidance
except ImportError:
    generate_guidance = None
    logging.getLogger(settings.APP_NAME).warning("未能导入 src.guidance_generator.generate_guidance")

# --- JSON Template Loading ---
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
DATA_DIR = os.path.join(BASE_DIR, 'data')
INTERROGATION_TEMPLATE_PATH = os.path.join(DATA_DIR, 'interrogation_template.json')
INTERROGATION_TEMPLATE: Dict[str, Any] = {"error": "模板未加载", "questions_answers": []}
try:
    with open(INTERROGATION_TEMPLATE_PATH, 'r', encoding='utf-8') as f:
        INTERROGATION_TEMPLATE = json.load(f)
except Exception as e:
    logging.getLogger(settings.APP_NAME).error(f"加载审讯模板失败: {e}", exc_info=True)

# --- Logger and Router ---
logger = logging.getLogger(settings.APP_NAME)
router = APIRouter(
    prefix="/admin",
    tags=["后台管理 (Admin)"],
    dependencies=[Depends(get_current_active_superuser)]
)

# --- 报告查询 Endpoint ---
@router.get(
    "/assessments/search",
    response_model=List[schemas.AssessmentSummary],
    summary="按身份证号查询评估记录"
)
async def search_assessments_by_id_card(
    id_card: str = Query(..., description="要查询的身份证号"),
    db: AsyncSession = Depends(get_db)
):
    logger.info(f"管理员正在按身份证号 '{id_card}' 查询评估记录")
    if not id_card:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="必须提供身份证号")
    try:
        assessments = await crud.assessment.get_assessments_by_id_card(db, id_card=id_card)
        if not assessments:
            logger.info(f"未找到身份证号为 '{id_card}' 的评估记录")
            return []
        logger.info(f"找到 {len(assessments)} 条身份证号为 '{id_card}' 的评估记录")
        return [schemas.AssessmentSummary.model_validate(a, from_attributes=True) for a in assessments]
    except Exception as e:
        logger.error(f"按身份证号查询评估时出错 ('{id_card}'): {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="查询评估记录时发生错误")

# --- 数据分析 Endpoint ---
@router.get(
    "/stats/demographics",
    response_model=schemas.DemographicsStats,
    summary="获取用户人口统计数据"
)
async def get_demographics_stats(db: AsyncSession = Depends(get_db)):
    logger.info("管理员请求人口统计数据")
    try:
        age_data = await crud.stats.get_age_distribution(db)
        gender_data = await crud.stats.get_gender_distribution(db)
        return schemas.DemographicsStats(ageData=age_data, genderData=gender_data)
    except Exception as e:
        logger.error(f"获取人口统计数据时出错: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="获取统计数据时发生错误")

# --- 辅助智能审讯笔录 Endpoints ---
@router.post(
    "/interrogation/start",
    response_model=schemas.InterrogationRecordRead,
    status_code=status.HTTP_201_CREATED,
    summary="开始新的审讯并获取模板"
)
async def start_interrogation(
    basic_info: schemas.InterrogationBasicInfo = Body(...),
    db: AsyncSession = Depends(get_db),
    current_admin: models.User = Depends(get_current_active_superuser)
):
    logger.info(f"管理员 {current_admin.username} 正在为 '{basic_info.person_name}' 开始新的审讯")
    if "error" in INTERROGATION_TEMPLATE:
        logger.error("审讯模板加载失败，无法开始审讯")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="审讯模板加载失败")

    template_data = copy.deepcopy(INTERROGATION_TEMPLATE)
    filled_basic_info = template_data.copy()
    filled_basic_info.update(basic_info.model_dump(exclude_unset=True))
    filled_basic_info.setdefault('person_name', basic_info.person_name)
    filled_basic_info.setdefault('person_gender', basic_info.person_gender)
    filled_basic_info.setdefault('person_id_type_number', basic_info.person_id_type_number)
    filled_basic_info.pop('questions_answers', None)
    filled_basic_info.pop('signature_section', None)
    initial_qas = template_data.get('questions_answers', [])

    try:
        new_record = await crud.interrogation.create_interrogation(
            db=db,
            interrogator_id=current_admin.id,
            basic_info=filled_basic_info,
            qas=initial_qas
        )
        logger.info(f"为 '{basic_info.person_name}' 创建了新的审讯记录 ID: {new_record.id}")
        return schemas.InterrogationRecordRead.model_validate(new_record, from_attributes=True)
    except Exception as e:
        logger.error(f"创建审讯记录时出错: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="无法开始新的审讯记录")

@router.post(
    "/interrogation/{record_id}/suggest",
    response_model=List[str],
    summary="获取下一个审讯问题的 AI 建议"
)
async def suggest_interrogation_question(
    record_id: int,
    current_qas: List[schemas.InterrogationQAInput] = Body(...),
    db: AsyncSession = Depends(get_db)
):
    logger.info(f"管理员请求审讯记录 ID: {record_id} 的下一个问题建议")
    if not suggest_next_question:
        raise HTTPException(status_code=status.HTTP_501_NOT_IMPLEMENTED, detail="AI 建议功能未配置")

    record = await crud.interrogation.get_interrogation(db, record_id=record_id)
    if not record:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="审讯记录未找到")

    basic_info = record.basic_info or {}
    try:
        suggestions = suggest_next_question(basic_info=basic_info, history=current_qas)
        logger.info(f"为审讯记录 ID: {record_id} 生成了 {len(suggestions)} 条建议")
        return suggestions
    except Exception as e:
        logger.error(f"生成审讯建议时出错 (ID: {record_id}): {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="获取 AI 建议时出错")

@router.put(
    "/interrogation/{record_id}",
    response_model=schemas.InterrogationRecordRead,
    summary="保存更新后的审讯笔录"
)
async def save_interrogation(
    record_id: int,
    record_update: schemas.InterrogationRecordUpdate = Body(...),
    db: AsyncSession = Depends(get_db)
):
    logger.info(f"管理员正在保存审讯记录 ID: {record_id}")
    try:
        updated_record = await crud.interrogation.update_interrogation(
            db=db,
            record_id=record_id,
            update_data=record_update
        )
        if not updated_record:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="审讯记录未找到")
        logger.info(f"审讯记录 ID: {record_id} 已成功保存")
        return schemas.InterrogationRecordRead.model_validate(updated_record, from_attributes=True)
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"保存审讯记录时出错 (ID: {record_id}): {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="保存审讯记录时发生错误")

# --- 专项指导方案 Endpoints ---
async def get_guidance_for_person(
    db: AsyncSession,
    id_card: str,
    guidance_type: str
) -> schemas.GuidanceResponse:
    logger.info(f"为身份证号 '{id_card}' 生成 '{guidance_type}' 指导方案")
    if not generate_guidance:
        raise HTTPException(status_code=status.HTTP_501_NOT_IMPLEMENTED, detail="指导方案生成功能未配置")

    assessment = await crud.assessment.get_latest_completed_by_id_card(db, id_card=id_card)
    if not assessment:
        logger.warning(f"未找到身份证号 '{id_card}' 的已完成评估报告")
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="未找到该身份证号对应的已完成评估报告")
    if not assessment.report_text:
        logger.warning(f"找到评估记录 ID {assessment.id}，但报告文本为空")
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="找到评估记录，但报告内容为空")

    try:
        guidance_text = generate_guidance(report_text=assessment.report_text, scenario=guidance_type)
        if not guidance_text:
            raise ValueError("AI未能生成指导方案文本")
        logger.info(f"成功为身份证号 '{id_card}' 生成 '{guidance_type}' 指导方案")
        return schemas.GuidanceResponse(
            report=schemas.ReportData.model_validate(assessment, from_attributes=True),
            guidance=guidance_text
        )
    except ValueError as ve:
        logger.error(f"生成指导方案时值错误 (ID卡: {id_card}, 类型: {guidance_type}): {ve}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"生成指导方案失败: {ve}")
    except Exception as e:
        logger.error(f"生成指导方案时出错 (ID卡: {id_card}, 类型: {guidance_type}): {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="生成指导方案时发生内部错误")

@router.get(
    "/guidance/petitioner",
    response_model=schemas.GuidanceResponse,
    summary="获取上访户情绪疏导方案"
)
async def get_petitioner_guidance(
    id_card: str = Query(..., description="上访户身份证号"),
    db: AsyncSession = Depends(get_db)
):
    return await get_guidance_for_person(db, id_card, "petitioner")

@router.get(
    "/guidance/juvenile",
    response_model=schemas.GuidanceResponse,
    summary="获取未成年人心理辅导方案"
)
async def get_juvenile_guidance(
    id_card: str = Query(..., description="未成年人身份证号"),
    db: AsyncSession = Depends(get_db)
):
    return await get_guidance_for_person(db, id_card, "juvenile")

@router.get(
    "/guidance/police",
    response_model=schemas.GuidanceResponse,
    summary="获取民辅警心理调适建议"
)
async def get_police_guidance(
    id_card: str = Query(..., description="民辅警身份证号"),
    db: AsyncSession = Depends(get_db)
):
    return await get_guidance_for_person(db, id_card, "police")

# --- 用户管理 Endpoints ---
@router.get(
    "/users",
    response_model=schemas.UserListResponse,
    summary="获取用户列表"
)
async def list_users(
    db: AsyncSession = Depends(get_db),
    skip: int = Query(0, ge=0, description="跳过的记录数"),
    limit: int = Query(100, ge=1, le=200, description="返回的最大记录数")
):
    logger.info(f"管理员请求用户列表，skip={skip}, limit={limit}")
    try:
        users, total_count = await crud.user.get_users(db, skip=skip, limit=limit)
        return schemas.UserListResponse(
            total=total_count,
            users=[schemas.User.model_validate(u, from_attributes=True) for u in users]
        )
    except Exception as e:
        logger.error(f"获取用户列表时出错: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="获取用户列表失败")

@router.patch(
    "/users/{user_id}",
    response_model=schemas.User,
    summary="更新用户信息 (管理员)"
)
async def update_user_admin(
    user_id: int,
    user_in: schemas.UserUpdateAdmin = Body(...),
    db: AsyncSession = Depends(get_db),
    current_admin: models.User = Depends(get_current_active_superuser)
):
    logger.info(f"管理员 {current_admin.username} 正在更新用户 ID: {user_id}")
    db_user = await crud.user.get_user(db, user_id=user_id)
    if not db_user:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="用户未找到")

    try:
        updated_user = await crud.user.update_user_admin(db=db, db_user=db_user, user_in=user_in)
        logger.info(f"用户 ID {user_id} 信息已由管理员 {current_admin.username} 更新")
        return schemas.User.model_validate(updated_user, from_attributes=True)
    except Exception as e:
        logger.error(f"管理员更新用户 ID {user_id} 时出错: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="更新用户信息失败")

# --- 属性管理 Endpoints ---
@router.post(
    "/attributes",
    response_model=schemas.AttributeRead,
    status_code=status.HTTP_201_CREATED,
    summary="创建新属性标签"
)
async def create_new_attribute(
    attribute_in: schemas.AttributeCreate,
    db: AsyncSession = Depends(get_db)
):
    logger.info(f"Admin: 尝试创建属性: Name='{attribute_in.name}', Category='{attribute_in.category}'")
    try:
        attribute = await crud.attribute.create_attribute(db=db, attribute_in=attribute_in)
        return attribute
    except ValueError as ve:
        logger.warning(f"Admin: 创建属性失败，值错误: {ve}")
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=str(ve))
    except Exception as e:
        logger.error(f"Admin: 创建属性时发生意外错误: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="创建属性失败")

@router.get(
    "/attributes",
    response_model=List[schemas.AttributeRead],
    summary="获取属性标签列表"
)
async def list_attributes(
    db: AsyncSession = Depends(get_db),
    skip: int = Query(0, ge=0, description="跳过的记录数"),
    limit: int = Query(100, ge=1, le=500, description="每页最大记录数"),
    category: Optional[str] = Query(None, description="按分类过滤 (例如 '情绪状态')")
):
    logger.info(f"Admin: 获取属性列表, skip={skip}, limit={limit}, category={category}")
    try:
        attributes = await crud.attribute.get_attributes(db=db, skip=skip, limit=limit, category=category)
        return attributes
    except Exception as e:
        logger.error(f"Admin: 获取属性列表时出错: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="获取属性列表失败")

@router.get(
    "/attributes/{attribute_id}",
    response_model=schemas.AttributeRead,
    summary="获取单个属性详情"
)
async def read_attribute(
    attribute_id: int,
    db: AsyncSession = Depends(get_db)
):
    logger.info(f"Admin: 请求属性详情 ID: {attribute_id}")
    db_attribute = await crud.attribute.get_attribute(db, attribute_id=attribute_id)
    if db_attribute is None:
        logger.warning(f"Admin: 未找到属性 ID: {attribute_id}")
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="属性未找到")
    return db_attribute

@router.put(
    "/attributes/{attribute_id}",
    response_model=schemas.AttributeRead,
    summary="更新属性标签"
)
async def update_existing_attribute(
    attribute_id: int,
    attribute_in: schemas.AttributeUpdate,
    db: AsyncSession = Depends(get_db)
):
    logger.info(f"Admin: 尝试更新属性 ID: {attribute_id}")
    db_attribute = await crud.attribute.get_attribute(db, attribute_id=attribute_id)
    if not db_attribute:
        logger.warning(f"Admin: 更新失败，未找到属性 ID: {attribute_id}")
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="属性未找到")

    try:
        updated_attribute = await crud.attribute.update_attribute(
            db=db, db_attribute=db_attribute, attribute_in=attribute_in
        )
        return updated_attribute
    except ValueError as ve:
        logger.warning(f"Admin: 更新属性 ID {attribute_id} 失败: {ve}")
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=str(ve))
    except Exception as e:
        logger.error(f"Admin: 更新属性 ID {attribute_id} 时发生意外错误: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="更新属性失败")

@router.delete(
    "/attributes/{attribute_id}",
    status_code=status.HTTP_204_NO_CONTENT,
    summary="删除属性标签"
)
async def delete_existing_attribute(
    attribute_id: int,
    db: AsyncSession = Depends(get_db)
):
    logger.warning(f"Admin: 尝试删除属性 ID: {attribute_id} - 操作可能因外键约束而失败")
    try:
        deleted = await crud.attribute.delete_attribute(db=db, attribute_id=attribute_id)
        if not deleted:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="属性未找到")
        return Response(status_code=status.HTTP_204_NO_CONTENT)
    except ValueError as ve:
        logger.error(f"Admin: 删除属性 ID {attribute_id} 失败: {ve}")
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=str(ve))
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"Admin: 删除属性 ID {attribute_id} 时发生意外错误: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="删除属性失败")

# --- 评估与属性关联管理 Endpoints ---
@router.post(
    "/assessments/{assessment_id}/attributes/{attribute_id}",
    response_model=schemas.AssessmentSummary,
    status_code=status.HTTP_201_CREATED,
    summary="将属性关联到评估记录"
)
async def associate_attribute_with_assessment(
    assessment_id: int,
    attribute_id: int,
    db: AsyncSession = Depends(get_db)
):
    logger.info(f"Admin: 尝试将属性 ID {attribute_id} 关联到评估 ID {assessment_id}")
    updated_assessment = await crud.assessment.add_attribute_to_assessment(
        db=db, assessment_id=assessment_id, attribute_id=attribute_id
    )
    if updated_assessment is None:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="评估记录或属性未找到")
    return schemas.AssessmentSummary.model_validate(updated_assessment, from_attributes=True)

@router.delete(
    "/assessments/{assessment_id}/attributes/{attribute_id}",
    status_code=status.HTTP_204_NO_CONTENT,
    summary="从评估记录解除属性关联"
)
async def disassociate_attribute_from_assessment(
    assessment_id: int,
    attribute_id: int,
    db: AsyncSession = Depends(get_db)
):
    logger.info(f"Admin: 尝试从评估 ID {assessment_id} 解除属性 ID {attribute_id} 的关联")
    updated_assessment = await crud.assessment.remove_attribute_from_assessment(
        db=db, assessment_id=assessment_id, attribute_id=attribute_id
    )
    if updated_assessment is None:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="评估记录或属性未找到")
    return Response(status_code=status.HTTP_204_NO_CONTENT)

# --- 设置评估的所有属性 Endpoint ---
class AssessmentAttributesUpdate(BaseModel):
    attribute_ids: List[int] = Field(..., description="要设置的属性 ID 列表 (将覆盖现有所有关联)")

@router.put(
    "/assessments/{assessment_id}/attributes",
    response_model=schemas.AssessmentSummary,
    summary="设置评估记录的所有属性标签"
)
async def set_assessment_attributes_endpoint(
    assessment_id: int,
    attributes_in: AssessmentAttributesUpdate = Body(...),
    db: AsyncSession = Depends(get_db)
):
    logger.info(f"Admin: 尝试设置评估 ID {assessment_id} 的属性为: {attributes_in.attribute_ids}")
    updated_assessment = await crud.assessment.set_assessment_attributes(
        db=db, assessment_id=assessment_id, attribute_ids=attributes_in.attribute_ids
    )
    if updated_assessment is None:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="评估记录未找到")
    return schemas.AssessmentSummary.model_validate(updated_assessment, from_attributes=True)

--------------------------------------------------
文件路径: app\routers\assessments.py
--------------------------------------------------
# FILE: app/routers/assessments.py (修改后)
import logging
import os
import json
from datetime import datetime
from fastapi import APIRouter, HTTPException, Depends, File, UploadFile, Form, Request, status
from typing import Dict, Any, Optional
from sqlalchemy.ext.asyncio import AsyncSession
# --- 修改: 导入更具体的数据库异常 ---
from sqlalchemy.exc import IntegrityError, SQLAlchemyError

# --- 核心应用导入 ---
from app.core.config import settings
from app.schemas.assessment import AssessmentSubmitResponse
# 确保 Celery 任务可导入
try:
    from app.tasks.analysis import run_ai_analysis
except ImportError:
    run_ai_analysis = None # 如果导入失败，定义为 None
    logging.getLogger(settings.APP_NAME or "FallbackLogger").error("未能导入 Celery 任务 'run_ai_analysis'。后台处理已禁用。")

# --- 认证与数据库导入 ---
from app.core.deps import get_current_active_user, get_db # 使用异步 get_db
from app import models, schemas # 导入 schemas 供潜在使用
# 导入主 crud 包 (确保 app/crud/__init__.py 导入了 assessment)
from app import crud

# --- 工具函数 ---
try:
    from werkzeug.utils import secure_filename
except ImportError:
    import re
    def secure_filename(filename: str) -> str:
        """
        一个 werkzeug.utils.secure_filename 的基本替代实现。
        限制字符集并防止路径遍历。
        """
        if not filename:
            return "invalid_filename"
        # 移除不安全的字符
        filename = re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)
        # 移除开头/结尾的点或下划线
        filename = filename.strip('._')
        # 防止文件名变为空
        return filename if filename else "invalid_filename"
    # 安全地获取 logger
    logging.getLogger(settings.APP_NAME or "FallbackLogger").warning("未安装 werkzeug。使用基本的 secure_filename 备用方案。")

# 使用配置的应用 logger
logger = logging.getLogger(settings.APP_NAME)
router = APIRouter()

# --- API 端点 ---
@router.post(
    "/assessments/submit",
    response_model=AssessmentSubmitResponse,
    tags=["Assessments"],
    status_code=status.HTTP_202_ACCEPTED # 成功时返回 202 Accepted
)
async def submit_assessment(
    # --- 依赖 ---
    db: AsyncSession = Depends(get_db),
    request: Request = None, # 保留用于解析表单数据
    current_user: models.User = Depends(get_current_active_user), # 认证依赖

    # --- 表单字段 (名称需与 JS 中的 FormData 匹配) ---
    name: str = Form(..., description="姓名"),
    gender: str = Form(..., description="性别"),
    age: int = Form(..., gt=0, description="年龄"),
    id_card: Optional[str] = Form(None, description="身份证号"),
    occupation: Optional[str] = Form(None, description="职业"),
    case_name: Optional[str] = Form(None, description="案件名称"),
    case_type: Optional[str] = Form(None, description="案件类型"),
    identity_type: Optional[str] = Form(None, description="人员身份"),
    person_type: Optional[str] = Form(None, description="人员类型"),
    marital_status: Optional[str] = Form(None, description="婚姻状况"),
    children_info: Optional[str] = Form(None, description="子女情况"),
    criminal_record: Optional[int] = Form(0, ge=0, le=1, description="有无犯罪前科 (0:无, 1:有)"),
    health_status: Optional[str] = Form(None, description="健康情况"),
    phone_number: Optional[str] = Form(None, description="手机号"),
    domicile: Optional[str] = Form(None, description="归属地"),
    scale_type: Optional[str] = Form(None, description="选择的量表代码"),
    # 确保 'image' 匹配 HTML/JS 中文件输入的 name 属性
    image: Optional[UploadFile] = File(None, description="上传的绘画图片")
):
    """
    接收来自已认证用户的评估数据。
    保存数据并排队等待后台 AI 分析任务。
    """
    # +++ 在潜在的数据库错误发生前获取用户名和 ID +++
    submitter_username = current_user.username
    submitter_id = current_user.id
    logger.info(f"用户 '{submitter_username}' (ID: {submitter_id}) 正在提交新的评估，主体姓名: {name}")

    # --- 1. 收集基础信息 (将表单字段映射到数据库模型字段) ---
    basic_info: Dict[str, Any] = {
        "subject_name": name, # 将表单的 'name' 映射到数据库模型的 'subject_name'
        "gender": gender,
        "age": age,
        "id_card": id_card,
        "occupation": occupation,
        "case_name": case_name,
        "case_type": case_type,
        "identity_type": identity_type,
        "person_type": person_type,
        "marital_status": marital_status,
        "children_info": children_info,
        "criminal_record": criminal_record, # 已经是 int 0 或 1
        "health_status": health_status,
        "phone_number": phone_number,
        "domicile": domicile,
        "submitter_id": submitter_id # 添加认证用户的 ID
    }
    logger.debug(f"收集的基础信息 (待存入数据库): {basic_info}")

    # --- 2. 处理图片上传 ---
    image_relative_path: Optional[str] = None
    image_full_path: Optional[str] = None # 跟踪完整路径以备保存
    image_was_saved_to_disk: bool = False # 标记，以便在出错时清理

    if image and image.filename:
        # 清理文件名
        original_filename = secure_filename(image.filename)
        if original_filename == "invalid_filename":
             logger.warning(f"用户 {submitter_username} 上传了无效的文件名 '{image.filename}'。")
             # 可选择抛出 HTTPException 或在没有图片的情况下继续
             # raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="无效的文件名。")
             image = None # 视为未上传图片

        if image: # 再次检查文件名检查后 image 是否仍然有效
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            # 使用身份证号（如果提供）或姓名（如果提供）或 'UnknownID' 作为文件名的一部分
            id_part = secure_filename(id_card if id_card else (name if name else 'UnknownID'))
            base, ext = os.path.splitext(original_filename)
            safe_base = base[:50] # 限制基本文件名长度
            # 标准化扩展名为小写
            ext_lower = ext.lower()
            # 允许的文件类型示例
            allowed_extensions = {'.png', '.jpg', '.jpeg', '.gif', '.webp'}
            if ext_lower not in allowed_extensions:
                logger.warning(f"用户 {submitter_username} 上传了不允许的文件类型: {ext_lower}")
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"不允许的文件类型: {ext}. 请上传 {', '.join(allowed_extensions)} 格式的文件。"
                )

            # 构建用于保存的安全文件名
            image_filename_to_save = f"{id_part}_{timestamp}_{safe_base}{ext_lower}"
            image_full_path = os.path.join(settings.UPLOADS_DIR, image_filename_to_save)
            # 在数据库中存储相对路径（或仅文件名）
            image_relative_path = image_filename_to_save # 或根据你提供文件的方式进行调整

            try:
                # 确保上传目录存在
                os.makedirs(settings.UPLOADS_DIR, exist_ok=True)
                # 异步读取文件内容并保存
                file_content = await image.read()
                with open(image_full_path, "wb") as buffer:
                    buffer.write(file_content)
                image_was_saved_to_disk = True # 标记为已保存，以备潜在清理
                logger.info(f"图片由用户 {submitter_username} 保存至: {image_full_path}")
            except OSError as e: # 捕获文件系统相关的错误
                logger.error(f"用户 {submitter_username} 保存上传图片至 {image_full_path} 时发生文件系统错误: {e}", exc_info=True)
                # 如果保存失败则不继续，通知用户
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"保存上传文件时发生服务器错误。")
            except Exception as e:
                logger.error(f"用户 {submitter_username} 保存上传图片至 {image_full_path} 时发生未知错误: {e}", exc_info=True)
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"处理上传文件时发生意外错误。")
            finally:
                 # 确保文件被关闭 (UploadFile 应该会处理这个，但这是好习惯)
                 await image.close()
    else:
        logger.info(f"用户 {submitter_username} 未上传图片。")

    # --- 3. 收集量表答案 (从动态表单字段 q1, q2...) ---
    scale_answers_dict: Dict[str, Any] = {}
    scale_answers_json: Optional[str] = None
    if scale_type:
        if request is None:
             logger.error("未注入 Request 对象，无法解析量表答案。这是一个服务器配置问题。")
             # 这表示服务器端设置问题
             raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="内部服务器错误: 无法访问请求对象。")
        try:
            # 异步获取所有表单数据
            form_data = await request.form()
            for key, value in form_data.items():
                # 检查 key 是否以 'q' 开头后跟数字
                if key.startswith('q') and key[1:].isdigit():
                    # 尝试将值转换为数字（如果可能），否则保持字符串
                    try:
                        scale_answers_dict[key] = int(value)
                    except ValueError:
                        try:
                            scale_answers_dict[key] = float(value)
                        except ValueError:
                            scale_answers_dict[key] = value # 保留为字符串

            if scale_answers_dict:
                # 将收集到的答案转换为 JSON 字符串以便数据库存储
                scale_answers_json = json.dumps(scale_answers_dict, ensure_ascii=False, sort_keys=True) # 排序以保证一致性
                logger.info(f"用户 {submitter_username} 为量表 '{scale_type}' 收集到的答案: {len(scale_answers_dict)} 条")
                logger.debug(f"量表答案 (JSON): {scale_answers_json}")
            else:
                logger.warning(f"用户 {submitter_username} 提供了量表类型 '{scale_type}', 但未在表单中找到以 'q' 开头的答案。")
                # 根据需求，你可能需要抛出错误或允许提交时没有答案
                # raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"选择了量表 '{scale_type}' 但未提供答案。")
        except Exception as e:
             logger.error(f"用户 {submitter_username} 解析量表答案时出错: {e}", exc_info=True)
             # 在没有量表数据的情况下继续或抛出错误
             scale_answers_json = None # 确保如果解析失败则为 None
             # raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"解析量表答案时出错: {e}")

    # --- 4. 使用异步 CRUD 保存初始数据 ---
    assessment_id: Optional[int] = None
    new_assessment: Optional[models.Assessment] = None # 初始化为 None
    try:
        # --- *** 通过导入的 crud 包访问 assessment CRUD *** ---
        # 现在需要 app/crud/__init__.py 包含 `from . import assessment`
        new_assessment = await crud.assessment.create(
            db=db,
            # 使用与 Assessment 模型字段匹配的关键字参数传递收集的数据
            **basic_info, # 解包基础信息字典
            image_path=image_relative_path, # 存储相对路径/文件名
            questionnaire_type=scale_type,
            questionnaire_data=scale_answers_json, # 存储 JSON 字符串
            report_text=None # 初始报告文本为空
        )
        # --- 移除这里的检查，因为 CRUD 函数现在保证返回有效对象或抛出异常 ---
        # if not new_assessment or not hasattr(new_assessment, 'id'):
        #     raise ValueError("数据保存操作未返回有效的评估对象ID。")

        # 现在可以安全获取 ID (假设 create 成功时总会提交并刷新对象)
        # 注意: 如果 create 内部没有 commit/refresh, ID 可能还是 None
        # 确保 crud.assessment.create 在成功时返回带有 ID 的对象
        if new_assessment and new_assessment.id:
            assessment_id = new_assessment.id
            logger.info(f"评估数据由用户 {submitter_username} 保存成功。评估 ID: {assessment_id}")
        else:
             # 这是一个异常情况，如果 CRUD 成功但没有 ID
             logger.error(f"用户 {submitter_username} 的评估数据似乎已保存，但未能获取 ID。CRUD 实现可能需要检查。")
             raise SQLAlchemyError("数据库操作成功，但未能检索到新记录的 ID。") # 抛出一个通用的 DB 错误

    except IntegrityError as ie: # --- 捕获 IntegrityError (例如，唯一约束冲突) ---
        logger.warning(f"用户 {submitter_username} 保存评估数据时发生数据库完整性错误: {ie}", exc_info=True)
        await db.rollback() # 回滚数据库事务
        # 清理可能已保存的图片
        if image_was_saved_to_disk and image_full_path and os.path.exists(image_full_path):
            try:
                os.remove(image_full_path)
                logger.info(f"因数据库完整性错误清理了文件 {image_full_path}")
            except Exception as rm_err:
                logger.warning(f"数据库完整性错误后无法移除文件 {image_full_path}: {rm_err}")
        # 返回 409 Conflict 状态码
        # 可以根据具体错误 (ie.args) 提供更具体的 detail，但要小心暴露内部信息
        error_detail = "数据保存冲突。可能某个唯一字段（如身份证号）已存在。"
        # 检查是否是特定的唯一约束错误，例如，如果你的身份证字段有唯一约束 'uq_assessment_id_card'
        # if "uq_assessment_id_card" in str(ie).lower():
        #     error_detail = "保存失败：该身份证号已被使用。"
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=error_detail)

    except TypeError as te: # --- 捕获 TypeError (通常来自模型初始化时字段类型不匹配) ---
        logger.warning(f"用户 {submitter_username} 提交的数据字段与预期模型类型不匹配: {te}", exc_info=True)
        await db.rollback() # 尽管可能还没到数据库操作，回滚以防万一
        # 清理可能已保存的图片
        if image_was_saved_to_disk and image_full_path and os.path.exists(image_full_path):
             try:
                 os.remove(image_full_path)
                 logger.info(f"因类型错误清理了文件 {image_full_path}")
             except Exception as rm_err:
                 logger.warning(f"类型错误后无法移除文件 {image_full_path}: {rm_err}")
        # 返回 422 Unprocessable Entity 状态码，表示数据无法处理
        raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail=f"提交的数据字段无效或类型错误: {te}")

    except SQLAlchemyError as dbe: # --- 捕获其他 SQLAlchemy 相关错误 (连接、其他约束等) ---
         logger.error(f"用户 {submitter_username} 保存评估数据时发生数据库操作错误: {dbe}", exc_info=True)
         await db.rollback() # 回滚数据库事务
         # 清理可能已保存的图片
         if image_was_saved_to_disk and image_full_path and os.path.exists(image_full_path):
             try:
                 os.remove(image_full_path)
                 logger.info(f"因数据库操作错误清理了文件 {image_full_path}")
             except Exception as rm_err:
                 logger.warning(f"数据库操作错误后无法移除文件 {image_full_path}: {rm_err}")
         # 返回 500 Internal Server Error
         raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"数据库操作失败。请稍后重试或联系管理员。")

    except Exception as e: # --- 捕获所有其他意外错误 ---
        logger.error(f"用户 {submitter_username} 处理评估提交时发生意外错误: {e}", exc_info=True)
        await db.rollback() # 尝试回滚以防万一
        # 清理可能已保存的图片
        if image_was_saved_to_disk and image_full_path and os.path.exists(image_full_path):
            try:
                os.remove(image_full_path)
                logger.info(f"因意外错误清理了文件 {image_full_path}")
            except Exception as rm_err:
                logger.warning(f"意外错误后无法移除文件 {image_full_path}: {rm_err}")
        # 返回 500 Internal Server Error
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"处理请求时发生内部服务器错误。")

    # --- 5. 触发 Celery 任务 (仅当 assessment_id 成功获取后执行) ---
    task_id: Optional[str] = None
    if assessment_id: # 只有在数据成功保存并获取 ID 后才排队
        if run_ai_analysis:
            try:
                # 仅传递任务所需的 ID
                task = run_ai_analysis.delay(assessment_id)
                task_id = task.id
                logger.info(f"已为评估 ID: {assessment_id} (提交者: {submitter_username}) 排队 AI 分析任务。任务 ID: {task_id}")
            except Exception as celery_err:
                 # 记录错误，但请求本身是成功的（数据已保存）
                 logger.error(f"为评估 ID {assessment_id} (提交者: {submitter_username}) 排队 Celery 任务失败: {celery_err}", exc_info=True)
                 # 不要在此处引发 HTTPException，因为主要操作（保存数据）已成功。
                 # 响应消息将指示排队失败。
        else:
             logger.warning(f"Celery 任务 'run_ai_analysis' 未加载或不可用。评估 ID: {assessment_id} 的后台处理将不会运行。")

    # --- 6. 构建 API 响应 (基于成功保存和任务排队状态) ---
    if assessment_id:
        status_code_resp: str
        message: str
        if task_id:
            message = "评估数据已接收，正在后台进行 AI 分析。"
            status_code_resp = "processing_queued" # 状态码：处理已排队
        elif run_ai_analysis is None: # 检查任务函数本身是否为 None
            message = f"评估数据已接收 (ID: {assessment_id})，但后台分析任务未配置或导入失败。"
            status_code_resp = "warning_task_unavailable" # 状态码：警告，任务不可用
        else: # 任务函数存在但 .delay() 失败
            message = f"评估数据已接收 (ID: {assessment_id})，但启动后台处理任务时出错。"
            status_code_resp = "warning_queueing_failed" # 状态码：警告，排队失败

        return AssessmentSubmitResponse(
            status=status_code_resp,
            message=message,
            submission_id=assessment_id
            # task_id=task_id # 可选地在响应中包含 task_id
        )
    else:
        # 理论上，如果上面的异常处理正确，这个情况不应该发生
        logger.error(f"评估 ID 未能生成，但未捕获到明确异常。提交者: {submitter_username}。这可能表示 CRUD 函数实现有问题。")
        # 即使没有 assessment_id，如果代码执行到这里，意味着没有抛出预期的异常
        # 但这仍然是一个错误状态，因为我们期望有 ID
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="数据似乎已保存，但在获取确认 ID 时遇到问题。")

--------------------------------------------------
文件路径: app\routers\auth.py
--------------------------------------------------
# FILE: app/routers/auth.py
import logging
from datetime import timedelta
from typing import Any

from fastapi import APIRouter, Depends, HTTPException, status, Body # +++ Import Body
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.exc import IntegrityError # +++ Import for catching DB errors

from app.core.config import settings
from app.core import security
from app import crud, models, schemas
from app.core.deps import get_db, get_current_active_user

logger = logging.getLogger(settings.APP_NAME)
router = APIRouter()

# --- Existing Login Endpoint (No changes needed here) ---
@router.post("/auth/token", response_model=schemas.Token, tags=["Authentication"])
async def login_for_access_token(
    db: AsyncSession = Depends(get_db),
    form_data: OAuth2PasswordRequestForm = Depends()
) -> Any:
    """
    OAuth2 compatible token login, get an access token for future requests.
    Accepts standard form data with 'username' and 'password'.
    """
    logger.info(f"User login attempt: {form_data.username}")
    user = await crud.user.get_user_by_username(db, username=form_data.username)

    if not user or not security.verify_password(form_data.password, user.hashed_password):
        logger.warning(f"Login failed for user: {form_data.username}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )

    if not user.is_active:
        logger.warning(f"Inactive user login attempt: {form_data.username}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Inactive user")

    access_token = security.create_access_token(subject=user.username)
    logger.info(f"User login successful: {form_data.username}")

    return {"access_token": access_token, "token_type": "bearer"}

# +++ NEW Registration Endpoint +++
@router.post("/auth/register", response_model=schemas.User, status_code=status.HTTP_201_CREATED, tags=["Authentication"])
async def register_user(
    *,
    db: AsyncSession = Depends(get_db),
    # Expecting JSON body with username and password
    user_in: schemas.UserCreate = Body(...) # Use UserCreate schema, require body
) -> Any:
    """
    Create new user. Requires username and password.
    """
    logger.info(f"User registration attempt: {user_in.username}")

    # Check if user already exists
    existing_user = await crud.user.get_user_by_username(db, username=user_in.username)
    if existing_user:
        logger.warning(f"Username '{user_in.username}' already registered.")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Username already registered",
        )

    # Ensure password field is present (Pydantic validation should handle this, but double-check)
    if not user_in.password:
         raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="Password is required for registration.",
        )

    try:
        # Create the user using the async CRUD function
        # UserCreate schema already handles default values for optional fields
        # Password hashing happens inside crud.user.create_user
        user = await crud.user.create_user(db=db, user_in=user_in)
        logger.info(f"User '{user.username}' registered successfully with ID: {user.id}")
        # Return the created user data (excluding password) using the User schema
        return user
    except IntegrityError: # Catch potential race conditions or other DB unique constraint errors
        await db.rollback()
        logger.error(f"Database integrity error during registration for {user_in.username}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Username or email might already be registered (database error).",
        )
    except ValueError as ve: # Catch specific errors raised from CRUD (like explicit duplicate check)
         await db.rollback()
         logger.warning(f"Registration failed for {user_in.username}: {ve}")
         raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(ve), # Return the specific error message (e.g., "Username already registered")
         )
    except Exception as e:
        await db.rollback()
        logger.error(f"Unexpected error during registration for {user_in.username}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An internal error occurred during registration.",
        )


# --- Existing /users/me Endpoint (No changes needed) ---
@router.get("/users/me", response_model=schemas.User, tags=["Users"])
async def read_users_me(
    current_user: models.User = Depends(get_current_active_user)
):
     """
     Get current logged in user's details.
     """
     return current_user

--------------------------------------------------
文件路径: app\routers\encyclopedia.py
--------------------------------------------------
# 文件: app/routers/encyclopedia.py
import logging
import random
from typing import Optional, List, Dict
from fastapi import APIRouter, HTTPException, Query, status

from app.core.config import settings
from app.schemas.encyclopedia import EncyclopediaEntry, CategoriesResponse, EntriesResponse # 导入模型

logger = logging.getLogger(settings.APP_NAME)
router = APIRouter()

# --- 辅助函数：从加载的条目中获取数据 ---
def get_all_entries() -> List[Dict[str, str]]:
    """安全地获取配置中的百科条目列表"""
    entries = settings.PSYCHOLOGY_ENTRIES
    if not isinstance(entries, list):
        logger.error("配置中的 PSYCHOLOGY_ENTRIES 不是列表！")
        return []
    return entries

# --- 端点 1: 获取分类 ---
@router.get(
    "/encyclopedia/categories",
    response_model=CategoriesResponse,
    tags=["Encyclopedia"],
    summary="获取所有心理百科分类"
)
async def get_encyclopedia_categories():
    """
    返回所有心理百科条目的唯一分类名称列表。
    """
    all_entries = get_all_entries()
    if not all_entries:
        return CategoriesResponse(categories=[])

    # 提取所有分类并去重，然后排序
    try:
        categories = sorted(list(set(entry.get("category", "未分类") for entry in all_entries)))
        return CategoriesResponse(categories=categories)
    except Exception as e:
         logger.error(f"提取百科分类时出错: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail="处理分类列表时出错")


# --- 端点 2: 获取条目 (包含过滤和随机功能) ---
@router.get(
    "/encyclopedia/entries",
    # 响应模型根据情况可能是列表或单个条目，用 Union 或 Any，或者为随机单独建模型/端点
    # 为清晰起见，我们让它主要返回列表，随机情况特殊处理返回单个条目模型
    response_model=EntriesResponse, # 主要返回列表
    tags=["Encyclopedia"],
    summary="获取心理百科条目"
)
async def get_encyclopedia_entries(
    category: Optional[str] = Query(None, description="按分类过滤条目"),
    random_tip: Optional[bool] = Query(False, description="是否从'心理小贴士'分类中随机获取一条") # 参数名改为 random_tip
    # 可以添加分页参数: page: int = Query(1, ge=1), size: int = Query(10, ge=1, le=100)
):
    """
    获取心理百科条目。
    - 提供 `category` 参数以按分类过滤。
    - 提供 `random_tip=true` 以获取一条随机的“心理小贴士”。(此时忽略 category 参数)
    """
    all_entries = get_all_entries()
    if not all_entries:
        if random_tip:
             # 返回一个默认的 EncyclopediaEntry 结构
             return EntriesResponse(entries=[EncyclopediaEntry(category="心理小贴士", title="提示", content="暂无可用小贴士。")])
        else:
             return EntriesResponse(entries=[]) # 返回空列表

    target_entries = all_entries

    # --- 处理随机小贴士逻辑 ---
    if random_tip:
        tips_category_name = "心理小贴士" # 明确指定分类名称
        tip_entries = [entry for entry in all_entries if entry.get("category") == tips_category_name]

        if not tip_entries:
            logger.warning("请求随机小贴士，但 '心理小贴士' 分类下没有条目。")
            # 返回一个默认的 EncyclopediaEntry 结构，放入列表中
            return EntriesResponse(entries=[EncyclopediaEntry(category=tips_category_name, title="提示", content="暂无可用小贴士。")])

        selected_entry_dict = random.choice(tip_entries)
        # 将选中的字典包装在列表中返回，以匹配 EntriesResponse
        selected_entry_model = EncyclopediaEntry(**selected_entry_dict)
        return EntriesResponse(entries=[selected_entry_model]) # 返回包含单个随机条目的列表

    # --- 处理按分类过滤逻辑 (如果不是请求随机小贴士) ---
    if category:
        target_entries = [entry for entry in all_entries if entry.get("category") == category]
        if not target_entries:
             # 如果指定了分类但找不到，返回空列表是合理的
             logger.info(f"请求分类 '{category}'，但未找到条目。")
             return EntriesResponse(entries=[])

    # --- (可选) 实现分页 ---
    # total = len(target_entries)
    # start = (page - 1) * size
    # end = start + size
    # paged_entries_dicts = target_entries[start:end]

    # --- 将字典列表转换为 Pydantic 模型列表 ---
    # 如果没有分页，直接使用 target_entries
    try:
        result_entries = [EncyclopediaEntry(**entry_dict) for entry_dict in target_entries]
    except Exception as e:
         logger.error(f"将百科条目字典转换为 Pydantic 模型时出错: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail="处理百科条目数据时出错")

    # 返回结果
    return EntriesResponse(entries=result_entries) # , total=total, page=page, size=size) 如果实现分页

--------------------------------------------------
文件路径: app\routers\reports.py
--------------------------------------------------
# app/routers/reports.py
import logging
import json
import asyncio
from fastapi import APIRouter, HTTPException, Depends, status
from sqlalchemy.ext.asyncio import AsyncSession
from pydantic import ValidationError # 确保导入 ValidationError

# --- Core App Imports ---
from app.core.config import settings
from app.schemas.report import ReportResponse, ReportData, ReportStatusResponse

# --- Authentication & Database Imports ---
from app.core.deps import get_current_active_user, get_db
from app import models
from app import crud
from app.models.assessment import STATUS_COMPLETE, STATUS_FAILED, STATUS_PENDING, STATUS_PROCESSING

logger = logging.getLogger(settings.APP_NAME)
router = APIRouter()

@router.get(
    "/{assessment_id}",
    response_model=ReportResponse,
    summary="获取指定评估的分析报告 (仅在完成后)",
    tags=["Reports"],
    responses={
        status.HTTP_200_OK: {"description": "成功获取报告，或报告尚未完成/失败"},
        status.HTTP_404_NOT_FOUND: {"description": "评估记录未找到"},
        status.HTTP_403_FORBIDDEN: {"description": "无权访问此报告"},
        status.HTTP_500_INTERNAL_SERVER_ERROR: {"description": "服务器内部错误"},
    }
)
async def get_report_by_id(
    assessment_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: models.User = Depends(get_current_active_user)
):
    """
    根据评估 ID 获取单个评估报告的详细信息。
    """
    logger.info(f"[Reports Router - Full] User {current_user.username} requesting full report for ID: {assessment_id}")

    try:
        logger.debug(f"[Reports Router - Full] Fetching assessment ID {assessment_id} using crud.assessment.get")
        assessment: models.Assessment | None = await crud.assessment.get(db=db, id=assessment_id)

        if not assessment:
            logger.warning(f"[Reports Router - Full] Assessment ID {assessment_id} NOT FOUND. Raising 404.")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"评估 ID {assessment_id} 不存在。",
            )

        logger.info(f"[Reports Router - Full] Found assessment ID {assessment_id}. Current status: '{assessment.status}'.")

        # --- 权限检查 (根据需要启用) ---
        # if not current_user.is_superuser and assessment.submitter_id != current_user.id:
        #     logger.warning(...)
        #     raise HTTPException(...)

        # --- 检查状态 ---
        if assessment.status != STATUS_COMPLETE:
            status_message_map = {
                STATUS_PENDING: "报告正在等待处理。",
                STATUS_PROCESSING: "报告正在生成中，请稍后。",
                STATUS_FAILED: "报告生成失败。",
            }
            message = status_message_map.get(assessment.status, "报告处于未知或非完成状态。")
            logger.info(f"[Reports Router - Full] Assessment ID {assessment_id} status is '{assessment.status}'. Returning message: '{message}'")
            return ReportResponse(report=None, message=message)

        # --- 状态是 COMPLETE，检查报告文本 ---
        logger.debug(f"[Reports Router - Full] Status is COMPLETE for ID {assessment_id}. Checking report_text.")
        if not assessment.report_text or assessment.report_text.strip() == "":
            logger.error(f"[Reports Router - Full] CRITICAL: Assessment ID {assessment_id} has status COMPLETE but report_text is empty!")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="报告状态与内容不一致，请联系管理员。"
            )

        # --- 状态是 COMPLETE 且文本存在，准备返回数据 ---
        logger.info(f"[Reports Router - Full] Report text found for completed assessment ID {assessment_id}. Preparing response data.")

        # +++ 修改点：在验证前准备数据，并解析 JSON +++
        data_for_validation = assessment.__dict__.copy() # 复制模型属性到字典
        parsed_q_data = None # 初始化为 None
        q_data_str = data_for_validation.get('questionnaire_data')

        if isinstance(q_data_str, str):
            logger.debug("[Reports Router - Full] Attempting to parse questionnaire_data JSON string before validation.")
            try:
                parsed_q_data = json.loads(q_data_str)
                if not isinstance(parsed_q_data, dict): # 确保解析结果是字典
                    logger.warning(f"[Reports Router - Full] Parsed questionnaire_data is not a dictionary (type: {type(parsed_q_data)}). Setting to None.")
                    parsed_q_data = None
                else:
                    logger.debug("[Reports Router - Full] Successfully parsed questionnaire_data into a dictionary.")
            except json.JSONDecodeError:
                logger.error(f"[Reports Router - Full] Failed to decode questionnaire_data JSON string: {q_data_str[:100]}... Setting to None.")
                parsed_q_data = None # 解析失败则设为 None
        elif q_data_str is not None:
            # 如果数据库中不是字符串也不是 None，记录警告
            logger.warning(f"[Reports Router - Full] questionnaire_data from DB is not a string or None (type: {type(q_data_str)}). Attempting to use as is if schema allows, otherwise will be None.")
            # 尝试直接使用，如果 Pydantic 允许的话，否则保持 None
            parsed_q_data = q_data_str if isinstance(q_data_str, dict) else None

        # 将解析后的（或原始的 None）值放回待验证的数据字典中
        data_for_validation['questionnaire_data'] = parsed_q_data
        # +++ 修改结束 +++

        try:
            # --- 修改点：使用准备好的字典进行验证 ---
            # 不再需要 from_attributes=True，因为我们传入的是字典
            report_data = ReportData.model_validate(data_for_validation)
            # --- 修改结束 ---

            # --- 移除这里冗余的 questionnaire_data 解析逻辑 ---
            # if isinstance(assessment.questionnaire_data, str):
            #    ... (这部分逻辑已移到 model_validate 之前) ...
            # --- 移除结束 ---

            logger.info(f"[Reports Router - Full] Successfully built ReportData for ID {assessment_id}.")
            return ReportResponse(report=report_data, message="报告获取成功。")

        except ValidationError as pydantic_err:
            # 捕获 Pydantic 验证错误 (理论上现在不应该因为 questionnaire_data 类型错误了，但可能还有其他字段问题)
            logger.error(f"[Reports Router - Full] Pydantic validation error creating ReportData for ID {assessment_id}: {pydantic_err}", exc_info=True)
            # 记录导致错误的原始数据 (字典形式)
            logger.debug(f"[Reports Router - Full] Data causing validation error: {data_for_validation}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="处理报告数据格式时出错。"
            )

    except HTTPException as http_exc:
         logger.warning(f"[Reports Router - Full] Raising known HTTPException for ID {assessment_id}: Status={http_exc.status_code}, Detail={http_exc.detail}")
         raise http_exc
    except Exception as e:
        logger.exception(f"[Reports Router - Full] Unexpected error processing request for ID {assessment_id}. User: {current_user.username}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="获取报告时发生未知错误。",
        )

# --- get_report_status 函数 (保持不变) ---
@router.get(
    "/{assessment_id}/status",
    response_model=ReportStatusResponse,
    summary="获取指定评估的处理状态",
    tags=["Reports"],
    responses={
        status.HTTP_200_OK: {"description": "成功获取状态"},
        status.HTTP_404_NOT_FOUND: {"description": "评估记录未找到"},
        status.HTTP_403_FORBIDDEN: {"description": "无权访问此评估状态"},
        status.HTTP_500_INTERNAL_SERVER_ERROR: {"description": "服务器内部错误"}
    }
)
async def get_report_status(
    assessment_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: models.User = Depends(get_current_active_user)
):
    """获取指定评估 ID 的当前处理状态。包含短暂重试以处理可见性延迟。"""
    logger.info(f"[Reports Router - Status] User {current_user.username} requesting status for ID: {assessment_id}")
    max_retries = 2
    retry_delay = 0.5
    for attempt in range(max_retries):
        try:
            logger.debug(f"[Reports Router - Status] Attempt {attempt + 1}/{max_retries}: Fetching assessment ID {assessment_id}")
            assessment = await crud.assessment.get(db=db, id=assessment_id)
            if assessment:
                logger.info(f"[Reports Router - Status] Attempt {attempt + 1}: Found status '{assessment.status}' for ID: {assessment_id}")
                return ReportStatusResponse(status=assessment.status)
            else:
                logger.warning(f"[Reports Router - Status] Attempt {attempt + 1}: crud.assessment.get returned None for ID {assessment_id}.")
                if attempt < max_retries - 1:
                    logger.info(f"[Reports Router - Status] Waiting {retry_delay}s before retrying fetch for ID {assessment_id}...")
                    await asyncio.sleep(retry_delay)
                else:
                    logger.error(f"[Reports Router - Status] All {max_retries} attempts failed to find assessment ID {assessment_id}. Raising 404.")
                    raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="评估记录未找到或暂不可见")
        except HTTPException as http_exc:
             if http_exc.status_code == 404 and attempt == max_retries - 1:
                 raise http_exc
             if http_exc.status_code == 403:
                 raise http_exc
             logger.warning(f"[Reports Router - Status] Caught handled HTTPException during attempt {attempt + 1}: {http_exc.status_code}, Detail: {http_exc.detail}")
             if attempt == max_retries - 1: raise http_exc
        except Exception as e:
            logger.exception(f"[Reports Router - Status] Unexpected error during attempt {attempt + 1} for ID {assessment_id}", exc_info=True)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="获取报告状态时发生内部错误"
            )
    logger.error(f"[Reports Router - Status] Reached end of function unexpectedly after retries for ID {assessment_id}.")
    raise HTTPException(status_code=500, detail="处理状态请求时发生意外流程错误。")

--------------------------------------------------
文件路径: app\routers\scales.py
--------------------------------------------------
# app/routers/scales.py
import logging
import json # Ensure json is imported
from fastapi import APIRouter, HTTPException, Depends

# Import Pydantic models and other necessary components
from app.schemas.scale import ScaleInfo, ScaleQuestion, ScaleOption, AvailableScalesResponse, ScaleQuestionsResponse
from src.data_handler import DataHandler
from app.core.config import settings

# Get the logger instance configured in main.py
# Ensure the logger name matches the one used in main.py's setup_logging call
logger = logging.getLogger(settings.APP_NAME)

# Create an APIRouter instance
router = APIRouter()

# --- Dependency Injection ---
# def get_data_handler():
#     """Dependency function to get a DataHandler instance."""
#     try:
#         # Use the database path from the application settings
#         return DataHandler(db_path=settings.DB_PATH)
#     except Exception as e:
#         # Log the error and raise an HTTP exception if DataHandler fails to initialize
#         logger.error(f"Failed to initialize DataHandler: {e}", exc_info=True)
#         raise HTTPException(status_code=500, detail="Database handler initialization failed.")
def get_data_handler():
    try:
        # <<< FIX: 使用 DB_PATH_SQLITE >>>
        return DataHandler(db_path=settings.DB_PATH_SQLITE)
    except Exception as e:
        logger.error(f"Failed to initialize DataHandler: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"数据库处理程序初始化失败: {e}")

# --- API Endpoints ---

@router.get(
    "/scales",
    response_model=AvailableScalesResponse, # Use keyword argument for response_model
    tags=["Scales"]                          # Use keyword argument for tags
)
async def get_available_scales(dh: DataHandler = Depends(get_data_handler)):
    """
    Retrieves a list of all available scale types (code and name).
    """
    logger.info("Request received for available scales.")
    try:
        # Fetch scale types from the data handler
        scales = dh.get_all_scale_types() # This should return List[Dict] e.g. [{'code': 'SAS', 'name': '...'}, ...]

        # Convert the list of dictionaries to a list of Pydantic models
        scale_infos = [ScaleInfo(code=s["code"], name=s["name"]) for s in scales]

        # Return the response using the Pydantic response model
        return AvailableScalesResponse(scales=scale_infos)
    except Exception as e:
        # Log any unexpected errors during the process
        logger.error(f"Error fetching available scales: {e}", exc_info=True)
        # Raise a generic 500 error to the client
        raise HTTPException(status_code=500, detail="Failed to fetch scale types from database.")


@router.get(
    "/scales/{scale_code}/questions",
    response_model=ScaleQuestionsResponse, # Use keyword argument
    tags=["Scales"]                          # Use keyword argument
)
async def get_scale_questions(scale_code: str, dh: DataHandler = Depends(get_data_handler)):
    """
    Retrieves all questions for a specific scale based on its code.
    """
    logger.info(f"Request received for questions of scale: {scale_code}")
    try:
        # Load question data using the data handler
        # This method should return List[Dict] or None
        questions_data = dh.load_questions_by_type(scale_code)

        # Handle case where no questions are found for the given scale code
        if questions_data is None:
            logger.warning(f"No questions found for scale code: {scale_code}")
            raise HTTPException(status_code=404, detail=f"Scale with code '{scale_code}' not found or has no questions.")

        questions_list = []
        # Iterate through the raw question data from the database
        for q_data in questions_data:
            # --- Data Validation and Transformation ---
            # Ensure the basic structure of question data is present
            if not all(k in q_data for k in ('number', 'text', 'options')):
                logger.warning(f"Skipping question data due to missing keys: {q_data}")
                continue # Skip this malformed question data

            options_list = []
            options_data_from_db = q_data.get('options', [])

            # Ensure options data is a list before processing
            if not isinstance(options_data_from_db, list):
                logger.warning(f"Options data for Q{q_data['number']} is not a list, skipping options. Data: {options_data_from_db}")
            else:
                # Iterate through options for the current question
                for opt in options_data_from_db:
                    # Ensure option structure is correct
                    if not isinstance(opt, dict) or not all(k in opt for k in ('text', 'score')):
                        logger.warning(f"Skipping invalid option data for Q{q_data['number']}: {opt}")
                        continue # Skip malformed option data

                    text_val = opt['text']
                    score_val_raw = opt['score']
                    score_val_numeric = None

                    # Explicitly convert score to a numeric type (int or float)
                    try:
                        if isinstance(score_val_raw, (int, float)):
                            score_val_numeric = score_val_raw
                        elif isinstance(score_val_raw, str):
                            # Attempt conversion from string
                            try:
                                score_float = float(score_val_raw)
                                score_val_numeric = int(score_float) if score_float.is_integer() else score_float
                            except ValueError:
                                logger.error(f"Could not convert score string '{score_val_raw}' to number for Q{q_data['number']}, option '{text_val}'. Skipping option.")
                                continue # Skip this option if conversion fails
                        else:
                            # Handle unexpected score types
                            logger.warning(f"Unexpected type for score ({type(score_val_raw)}) for Q{q_data['number']}, option '{text_val}'. Using 0 as fallback.")
                            score_val_numeric = 0

                    except Exception as conv_err:
                         logger.error(f"Error converting score '{score_val_raw}' for Q{q_data['number']}, option '{text_val}': {conv_err}", exc_info=True)
                         continue # Skip option on unexpected conversion error

                    # Only create ScaleOption if score conversion was successful
                    if score_val_numeric is not None:
                        try:
                            # Create Pydantic model instance for the option
                            options_list.append(ScaleOption(text=str(text_val), score=score_val_numeric))
                        except Exception as pydantic_option_err:
                            # Catch potential errors during Pydantic model instantiation
                            logger.error(f"Pydantic error creating ScaleOption for Q{q_data['number']}, option '{text_val}': {pydantic_option_err}", exc_info=True)
                            continue # Skip option if it fails validation

            # Create Pydantic model instance for the question
            try:
                questions_list.append(ScaleQuestion(
                    number=int(q_data['number']), # Ensure number is integer
                    text=str(q_data['text']),    # Ensure text is string
                    options=options_list
                ))
            except Exception as pydantic_q_err:
                # Catch potential errors during Pydantic model instantiation
                logger.error(f"Pydantic error creating ScaleQuestion for Q{q_data.get('number', 'N/A')}: {pydantic_q_err}", exc_info=True)
                continue # Skip this question if it fails validation

        # Log successful processing and return the validated response
        logger.info(f"Successfully processed {len(questions_list)} questions for scale {scale_code}.")
        return ScaleQuestionsResponse(questions=questions_list)

    except HTTPException as http_exc:
        # Re-raise known HTTP exceptions (like 404)
        raise http_exc
    except Exception as e:
        # Log unexpected errors and return a generic 500 response
        logger.error(f"Unexpected error fetching/processing questions for scale {scale_code}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to fetch questions for scale {scale_code}.")

--------------------------------------------------
文件路径: app\routers\sse.py
--------------------------------------------------
# app/routers/sse.py
import asyncio
import json
import logging
from fastapi import APIRouter, Depends, HTTPException, Request, status
from sse_starlette.sse import EventSourceResponse
import redis.asyncio as redis # 导入异步 redis 客户端

from app.core.config import settings
from app.core.deps import get_current_active_user # 保护 SSE 端点
from app import models

logger = logging.getLogger(settings.APP_NAME)
router = APIRouter()

# 全局（或更健壮的作用域）Redis 连接池
# 注意：在生产环境中，连接池管理应更完善
redis_pool = None

async def get_redis_pool():
    """获取或创建 Redis 连接池"""
    global redis_pool
    if redis_pool is None:
        try:
            # 使用 from_url 创建连接池
            redis_pool = redis.ConnectionPool.from_url(
                settings.REDIS_URL,
                decode_responses=True, # 自动解码响应为字符串
                max_connections=20     # 示例连接池大小
            )
            logger.info(f"成功创建 Redis 连接池，目标: {settings.REDIS_URL}")
        except Exception as e:
            logger.critical(f"创建 Redis 连接池失败: {e}", exc_info=True)
            raise HTTPException(status_code=503, detail="无法连接到实时通知服务。")
    return redis_pool

@router.get(
    "/sse/report-status/{submission_id}",
    tags=["SSE"],
    summary="订阅报告生成状态更新"
)
async def report_status_stream(
    submission_id: int,
    request: Request, # 用于检测客户端断开连接
    current_user: models.User = Depends(get_current_active_user), # 保护端点
    pool: redis.ConnectionPool = Depends(get_redis_pool) # 注入连接池
):
    """
    为指定提交 ID 创建 SSE 连接，等待报告就绪事件。
    需要用户已登录。
    """
    logger.info(f"用户 '{current_user.username}' (ID: {current_user.id}) 订阅评估 ID: {submission_id} 的状态更新。")

    channel_name = f"report-ready:{submission_id}"

    async def event_generator():
        # 从连接池获取单个连接
        async with redis.Redis(connection_pool=pool) as redis_client:
            # 创建 PubSub 客户端
            pubsub = redis_client.pubsub(ignore_subscribe_messages=True) # 忽略订阅确认消息
            try:
                # 订阅特定频道
                await pubsub.subscribe(channel_name)
                logger.info(f"SSE: 已订阅频道 '{channel_name}'")

                while True:
                    # 检查客户端是否已断开连接
                    if await request.is_disconnected():
                        logger.info(f"SSE: 客户端断开连接，取消订阅频道 '{channel_name}'。")
                        break

                    # 等待来自 Redis 的消息 (带超时)
                    try:
                        # 使用 asyncio.timeout 防止永久阻塞
                        async with asyncio.timeout(60): # 例如，每 60 秒检查一次连接
                            message = await pubsub.get_message() # timeout=None 已移除，由 asyncio.timeout 控制
                        if message:
                            logger.info(f"SSE: 从频道 '{channel_name}' 收到消息: {message}")
                            data = message.get("data")
                            if data:
                                try:
                                    # 假设消息是 JSON 字符串 {"status": "success"} 或 {"status": "failed", "error": "..."}
                                    payload = json.loads(data)
                                    if payload.get("status") == "success":
                                        logger.info(f"SSE: 报告 ID {submission_id} 已就绪，发送 'report_ready' 事件。")
                                        yield json.dumps({"event": "report_ready", "data": json.dumps({"submission_id": submission_id})})
                                        break # 报告就绪，结束此 SSE 流
                                    elif payload.get("status") == "failed":
                                         logger.warning(f"SSE: 报告 ID {submission_id} 生成失败，发送 'report_failed' 事件。 Error: {payload.get('error')}")
                                         yield json.dumps({"event": "report_failed", "data": json.dumps({"submission_id": submission_id, "error": payload.get('error', '未知错误')})})
                                         break # 任务失败，也结束流
                                    else:
                                         logger.warning(f"SSE: 从频道 '{channel_name}' 收到未知状态的消息: {payload}")
                                except json.JSONDecodeError:
                                     logger.error(f"SSE: 无法解码来自频道 '{channel_name}' 的消息: {data}")
                                except Exception as parse_err:
                                     logger.error(f"SSE: 解析消息时出错 ('{channel_name}'): {parse_err}", exc_info=True)
                        # else:
                            # 没有消息，继续等待 (asyncio.timeout 会处理超时)
                            # logger.debug(f"SSE: 频道 '{channel_name}' 无新消息。")
                            # 可以发送 keep-alive 注释
                            # yield ":" # 发送 SSE 注释以保持连接活跃

                    except asyncio.TimeoutError:
                         # 超时期间没有收到消息，发送 keep-alive 并继续循环
                         logger.debug(f"SSE: 频道 '{channel_name}' 等待超时，发送 keep-alive。")
                         yield ":"
                    except Exception as e:
                         logger.error(f"SSE: 处理频道 '{channel_name}' 消息时发生错误: {e}", exc_info=True)
                         # 发生错误时也考虑结束流，防止无限循环
                         break

            finally:
                # 确保取消订阅
                try:
                    if pubsub.connection: # 检查连接是否存在
                         await pubsub.unsubscribe(channel_name)
                         # 关闭 pubsub 连接 (通常由 Redis 客户端的 __aexit__ 处理)
                         # await pubsub.close()
                         logger.info(f"SSE: 已取消订阅频道 '{channel_name}'")
                except Exception as unsub_err:
                     logger.warning(f"SSE: 取消订阅频道 '{channel_name}' 时出错: {unsub_err}")

    # 返回 EventSourceResponse
    return EventSourceResponse(event_generator())

--------------------------------------------------
文件路径: app\routers\__init__.py
--------------------------------------------------


--------------------------------------------------
文件路径: app\schemas\assessment.py
--------------------------------------------------
#评估提交相关
# app/schemas/assessment.py
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional
from datetime import datetime

# --- 用于 POST /api/assessments/submit 的请求体 (部分数据将来自 Form) ---
# Pydantic 模型通常用于 JSON body，但 FastAPI 也能从 Form 字段映射
# 这里定义基础信息字段，方便验证和文档化，实际接收用 Form(...)
class BasicInfoSubmit(BaseModel):
    name: str = Field(..., description="姓名")
    gender: str = Field(..., description="性别")
    id_card: Optional[str] = Field(None, description="身份证号")
    age: int = Field(..., gt=0, description="年龄")
    occupation: Optional[str] = Field(None, description="职业")
    case_name: Optional[str] = Field(None, description="案件名称")
    case_type: Optional[str] = Field(None, description="案件类型")
    identity_type: Optional[str] = Field(None, description="人员身份")
    person_type: Optional[str] = Field(None, description="人员类型")
    marital_status: Optional[str] = Field(None, description="婚姻状况")
    children_info: Optional[str] = Field(None, description="子女情况")
    criminal_record: Optional[int] = Field(0, ge=0, le=1, description="有无犯罪前科 (0:无, 1:有)")
    health_status: Optional[str] = Field(None, description="健康情况")
    phone_number: Optional[str] = Field(None, description="手机号")
    domicile: Optional[str] = Field(None, description="归属地")
    # 注意：scale_type 和 scale_answers 会作为独立的 Form 字段传入

# --- 用于 POST /api/assessments/submit 的响应 ---
class AssessmentSubmitResponse(BaseModel):
    status: str = "success" # "success" or "error"
    message: str
    submission_id: Optional[int] = None # 成功时返回 ID
    
class AssessmentSummary(BaseModel):
    """用于在列表中显示的评估摘要信息"""
    id: int
    subject_name: Optional[str] = None
    questionnaire_type: Optional[str] = None
    status: str
    created_at: datetime
    submitter_id: Optional[int] = None # 关联提交者ID

    class Config:
        from_attributes = True

--------------------------------------------------
文件路径: app\schemas\attribute.py
--------------------------------------------------
# FILE: app/schemas/attribute.py
from pydantic import BaseModel, Field
from typing import Optional

class AttributeBase(BaseModel):
    """属性的基础模型，包含通用字段"""
    name: str = Field(..., max_length=100, description="属性名称 (e.g., 焦虑, 抑郁)")
    description: Optional[str] = Field(None, description="属性的详细描述")
    category: Optional[str] = Field(None, max_length=50, description="属性分类 (e.g., 情绪状态, 行为风险)")

class AttributeCreate(AttributeBase):
    """创建新属性时使用的模型"""
    pass # 继承 Base 即可

class AttributeUpdate(AttributeBase):
    """更新属性时使用的模型 (允许部分更新)"""
    name: Optional[str] = Field(None, max_length=100) # 更新时名称也可能可选

class AttributeRead(AttributeBase):
    """从数据库读取属性时使用的模型，包含 ID"""
    id: int

    class Config:
        from_attributes = True # Pydantic v2: orm_mode = True

--------------------------------------------------
文件路径: app\schemas\encyclopedia.py
--------------------------------------------------
# 文件: app/schemas/encyclopedia.py
from pydantic import BaseModel
from typing import List, Optional

class EncyclopediaEntry(BaseModel):
    """单个百科条目的结构"""
    category: str
    title: str
    content: str

class CategoriesResponse(BaseModel):
    """获取分类列表的响应"""
    categories: List[str]

class EntriesResponse(BaseModel):
    """获取条目列表的响应"""
    entries: List[EncyclopediaEntry]
    # 可以添加分页信息 (如果需要)
    # total: Optional[int] = None
    # page: Optional[int] = None
    # size: Optional[int] = None

# 可以复用 EncyclopediaEntry 作为随机条目的响应，或者定义一个更简单的
# class RandomTipResponse(BaseModel):
#     tip: str

--------------------------------------------------
文件路径: app\schemas\guidance.py
--------------------------------------------------
# FILE: app/schemas/guidance.py (新建)
from pydantic import BaseModel
from typing import Optional
from .report import ReportData # 导入报告数据模型

class GuidanceResponse(BaseModel):
    """指导方案 API 的响应模型"""
    report: Optional[ReportData] = Field(None, description="相关的原始报告摘要 (如果找到)") # 包含相关的报告详情
    guidance: Optional[str] = Field(None, description="生成的指导方案文本")    # 生成的指导方案文本
    error: Optional[str] = Field(None, description="如果生成过程中发生错误")       # 如果发生错误

--------------------------------------------------
文件路径: app\schemas\interrogation.py
--------------------------------------------------
# FILE: app/schemas/interrogation.py (新建)
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
from datetime import datetime

class InterrogationBasicInfo(BaseModel):
    """审讯笔录 - 开始时输入的基础信息"""
    person_name: str = Field(..., description="被讯问人姓名")
    person_gender: Optional[str] = Field(None, description="性别")
    person_id_type_number: Optional[str] = Field(None, description="身份证号")
    person_address: Optional[str] = Field(None, description="家庭住址")
    person_contact: Optional[str] = Field(None, description="联系方式")
    # 可以从前端原型或模板的 interrogation_template.json 中提取更多所需字段
    person_age: Optional[str] = Field(None, description="年龄") # 可能是字符串或数字
    person_dob: Optional[str] = Field(None, description="出生日期")
    person_hukou: Optional[str] = Field(None, description="户籍所在地")
    case_type: Optional[str] = Field(None, description="涉及案件类型") # 例如，从基础信息添加

class InterrogationQAInput(BaseModel):
    """单个问答对 (用于 AI 建议输入和最终保存)"""
    q: str = Field(..., description="问题文本")
    a: str = Field(..., description="回答文本")

class InterrogationRecordCreate(BaseModel): # 用于创建时内部传递
    """创建审讯记录的内部数据模型"""
    interrogator_id: int
    basic_info: Dict[str, Any] # 存储 JSON
    qas: List[Dict[str, str]]  # 存储 Q&A 列表的 JSON

class InterrogationRecordUpdate(BaseModel): # 用于 PUT 更新
    """更新审讯记录的数据模型"""
    basic_info: Optional[Dict[str, Any]] = Field(None, description="更新后的基本信息 (可选)") # 允许部分更新基本信息
    qas: Optional[List[InterrogationQAInput]] = Field(None, description="完整的问答列表 (覆盖旧的)") # 接收完整的问答列表进行覆盖
    status: Optional[str] = Field(None, description="状态 (e.g., 'ongoing', 'completed', 'cancelled')") # 允许更新状态
    full_text: Optional[str] = Field(None, description="最终生成的完整笔录文本 (可选)") # 添加 full_text

class InterrogationRecordRead(BaseModel): # 用于 GET 读取和 POST/PUT 响应
    """读取或响应审讯记录的数据模型"""
    id: int
    interrogator_id: int
    basic_info: Optional[Dict[str, Any]] = Field(None, description="基础信息")
    qas: Optional[List[InterrogationQAInput]] = Field(None, description="问答对列表") # 返回解析后的列表
    status: str = Field(..., description="记录状态")
    full_text: Optional[str] = Field(None, description="完整笔录文本") # 添加 full_text
    created_at: datetime = Field(..., description="创建时间")
    updated_at: datetime = Field(..., description="最后更新时间")

    class Config:
        from_attributes = True # 从 ORM 对象加载

--------------------------------------------------
文件路径: app\schemas\report.py
--------------------------------------------------
# FILE: app/schemas/report.py (修改后，添加 attributes 字段)
from pydantic import BaseModel, Field # 确保 Field 已导入
from typing import Optional, Dict, Any, List # <--- 导入 List
from datetime import datetime

# +++ 新增：定义用于在报告中展示的属性信息 Schema +++
# （理想情况下，这个模型应该在 app/schemas/attribute.py 中定义并从那里导入）
class AttributeRead(BaseModel):
    id: int
    name: str = Field(..., description="属性名称，例如'焦虑'、'高风险'")
    # 可以选择性地包含 description 或 category
    # description: Optional[str] = None
    # category: Optional[str] = None

    class Config:
        from_attributes = True # 允许从 ORM 对象属性创建
# +++ 属性 Schema 定义结束 +++


class ReportData(BaseModel):
    """报告详情的响应模型 (包含关联属性)"""
    id: int
    image_path: Optional[str] = None
    subject_name: Optional[str] = None
    age: Optional[int] = None
    gender: Optional[str] = None
    questionnaire_type: Optional[str] = None
    questionnaire_data: Optional[Dict[str, Any]] = None # 解析后的 JSON
    report_text: Optional[str] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    id_card: Optional[str] = None
    occupation: Optional[str] = None
    case_name: Optional[str] = None
    case_type: Optional[str] = None
    identity_type: Optional[str] = None
    person_type: Optional[str] = None
    marital_status: Optional[str] = None
    children_info: Optional[str] = None
    criminal_record: Optional[int] = None
    health_status: Optional[str] = None
    phone_number: Optional[str] = None
    domicile: Optional[str] = None
    status: Optional[str] = None # 报告状态

    # +++ 新增：关联的属性列表 +++
    # 这个字段将包含与此评估关联的所有属性对象的信息
    # 默认值为 None 或空列表 [] 都可以，取决于你的偏好
    attributes: Optional[List[AttributeRead]] = Field(default_factory=list, description="与此评估关联的属性标签列表")
    # +++ 属性字段添加结束 +++

    class Config: # Pydantic V2 使用 ConfigDict 或直接设置
        from_attributes = True # 保持不变，以便从 Assessment ORM 对象加载数据
    # ... 可以添加未来需要的其他报告字段 ...

class ReportResponse(BaseModel):
    """获取报告的 API 响应"""
    report: Optional[ReportData] = None
    message: Optional[str] = None # 用于传递状态信息（如处理中）或错误
    # error 字段可以移除，统一使用 message 字段

# 状态响应模型保持不变
class ReportStatusResponse(BaseModel):
    status: str

--------------------------------------------------
文件路径: app\schemas\scale.py
--------------------------------------------------
# 量表相关
# app/schemas/scale.py
from pydantic import BaseModel
from typing import List, Dict, Any, Optional

class ScaleOption(BaseModel):
    """量表问题的选项"""
    text: str
    score: int | float # 分数可能是整数或浮点数

class ScaleQuestion(BaseModel):
    """量表问题结构"""
    number: int
    text: str
    options: List[ScaleOption]

class ScaleInfo(BaseModel):
    """量表基本信息 (用于列表显示)"""
    code: str # 量表代码 (e.g., 'SAS', 'Personality')
    name: str # 量表显示名称

# --- 用于 /api/scales/{scale_code}/questions 的响应 ---
class ScaleQuestionsResponse(BaseModel):
    questions: Optional[List[ScaleQuestion]] = None
    error: Optional[str] = None # 如果找不到问题，可以返回错误信息

# --- 用于 /api/scales 的响应 ---
class AvailableScalesResponse(BaseModel):
    scales: List[ScaleInfo]

--------------------------------------------------
文件路径: app\schemas\stats.py
--------------------------------------------------
# FILE: app/schemas/stats.py (新建)
from pydantic import BaseModel
from typing import List, Dict, Any

class ChartData(BaseModel):
    """用于图表展示的数据结构"""
    labels: List[str]
    values: List[int | float] # 值可能是整数或浮点数

class DemographicsStats(BaseModel):
    """人口统计数据的响应模型"""
    ageData: ChartData
    genderData: ChartData

# 可以为其他统计数据添加更多模型，例如：
# class ScaleUsageStats(BaseModel):
#     scaleData: ChartData

--------------------------------------------------
文件路径: app\schemas\token.py
--------------------------------------------------
# app/schemas/token.py
from pydantic import BaseModel
from typing import Optional

class Token(BaseModel):
    """响应模型：登录成功后返回给客户端的令牌"""
    access_token: str
    token_type: str = "bearer" # 标准 OAuth2 类型

class TokenData(BaseModel):
    """内部模型：JWT 令牌载荷 (Payload) 的数据结构"""
    username: Optional[str] = None
    # 你未来可以在这里添加其他需要存储在令牌中的信息，
    # 例如用户 ID 或角色/权限 (scopes)
    # scopes: List[str] = []

--------------------------------------------------
文件路径: app\schemas\user.py
--------------------------------------------------
# FILE: app/schemas/user.py (添加 UserUpdateAdmin 和 UserListResponse)
from pydantic import BaseModel, EmailStr, Field
from typing import Optional, List # 添加 List 导入

class UserBase(BaseModel):
    username: str = Field(..., min_length=3, max_length=50, description="用户名")
    email: Optional[EmailStr] = Field(None, description="邮箱")
    full_name: Optional[str] = Field(None, max_length=100, description="全名")
    # 允许创建/更新时指定，但如果未指定，CRUD 层会使用默认值
    is_active: Optional[bool] = Field(True, description="是否激活")
    is_superuser: Optional[bool] = Field(False, description="是否超级用户")

class UserCreate(UserBase):
    password: str = Field(..., min_length=6, description="登录密码 (明文)")

class UserUpdate(UserBase):
    password: Optional[str] = Field(None, min_length=6, description="新密码 (可选，用于用户自我更新)")
    # 用户自我更新时不应能修改 is_active 和 is_superuser
    is_active: Optional[bool] = Field(None, description="不允许用户自我修改") # 阻止用户自己设置
    is_superuser: Optional[bool] = Field(None, description="不允许用户自我修改") # 阻止用户自己设置


# +++ 新增：管理员更新用户时使用的 Schema +++
class UserUpdateAdmin(BaseModel):
    """管理员更新用户信息时使用，允许修改激活和超管状态，密码可选"""
    email: Optional[EmailStr] = Field(None, description="新邮箱 (可选)")
    full_name: Optional[str] = Field(None, max_length=100, description="新全名 (可选)")
    is_active: Optional[bool] = Field(None, description="设置激活状态 (可选)") # 允许管理员修改
    is_superuser: Optional[bool] = Field(None, description="设置超级用户状态 (可选)") # 允许管理员修改
    password: Optional[str] = Field(None, min_length=6, description="设置新密码 (可选)")

class UserInDBBase(UserBase):
    id: int
    class Config:
        from_attributes = True # Pydantic V2 风格

class User(UserInDBBase):
    """用于 API 响应的用户模型 (不含密码)"""
    pass

# +++ 新增：用户列表响应 Schema +++
class UserListResponse(BaseModel):
    """用户列表接口的响应结构"""
    total: int = Field(..., description="符合条件的用户总数")
    users: List[User] = Field(..., description="当前页的用户列表")

# 内部数据库表示（通常不在 API 中直接暴露）
class UserInDB(UserInDBBase):
    hashed_password: str

--------------------------------------------------
文件路径: app\schemas\__init__.py
--------------------------------------------------
# FILE: app/schemas/__init__.py (更新后，包含 Attribute 相关 Schema)

# --- 基础模型 ---
# (如果你的 Base 模型在 schema 中定义，则保留，否则通常在 models 中)
# from .base import BaseSchema # 示例，如果存在

# --- 用户相关 ---
from .user import (
    User,
    UserCreate,
    UserUpdate,
    UserUpdateAdmin, # 管理员更新 Schema
    UserListResponse, # 用户列表响应 Schema
    UserInDB,
    UserBase
)

# --- 认证相关 ---
from .token import Token, TokenData

# --- 量表相关 ---
from .scale import (
    ScaleOption,
    ScaleQuestion,
    ScaleInfo,
    ScaleQuestionsResponse,
    AvailableScalesResponse
)

# --- 评估相关 ---
from .assessment import (
    BasicInfoSubmit,
    AssessmentSubmitResponse,
    AssessmentSummary # 评估摘要 Schema
)

# --- 报告相关 ---
# ReportData 现在内部引用了 AttributeRead，但我们仍在此处导出 ReportData
from .report import ReportData, ReportResponse, ReportStatusResponse

# --- 百科相关 ---
from .encyclopedia import (
    EncyclopediaEntry,
    CategoriesResponse,
    EntriesResponse
)

# --- 统计相关 ---
from .stats import DemographicsStats, ChartData

# --- 审讯相关 ---
from .interrogation import (
    InterrogationBasicInfo,
    InterrogationQAInput,
    InterrogationRecordCreate,
    InterrogationRecordUpdate,
    InterrogationRecordRead
)

# --- 指导方案相关 ---
from .guidance import GuidanceResponse

# +++ 新增：属性相关 +++
# 假设 AttributeRead, AttributeCreate, AttributeUpdate 在 attribute.py 中定义
try:
    from .attribute import AttributeRead, AttributeCreate, AttributeUpdate
    ATTRIBUTE_SCHEMAS_IMPORTED = True
    # 将需要导出的属性 Schema 名称添加到列表中
    ATTRIBUTE_SCHEMAS_TO_EXPORT = ["AttributeRead", "AttributeCreate", "AttributeUpdate"]
except ImportError:
    # 如果 attribute.py 或其中的模型尚未创建，避免报错
    print("警告: 未能从 app.schemas.attribute 导入属性相关 Schema。请确保文件和模型已创建。")
    ATTRIBUTE_SCHEMAS_IMPORTED = False
    ATTRIBUTE_SCHEMAS_TO_EXPORT = []
# +++ 属性导入结束 +++


# --- 定义公开接口 (__all__) ---
# 包含所有希望通过 `from app.schemas import *` 导入的 Schema 名称
__all__ = [
    # Base (if defined here)
    # "BaseSchema",

    # User
    "User", "UserCreate", "UserUpdate", "UserInDB", "UserBase",
    "UserListResponse", "UserUpdateAdmin",

    # Token
    "Token", "TokenData",

    # Scale
    "ScaleOption", "ScaleQuestion", "ScaleInfo", "ScaleQuestionsResponse", "AvailableScalesResponse",

    # Assessment
    "BasicInfoSubmit", "AssessmentSubmitResponse", "AssessmentSummary",

    # Report
    "ReportData", "ReportResponse", "ReportStatusResponse",

    # Encyclopedia
    "EncyclopediaEntry", "CategoriesResponse", "EntriesResponse",

    # Stats
    "DemographicsStats", "ChartData",

    # Interrogation
    "InterrogationBasicInfo", "InterrogationQAInput", "InterrogationRecordCreate",
    "InterrogationRecordUpdate", "InterrogationRecordRead",

    # Guidance
    "GuidanceResponse",

    # Attribute (conditionally add based on successful import)
    *ATTRIBUTE_SCHEMAS_TO_EXPORT # 使用星号解包列表
]

# (可选) 打印确认信息
print(f"[Schemas Init] 公开导出的 Schemas: {len(__all__)} 个。")
if not ATTRIBUTE_SCHEMAS_IMPORTED:
     print("[Schemas Init] 警告: 属性相关的 Schema 未被导出。")

--------------------------------------------------
文件路径: app\tasks\analysis.py
--------------------------------------------------
# app/tasks/analysis.py
import logging
import os
import sys
import asyncio
import json
# --- 使用异步和同步 Redis 客户端 ---
import redis.asyncio as aredis # 异步别名
import redis # 标准同步客户端

# --- 路径设置 (保持不变) ---
TASK_DIR = os.path.dirname(os.path.abspath(__file__))
APP_ROOT_FROM_TASK = os.path.dirname(TASK_DIR) # app/
PROJECT_ROOT_FROM_TASK = os.path.dirname(APP_ROOT_FROM_TASK) # PsychologyAnalysis/
if PROJECT_ROOT_FROM_TASK not in sys.path:
    sys.path.insert(0, PROJECT_ROOT_FROM_TASK)
    print(f"[Celery Task Init] 已添加 {PROJECT_ROOT_FROM_TASK} 到 sys.path 以供 worker 使用")

# --- 核心导入 (添加 STATUS_PENDING, STATUS_PROCESSING) ---
try:
    from app.core.celery_app import celery_app
    from app.core.config import settings
    from app.db.session import AsyncSessionLocal
    from app.crud import assessment as crud_assessment
    # +++ 导入所有需要的状态常量 +++
    from app.models.assessment import STATUS_COMPLETE, STATUS_FAILED, STATUS_PENDING, STATUS_PROCESSING
    # +++++++++++++++++++++++++++++++
    from src.ai_utils import generate_report_content
    from src.utils import setup_logging
    WORKER_LOGGER_NAME = f"{settings.APP_NAME}_Worker"
    setup_logging(log_level_str=settings.LOG_LEVEL,
                    log_dir_name=os.path.basename(settings.LOGS_DIR),
                    logger_name=WORKER_LOGGER_NAME)
    logger = logging.getLogger(WORKER_LOGGER_NAME)
    print(f"[Celery Task Init] Logger '{WORKER_LOGGER_NAME}' 配置完成。")
except ImportError as e:
        print(f"[Celery Task Init] CRITICAL: 无法导入 src 模块或状态常量: {e}", exc_info=True)
        generate_report_content = None
        # --- 添加后备定义 ---
        STATUS_COMPLETE = "complete"
        STATUS_FAILED = "failed"
        STATUS_PENDING = "pending"
        STATUS_PROCESSING = "processing"
        # ---------------------
        logger = logging.getLogger(__name__)
        if not logger.hasHandlers(): logging.basicConfig(level=logging.INFO)
        logger.critical(f"CRITICAL: 核心处理函数 (generate_report_content) 或状态常量导入失败: {e}")
except Exception as setup_err:
        print(f"[Celery Task Init] CRITICAL: 初始化失败: {setup_err}", exc_info=True)
        generate_report_content = None
        # --- 添加后备定义 ---
        STATUS_COMPLETE = "complete"
        STATUS_FAILED = "failed"
        STATUS_PENDING = "pending"
        STATUS_PROCESSING = "processing"
        # ---------------------
        logger = logging.getLogger(__name__)
        if not logger.hasHandlers(): logging.basicConfig(level=logging.INFO)
        logger.critical(f"CRITICAL: 初始化失败: {setup_err}")

# --- Redis Publish Function (保持不变) ---
def publish_report_status_sync(assessment_id: int, status: str, error_msg: str = None):
    """同步地将报告状态发布到 Redis。"""
    global logger
    if not hasattr(settings, 'REDIS_URL'):
         logger.error(f"Worker (Sync): settings 对象缺少 REDIS_URL，无法发布 ID {assessment_id} 的状态 '{status}'。")
         return
    channel_name = f"report-ready:{assessment_id}"
    message_payload = {"status": status}
    if error_msg:
        message_payload["error"] = error_msg
    message_json = json.dumps(message_payload)
    redis_client = None
    try:
        redis_client = redis.Redis.from_url(
            settings.REDIS_URL,
            decode_responses=True
        )
        redis_client.publish(channel_name, message_json)
        logger.info(f"Worker (Sync): 已向频道 '{channel_name}' 发布消息: {message_json}")
    except Exception as e:
        logger.error(f"Worker (Sync): 向频道 '{channel_name}' 发布消息时出错: {e}", exc_info=True)
    finally:
         if redis_client:
              try:
                   redis_client.close()
                   logger.debug(f"Worker (Sync): Redis client for publish ID {assessment_id} closed.")
              except Exception as close_err:
                   logger.warning(f"Worker (Sync): 关闭 Redis publish client 时出错: {close_err}")

# --- Celery 任务定义 (更新 global 声明) ---
@celery_app.task(bind=True, name='tasks.run_ai_analysis')
def run_ai_analysis(self, assessment_id: int):
    """
    Celery 任务：异步运行 AI 分析、更新报告文本和状态，完成后 *同步* 发布 Redis 消息。
    """
    # +++ 确保所有使用的状态常量都在 global 声明中 +++
    global logger, STATUS_COMPLETE, STATUS_FAILED, STATUS_PENDING, STATUS_PROCESSING
    # +++++++++++++++++++++++++++++++++++++++++++++++
    if logger is None:
        print("CRITICAL ERROR: Logger 在 run_ai_analysis 任务中不可用!")
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

    task_id_str = f"[Celery Task {self.request.id}]"
    logger.info(f"{task_id_str} 收到任务，评估 ID: {assessment_id}")

    if generate_report_content is None:
        error_msg = "核心处理函数导入失败"
        logger.error(f"{task_id_str} {error_msg}，中止任务 ID {assessment_id}。")
        try:
            async def update_fail_status():
                async with AsyncSessionLocal() as session:
                    # 使用 STATUS_FAILED 常量
                    await crud_assessment.update_status(db=session, assessment_id=assessment_id, new_status=STATUS_FAILED)
                    logger.info(f"{task_id_str} 已尝试将 ID {assessment_id} 状态更新为失败 (导入错误)。")
            asyncio.run(update_fail_status())
        except Exception as db_err:
             logger.error(f"{task_id_str} 在更新失败状态时出错 (导入错误)，ID {assessment_id}: {db_err}")
        publish_report_status_sync(assessment_id, "failed", error_msg) # 这里仍然用字符串 "failed" 发布
        return {"status": "failure", "assessment_id": assessment_id, "error": error_msg}

    async def _run_analysis_async():
        nonlocal assessment_id
        report_text_to_save = "处理失败：发生未知错误"
        # 使用状态常量初始化
        final_status = STATUS_FAILED
        error_detail = None
        updated_to_complete = False

        async with AsyncSessionLocal() as session:
            try:
                logger.info(f"{task_id_str} 正在异步加载评估数据 ID: {assessment_id}")
                assessment_record = await crud_assessment.get(db=session, id=assessment_id)

                if not assessment_record:
                    logger.error(f"{task_id_str} 无法找到评估记录 ID: {assessment_id}")
                    error_detail = f"评估记录 ID {assessment_id} 未找到"
                    # 使用状态常量返回
                    return {"status": STATUS_FAILED, "assessment_id": assessment_id, "error": error_detail}

                # +++ 更新状态为 processing (使用常量) +++
                if assessment_record.status == STATUS_PENDING:
                    # 使用常量进行比较和更新
                    logger.info(f"{task_id_str} 将评估 ID {assessment_id} 状态更新为 '{STATUS_PROCESSING}'")
                    try:
                        await crud_assessment.update_status(db=session, assessment_id=assessment_id, new_status=STATUS_PROCESSING)
                    except Exception as status_update_err:
                         logger.error(f"{task_id_str} 更新状态为 processing 时出错 (ID: {assessment_id}): {status_update_err}", exc_info=True)
                # +++++++++++++++++++++++++++++++++++++++

                submission_data = {}
                for column in assessment_record.__table__.columns:
                    submission_data[column.name] = getattr(assessment_record, column.name)

                logger.debug(f"{task_id_str} 已加载数据，准备调用核心处理函数，ID: {assessment_id}")
                generated_text = generate_report_content(
                    submission_data=submission_data,
                    config=settings.model_dump(),
                    task_logger=logger
                )

                if generated_text is None:
                    logger.error(f"{task_id_str} 核心处理函数返回 None，ID: {assessment_id}")
                    report_text_to_save = "错误：报告生成意外返回空"
                    error_detail = "报告生成返回空"
                    final_status = STATUS_FAILED
                elif "错误" in generated_text or "Error" in generated_text or "失败" in generated_text:
                    logger.error(f"{task_id_str} 核心处理函数返回错误信息，ID {assessment_id}: {generated_text[:200]}...")
                    report_text_to_save = generated_text
                    error_detail = f"AI 处理失败: {generated_text[:150]}"
                    final_status = STATUS_FAILED
                else:
                    logger.info(f"{task_id_str} 报告内容生成成功，ID: {assessment_id}")
                    report_text_to_save = generated_text
                    # 使用状态常量
                    final_status = STATUS_COMPLETE
                    error_detail = None

                logger.info(f"{task_id_str} 尝试异步更新报告文本到数据库，ID: {assessment_id}")
                report_to_save_str = str(report_text_to_save)
                updated_record = await crud_assessment.update_report_text(
                    db=session,
                    assessment_id=assessment_id,
                    report_text=report_to_save_str
                )

                if not updated_record:
                    logger.error(f"{task_id_str} 尝试更新数据库时记录 ID {assessment_id} 未找到！")
                    if final_status == STATUS_COMPLETE: final_status = STATUS_FAILED # 使用常量
                    error_detail = f"数据库更新失败 (更新时 ID: {assessment_id} 未找到)"
                else:
                    logger.info(f"{task_id_str} 数据库报告文本更新成功，ID: {assessment_id}")
                    # +++++++ 如果报告文本更新成功，并且预期状态是 complete，则更新状态 (使用常量) +++++++
                    if final_status == STATUS_COMPLETE:
                        logger.info(f"{task_id_str} 尝试异步更新评估状态为 '{STATUS_COMPLETE}'，ID: {assessment_id}")
                        try:
                            status_updated_record = await crud_assessment.update_status(
                                db=session,
                                assessment_id=assessment_id,
                                new_status=STATUS_COMPLETE # 使用常量
                            )
                            if status_updated_record:
                                logger.info(f"{task_id_str} 数据库状态更新为 '{STATUS_COMPLETE}' 成功，ID: {assessment_id}")
                                updated_to_complete = True
                            else:
                                logger.error(f"{task_id_str} 更新状态为 '{STATUS_COMPLETE}' 时记录 ID {assessment_id} 未找到！")
                                final_status = STATUS_FAILED # 使用常量
                                error_detail = f"数据库状态更新失败 (更新时 ID: {assessment_id} 未找到)"
                        except Exception as status_update_err:
                            logger.error(f"{task_id_str} 更新状态为 '{STATUS_COMPLETE}' 时出错，ID {assessment_id}: {status_update_err}", exc_info=True)
                            error_detail = f"数据库状态更新时出错: {status_update_err}"
                    # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

            except Exception as e:
                logger.error(f"{task_id_str} 在 _run_analysis_async 中发生意外错误，ID {assessment_id}: {e}", exc_info=True)
                error_message = f"任务执行失败: {type(e).__name__} - {str(e)}"
                report_text_to_save = error_message[:2000]
                final_status = STATUS_FAILED # 使用常量
                error_detail = error_message[:150]
                try:
                    logger.info(f"{task_id_str} 尝试将错误信息和失败状态写入数据库，ID {assessment_id}")
                    await crud_assessment.update_report_text(
                         db=session,
                         assessment_id=assessment_id,
                         report_text=report_text_to_save
                     )
                    # 使用常量更新状态
                    await crud_assessment.update_status(
                         db=session,
                         assessment_id=assessment_id,
                         new_status=STATUS_FAILED
                     )
                    logger.info(f"{task_id_str} 已将最终错误信息和失败状态写入数据库，ID {assessment_id}")
                except Exception as db_err_on_fail:
                    logger.error(f"{task_id_str} 在失败处理中写入数据库也失败了，ID {assessment_id}: {db_err_on_fail}")

        # 返回最终状态 (常量)，以及其他信息
        return {"status": final_status, "report_text": report_text_to_save, "error": error_detail, "updated_to_complete": updated_to_complete}

    # --- 运行异步块并发布状态 (使用常量) ---
    result = None
    final_task_status = STATUS_FAILED # 使用常量
    report_length = 0
    error_for_publish = "任务执行期间发生未知错误"
    publish_status_str = "failed" # 用于发布到 Redis 的状态字符串，保持 success/failed

    try:
        result = asyncio.run(_run_analysis_async())
        final_task_status = result.get("status", STATUS_FAILED)
        error_for_publish = result.get("error")

        if final_task_status == STATUS_COMPLETE and result.get("updated_to_complete"):
            report_length = len(result.get("report_text", ""))
            error_for_publish = None
            publish_status_str = "success" # Redis 发布 success
        elif final_task_status == STATUS_COMPLETE and not result.get("updated_to_complete"):
             logger.warning(f"{task_id_str} 任务成功完成但状态更新至DB失败，ID {assessment_id}。错误: {error_for_publish}")
             error_for_publish = "报告已生成，但最终状态更新至数据库时出错。"
             publish_status_str = "success" # 核心完成，仍发布 success
        else: # 任务失败
             logger.error(f"{task_id_str} 任务处理失败，ID {assessment_id}。错误: {error_for_publish}")
             publish_status_str = "failed" # Redis 发布 failed

        # *** 同步发布最终状态到 Redis (使用 "success" 或 "failed" 字符串) ***
        publish_report_status_sync(assessment_id, publish_status_str, error_for_publish)

    except Exception as task_exec_err:
        logger.critical(f"{task_id_str} Celery 任务执行期间发生顶层错误，ID {assessment_id}: {task_exec_err}", exc_info=True)
        error_msg = f"任务执行错误: {type(task_exec_err).__name__} - {str(task_exec_err)}"
        error_for_publish = error_msg[:150]
        final_task_status = STATUS_FAILED # 使用常量
        publish_status_str = "failed"     # Redis 发布 failed

        try:
            from src.data_handler import DataHandler
            sync_db_path = settings.DB_PATH_SQLITE
            sync_handler = DataHandler(db_path=sync_db_path)
            sync_handler.update_report_text(assessment_id, error_msg[:2000])
            # sync_handler.update_status(assessment_id, STATUS_FAILED) # 假设有同步更新状态方法
            logger.info(f"{task_id_str} 已尝试同步记录顶层错误到数据库，ID {assessment_id}")
        except Exception as sync_db_err:
            logger.error(f"{task_id_str} 同步记录顶层错误到数据库失败，ID {assessment_id}: {sync_db_err}")
        finally:
             publish_report_status_sync(assessment_id, publish_status_str, error_for_publish)

    # --- 返回任务结果 (使用 "success" 或 "failure" 字符串，与 Redis 发布一致) ---
    logger.info(f"{task_id_str} 任务处理完成，ID {assessment_id}, 结果: {final_task_status}")
    if final_task_status == STATUS_COMPLETE:
        return {"status": "success", "assessment_id": assessment_id, "report_length": report_length, "db_status_updated": result.get("updated_to_complete", False)}
    else:
        return {"status": "failure", "assessment_id": assessment_id, "error": error_for_publish}

--------------------------------------------------
文件路径: app\tasks\__init__.py
--------------------------------------------------


--------------------------------------------------
文件路径: config\psychology_encyclopedia.json
--------------------------------------------------

[
    {
      "category": "心理小贴士",
      "title": "深呼吸缓解压力",
      "content": "每天花几分钟进行深呼吸练习，有助于放松神经系统，缓解积累的压力和焦虑感。"
    },
    {
      "category": "心理小贴士",
      "title": "积极自我对话",
      "content": "有意识地用积极、鼓励的语言与自己对话，可以显著提升情绪状态和自信心。"
    },
    {
      "category": "心理小贴士",
      "title": "充足睡眠的重要性",
      "content": "保证每晚7-9小时的高质量睡眠，是维护身心健康、巩固记忆和情绪调节的基础。"
    },
    {
      "category": "常见心理现象",
      "title": "确认偏误 (Confirmation Bias)",
      "content": "人们倾向于寻找、解释和记住那些证实自己已有信念或假设的信息，而忽略矛盾的证据。"
    },
    {
      "category": "常见心理现象",
      "title": "安慰剂效应 (Placebo Effect)",
      "content": "指病人虽然获得无效的治疗，但却“预料”或“相信”治疗有效，而让病患症状得到舒缓的现象。"
    },
    {
      "category": "情绪管理",
      "title": "情绪标签化",
      "content": "尝试识别并命名你正在经历的情绪（例如，“我感到有些焦虑”），这有助于更好地理解和管理它。"
    },
    {
      "category": "情绪管理",
      "title": "ABC模型",
      "content": "由心理学家阿尔伯特·艾利斯提出，指事件(A)本身不直接导致情绪后果(C)，而是我们对事件的信念(B)导致了情绪后果。"
    }
    
  ]

--------------------------------------------------
文件路径: input\questionnaires\1测你性格最真实的一面.json
--------------------------------------------------
{
  "title": "测你性格最真实的一面",
  "description": "面具下的个人才最真实。社会心理学家笑侃每个人的家里都置有一个面具衣柜，以扮演不同的角色，应对不同的需要和责任。所以，当你摘下所有的面具，你的性格呈现哪种特质呢？本题告诉你答案。",
  "questions": [
    {
      "number": 1,
      "text": "若有块地是养老用的房子，你会盖在哪？",
      "options": [
        {"text": "靠近湖边", "score": 8},
        {"text": "靠近河边", "score": 15},
        {"text": "深山内", "score": 6},
        {"text": "森林", "score": 10}
      ]
    },
    {
      "number": 2,
      "text": "吃西餐最先动那一道？",
      "options": [
        {"text": "面包", "score": 6},
        {"text": "肉类", "score": 15},
        {"text": "沙拉", "score": 6},
        {"text": "饮料", "score": 6}
      ]
    },
    {
      "number": 3,
      "text": "如果节庆要喝点饮料，你认为如何搭配最适当呢？",
      "options": [
        {"text": "耶诞节／香槟", "score": 15},
        {"text": "新年／牛奶", "score": 6},
        {"text": "情人节／葡萄酒", "score": 1},
        {"text": "国庆日／威士忌", "score": 6}
      ]
    },
    {
      "number": 4,
      "text": "你通常什么时候洗澡？",
      "options": [
        {"text": "吃完晚饭後", "score": 10},
        {"text": "吃晚饭前", "score": 15},
        {"text": "看完电视後", "score": 6},
        {"text": "上床前", "score": 8},
        {"text": "早上起床才洗", "score": 3},
        {"text": "没有特定时间", "score": 6}
      ]
    },
    {
      "number": 5,
      "text": "如果你可以化为天空的一隅，希望自己成为什么呢？",
      "options": [
        {"text": "太阳", "score": 1},
        {"text": "月亮", "score": 1},
        {"text": "星星", "score": 8},
        {"text": "云", "score": 15}
      ]
    },
    {
      "number": 6,
      "text": "你觉得用红色笔写的［爱］字比用绿色笔更能代表真爱吗？",
      "options": [
        {"text": "是", "score": 1},
        {"text": "否", "score": 3}
      ]
    },
    {
      "number": 7,
      "text": "如果你在选择窗帘的颜色，你会选择。。。？",
      "options": [
        {"text": "红色", "score": 15},
        {"text": "蓝色", "score": 6},
        {"text": "绿色", "score": 6},
        {"text": "白色", "score": 8},
        {"text": "黄色", "score": 1},
        {"text": "橙色", "score": 3},
        {"text": "黑色", "score": 1},
        {"text": "紫色", "score": 10}
      ]
    },
    {
      "number": 8,
      "text": "挑选一种你最喜爱的水果吧！",
      "options": [
        {"text": "葡萄", "score": 1},
        {"text": "水梨", "score": 6},
        {"text": "橘子", "score": 8},
        {"text": "香蕉", "score": 15},
        {"text": "樱桃", "score": 3},
        {"text": "苹果", "score": 10},
        {"text": "葡萄柚", "score": 8},
        {"text": "哈密瓜", "score": 6},
        {"text": "柿子", "score": 3},
        {"text": "木瓜", "score": 10},
        {"text": "凤梨", "score": 15}
      ]
    },
    {
      "number": 9,
      "text": "若你是动物，你希望身上搭配什么颜色的毛？",
      "options": [
        {"text": "狮子／红毛", "score": 15},
        {"text": "猫咪／蓝毛", "score": 6},
        {"text": "大象／绿毛", "score": 1},
        {"text": "狐狸／黄毛", "score": 6}
      ]
    },
    {
      "number": 10,
      "text": "你会为名利权位，刻意讨好上司或朋友吗？",
      "options": [
        {"text": "会", "score": 3},
        {"text": "不会", "score": 1}
      ]
    },
    {
      "number": 11,
      "text": "你认为朋友比家人更重要吗？",
      "options": [
        {"text": "是", "score": 15},
        {"text": "否", "score": 6}
      ]
    },
    {
      "number": 12,
      "text": "若你是只白蝴蝶，会停在那一种颜色的花上咧？",
      "options": [
        {"text": "红色", "score": 15},
        {"text": "粉红色", "score": 8},
        {"text": "黄色", "score": 3},
        {"text": "紫色", "score": 6}
      ]
    },
    {
      "number": 13,
      "text": "假日无聊时，你会选择什么电视节目来看？",
      "options": [
        {"text": "综艺节目", "score": 10},
        {"text": "新闻节目", "score": 15},
        {"text": "连续剧", "score": 6},
        {"text": "体育转播", "score": 15},
        {"text": "电影频道", "score": 10}
      ]
    }
  ],
  "results": [
    {
      "range": {"min": 101, "max": 195},
      "conclusion": "【积极热情】个性开放，觉得助人为快乐之本。做事乾脆俐落，有时会过度激动，但又富有强烈的同情心，令人莫明的想和他们亲近。也因为他们的复原力很强，我们能轻易感觉一股够劲的行动力，和他们在一起就像有了一股生命的泉源，不会有想放弃的念头，因为他们总是抱持著乐观进取的态度。 ◎积极人：勇於追求目标理想，不会放弃任何希望，也具有越挫越勇的特质和困难环境中越不易击败的精神。 ◎热情人：生活圈广泛五彩缤纷，比较不拘小节，因此造成他们的个性坦率直来直往活泼好动的性格，也常有孩子气的举动。"
    },
    {
      "range": {"min": 90, "max": 100},
      "conclusion": "【领导人】做事慢条斯理，喜欢思考、沉淀思绪，爱好命令别人，讨厌别人的反抗与被质疑的态度，不容许也不容许自己输给别人。喜爱学习，想让自己成为最好的。而达不到目标时，会不分青红皂白的生闷气。"
    },
    {
      "range": {"min": 79, "max": 89},
      "conclusion": "【感性人】表达能力丰富，想像空间大，因此常胡思乱想而变的多愁善感，容易沉醉在罗曼蒂克与甜言蜜语渔，对爱情总是既期待又怕受伤，常无厘头又莫明的对号入坐。个性属於优柔寡断型，通不顾现实只跟著感觉走，让人猜不著他的想法与思考逻辑。"
    },
    {
      "range": {"min": 60, "max": 78},
      "conclusion": "【理性＆淡定】做事总是深思熟虑考虑再三，谨慎小心，冷静且也当个容易妥协的人，有时候宁愿自己承受舆论与压力，也不愿说出来和好友谈谈，因为他们总是认为自己能熬过那么不堪苦的日子，痕都只是在逞强罢了。他们通常讨厌被束缚，更市酷爱自由！ ◎理性人：深思熟虑为第一原则，凡事要求公私分明，生活可能较拘谨严肃，对於赞美悲伤或开心等没什么差异性。 ◎淡定人：与世无争恬定主意者，内心没什么波澜，就像温驯的绵羊，只要能够生活就好，不必计较太多，成为只羡鸳鸯不羡仙的那一类人。"
    },
    {
      "range": {"min": 40, "max": 59},
      "conclusion": "【双重＆孤寂】环境的因素会让你不知道该怎么表现你自己，所以你可能有见人说人话的习惯，其实你热爱人多的时候，只是有时会导致你慌乱，不过你还会因为现实的需要而委屈自己，配合他人！通常会得不到满足而受挫，造成自闭。 ◎双重人：不会适时表达情感，压抑情绪总是他们碰到阻碍和困难时的第一个反应，学习如何发泄情绪与传达自己的意见，市必须优先学习的。 ◎孤寂人：对於现实不满，不易与人相处，难以找到生活的目标与重心，觉得没人了解自己，常引发强烈的自我防卫意识，就算与人交往，心中仍有一份挥之不去的孤单。"
    },
    {
      "range": {"min": 0, "max": 39},
      "conclusion": "【现实＆自我】喜欢多变刺激的事，是个很有心机的人，而且计划周详，别人对你难以揣测，对任何事你都充满企图心，刚愎自用，想突显求表现。常追求遥不可及的梦想，造成不平衡的心态，隐瞒自己也欺骗别人。 ◎现实人：为了讨好上司、朋友，让人觉得墙头草两边倒，心机重，心眼小，自私又自利，但往往能为自己打算未来，为自己创造一番天地。 ◎自我人：常透过主观的感受来表达意见，然而，人际关系的走样，或许是造成压力的来源。不自觉的划地自限压抑情绪，也不愿被外在所影响而尝试改变，更不会考虑别人的感受，即便经历了挫折？仍然固执自己的理念。"
    }
  ]
}

--------------------------------------------------
文件路径: input\questionnaires\2亲子关系问卷量表.json
--------------------------------------------------
{
    "title": "亲子关系问卷量表",
    "description": "该量表主要在于测量您和孩子的关系如何。家长您好：请根据你的实际情况（实际感受）作答。答案没有对错之分，对每一个句子无须多考虑。",
    "questions": [
      {
        "number": 1,
        "text": "不管我的工作或生活再忙碌，每一天我都会留一些时间给子女",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 2,
        "text": "我能经常保持愉快的心情和孩子相处",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 3,
        "text": "我认为孩子是有理性的，能自己面对和解决问题",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 4,
        "text": "和孩子对话时，我甚少使用「你应该…」、「你最好…否则…」、「你再不… 我就…」的语气和孩子交谈",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 5,
        "text": "我觉得孩子能欢喜的生活，比成绩好更重要",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 6,
        "text": "我觉得孩子犯错和惹麻烦是成长必经的过程",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 7,
        "text": "孩子说话时，我能耐心专注的听完",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 8,
        "text": "我能经常和孩子有亲密的接触（如摸头、拍肩、拍手、相互拥抱……）",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 9,
        "text": "即使孩子犯了错，我也不会因此就认为他（她）是个坏孩子",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 10,
        "text": "我经常给自己和孩子充裕的时间，避免催促孩子",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 11,
        "text": "不论孩子发生什么事，我都能以孩子的立场，分析孩子内心的感受",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 12,
        "text": "亲子间有冲突时，我不认为一定是孩子的错",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 13,
        "text": "我能给孩子充分的自主空间，决定自己的事",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 14,
        "text": "我要求孩子做的事情，我自己都能做到",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 15,
        "text": "我答应孩子的事情，我一定都会履行",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 16,
        "text": "我与孩子谈话时，我能了解孩子内心真正的感受",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 17,
        "text": "我了解孩子内心的喜好和厌恶",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 18,
        "text": "孩子愿意主动的告诉我，他在外面发生的事情和内心感受",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 19,
        "text": "和孩子谈完话，我甚少批评或指责孩子的想法",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      },
      {
        "number": 20,
        "text": "我满意我目前的家庭和孩子的状况",
        "options": [
          {"text": "很不符合", "score": 1},
          {"text": "不符合", "score": 2},
          {"text": "一般", "score": 3},
          {"text": "符合", "score": 4},
          {"text": "非常符合", "score": 5}
        ]
      }
    ],
    "results": [
      {
        "range": {"min": 20, "max": 59},
        "conclusion": "亲子关系已有了危机，须马上调整。 建议：您需要缓和与孩子之间的关系，可以尝试与孩子多聊聊学校里的生活，每天有哪些开心和不开心的事情，试着去倾听孩子的声音，像对待朋友般给出建议而不是严厉地去指责孩子。平常的时候可以尝试与孩子一同参与一些户外活动，增进彼此间的联系。"
      },
      {
        "range": {"min": 60, "max": 79},
        "conclusion": "你们的相处还算良好。 建议：您和孩子相处的较好，继续保持这样的状况，尝试进一步地去与孩子沟通内心的想法和观念，您和孩子的关系可以更进一步。"
      },
      {
        "range": {"min": 80, "max": 100},
        "conclusion": "恭喜你，你们的亲子关系很好 建议：您和孩子相处的非常好，请继续保持下去。"
      }
    ]
  }

--------------------------------------------------
文件路径: input\questionnaires\3焦虑症自评量表 (SAS).json
--------------------------------------------------
{
  "title": "焦虑症自评量表 (SAS)",
  "description": "如果你最近总是感到惶恐、不安，患得患失，无法安心做事情，似乎身后总有一头怪兽追着你跑，那么，测测你的焦虑程度水平，了解自己的焦虑程度，我们需要更加清晰地去觉察自己内在的情绪状态。本焦虑自评量表 Self-Rating Anxiety Scale（SAS）由华裔教授Zung于1971年编制，含有20个条目，用于评出焦虑的主观感受。下面请根据您最近一周的感受进行选择。",
  "questions": [
    {
      "number": 1,
      "text": "我感到比往常更加神经过敏的焦虑",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 2,
      "text": "我无缘无故感到担心",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 3,
      "text": "我容易心烦意乱或感到恐慌",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 4,
      "text": "我感到我的身体好像被分成几块，支离破碎",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 5,
      "text": "我感到事事都很顺利，不会有倒霉的事情发生",
      "options": [
        {"text": "很少有", "score": 4},
        {"text": "有时有", "score": 3},
        {"text": "大部分时间有", "score": 2},
        {"text": "绝大部分时间有", "score": 1}
      ]
    },
    {
      "number": 6,
      "text": "我的四肢拌动和震颤",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 7,
      "text": "我因头痛、颈痛和背痛而烦恼",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 8,
      "text": "我感到无力而且容易疲劳",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 9,
      "text": "我感到平静，能安静坐下来",
      "options": [
        {"text": "很少有", "score": 4},
        {"text": "有时有", "score": 3},
        {"text": "大部分时间有", "score": 2},
        {"text": "绝大部分时间有", "score": 1}
      ]
    },
    {
      "number": 10,
      "text": "我感到我的心跳较快",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 11,
      "text": "我因阵阵的眩晕而不舒服",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 12,
      "text": "我有阵阵要晕倒的感觉",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 13,
      "text": "我呼吸时进气和出气都不费力",
      "options": [
        {"text": "很少有", "score": 4},
        {"text": "有时有", "score": 3},
        {"text": "大部分时间有", "score": 2},
        {"text": "绝大部分时间有", "score": 1}
      ]
    },
    {
      "number": 14,
      "text": "我的手指和脚趾感到麻木和刺激",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 15,
      "text": "我因胃痛和消化不良而苦恼",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 16,
      "text": "我必须频繁排尿",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 17,
      "text": "我的手总是温暖而干燥",
      "options": [
        {"text": "很少有", "score": 4},
        {"text": "有时有", "score": 3},
        {"text": "大部分时间有", "score": 2},
        {"text": "绝大部分时间有", "score": 1}
      ]
    },
    {
      "number": 18,
      "text": "我觉得脸发烧发红",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    },
    {
      "number": 19,
      "text": "我容易入睡，晚上休息很好",
      "options": [
        {"text": "很少有", "score": 4},
        {"text": "有时有", "score": 3},
        {"text": "大部分时间有", "score": 2},
        {"text": "绝大部分时间有", "score": 1}
      ]
    },
    {
      "number": 20,
      "text": "我做恶梦",
      "options": [
        {"text": "很少有", "score": 1},
        {"text": "有时有", "score": 2},
        {"text": "大部分时间有", "score": 3},
        {"text": "绝大部分时间有", "score": 4}
      ]
    }
  ],
  "results": [
    {
      "range": {"min": 0, "max": 49},
      "conclusion": "您的焦虑水平在正常范围之内，正常范围内的焦虑水平，不意味着您此刻没有焦虑情绪，可能您正感到轻微的不适、不安，我们将此称为适度焦虑。适度焦虑是一种“合理情绪”，也就是我们平常会感受到的一些情绪，比如考试前的紧张，上台演讲之前的焦虑，重大抉择之前的恐慌，适度焦虑有助于我们认清自身状态，抱持警惕和刺激，化焦虑为动力，激发人的行动力。如果您为此感到困扰，让我们以“不评判”为目的去观察自己的情绪、想法，不与它较劲，观察它，品味它，体会它，从而去接纳它，也许你会发现，接纳焦虑情绪，比逃避焦虑，会让你感到更轻松，更舒适。如果你感到轻松，继续保持愉快心情吧！（以上测试结果仅供参考）"
    },
    {
      "range": {"min": 50, "max": 59},
      "conclusion": "您的焦虑水平为轻度，也许焦虑情绪已经让你感到困扰，甚至有时感到肌肉紧张，呼吸不畅，过度思索感到无法停止思考，让你的生活受到了一定的影响，缓解焦虑小建议： 1.接纳自己的情绪，当你对抗、或者回避焦虑的时候，也许你会发现自己感到更加的不自在。我们不需要“克服”焦虑，我们需要的是学会与焦虑共存。可尝试了解“森田疗法”，也许会让您有收获； 2.去跑步吧！坚持每天做有氧运动可以起到避免焦虑的作用，运动也可以给我们提供时间寻求有效的解决问题的方法，还可以缓解紧张情绪； 3.体验深度放松：冥想、瑜伽、正念等方式是经过验证过的有助于人们深度放松的方法，这种深度放松的方式可以让人养成遇事平和冷静的习惯； 4.如果做了诸多尝试与调整，依然存在困扰，建议寻找平台的专业心理咨询师寻求心理帮助。祝好。【温馨提示：在本量表中得分并不能作为焦虑症的诊断标准，仅供你了解自己使用。焦虑症的诊断和治疗，只有精神科医生才能进行哦】"
    },
    {
      "range": {"min": 60, "max": 69},
      "conclusion": "您的焦虑水平为中度，引起你焦虑的原因，可能不只是当下生活中的实际困扰，还包括对未来无名的担心，甚至是模糊的、不明原因的焦虑和担心，而且焦虑情绪似乎已经不只是影响到了你的心态、正常生活，你的身体也感受到了焦虑，身体过分紧张，肌肉过于紧绷，呼吸不够轻松，过分机警等等，在此情况下，可尝试运动、呼吸法、肌肉渐进放松法、正念、冥想、瑜伽等方式缓解情绪，如果感到靠自己的调整依然无法摆脱焦虑情绪，建议寻找本平台专业的心理咨询师寻求专业帮助，同时，建议到精神科或心理科做进一步检查，明确的焦虑症诊断需要到精神科进行，对于较严重的焦虑症，尽早进行药物治疗才是关键。多多关爱自己的心，祝好。【温馨提示：在本量表中得分并不能作为焦虑症的诊断标准，仅供你了解自己使用。焦虑症的诊断和治疗，只有精神科医生才能进行哦】"
    },
    {
      "range": {"min": 70, "max": 80},
      "conclusion": "您的焦虑水平较高，可尝试运动、呼吸法、肌肉渐进放松法、正念、冥想、瑜伽等方式缓解情绪，如果感到靠自己的调整依然无法摆脱焦虑情绪，建议寻找本平台专业的心理咨询师寻求专业帮助，同时，建议到精神科或心理科做进一步检查，明确的焦虑症诊断需要到精神科进行，对于较严重的焦虑症，尽早进行药物治疗才是关键。焦虑症的主要症状是，内心充满了过度的、长久的、模糊的、不明原因的焦虑和担心。其具体症状包括以下四类：身体紧张、自主神经系统反应性过强、对未来无名的担心、过分机警。这些症状有时单独存在，也可同时出现。多多关爱自己的心，祝好。【温馨提示：在本量表中得分并不能作为焦虑症的诊断标准，仅供你了解自己使用。焦虑症的诊断和治疗，只有精神科医生才能进行哦】"
    }
  ]
}

--------------------------------------------------
文件路径: input\questionnaires\4标准量表：抑郁症自测量表 (SDS).json
--------------------------------------------------
{
  "title": "标准量表：抑郁症自测量表 (SDS)",
  "description": "抑郁自评量表 Self-Rating Depression Scale（SDS）是由美国杜克大学医学院的William W.K.Zung于1965年编制的，是目前应用最广泛的抑郁自评量表之一。整个量表共20个条目，用于衡量抑郁状态的轻重程度及其在治疗中的变化，适用于具有抑郁症状的成年人。下面请根据您最近一周的感受进行选择。",
  "questions": [
    {
      "number": 1,
      "text": "我觉得闷闷不乐，情绪低沉",
      "options": [
        {"text": "没有或很少时间", "score": 1},
        {"text": "小部分时间", "score": 2},
        {"text": "相当多时间", "score": 3},
        {"text": "绝大部分或全部时间", "score": 4}
      ]
    },
    {
      "number": 2,
      "text": "我觉得一天之中早晨最好",
      "options": [
        {"text": "没有或很少时间", "score": 4},
        {"text": "小部分时间", "score": 3},
        {"text": "相当多时间", "score": 2},
        {"text": "绝大部分或全部时间", "score": 1}
      ]
    },
    {
      "number": 3,
      "text": "我一阵阵地哭出来或是想哭",
      "options": [
        {"text": "没有或很少时间", "score": 1},
        {"text": "小部分时间", "score": 2},
        {"text": "相当多时间", "score": 3},
        {"text": "绝大部分或全部时间", "score": 4}
      ]
    },
    {
      "number": 4,
      "text": "我夜间睡眠不好",
      "options": [
        {"text": "没有或很少时间", "score": 1},
        {"text": "小部分时间", "score": 2},
        {"text": "相当多时间", "score": 3},
        {"text": "绝大部分或全部时间", "score": 4}
      ]
    },
    {
      "number": 5,
      "text": "我吃的和平时一样多",
      "options": [
        {"text": "没有或很少时间", "score": 4},
        {"text": "小部分时间", "score": 3},
        {"text": "相当多时间", "score": 2},
        {"text": "绝大部分或全部时间", "score": 1}
      ]
    },
    {
      "number": 6,
      "text": "我与异性接触时和以往一样感到愉快",
      "options": [
        {"text": "没有或很少时间", "score": 4},
        {"text": "小部分时间", "score": 3},
        {"text": "相当多时间", "score": 2},
        {"text": "绝大部分或全部时间", "score": 1}
      ]
    },
    {
      "number": 7,
      "text": "我发觉我的体重在下降",
      "options": [
        {"text": "没有或很少时间", "score": 1},
        {"text": "小部分时间", "score": 2},
        {"text": "相当多时间", "score": 3},
        {"text": "绝大部分或全部时间", "score": 4}
      ]
    },
    {
      "number": 8,
      "text": "我有便秘的苦恼",
      "options": [
        {"text": "没有或很少时间", "score": 1},
        {"text": "小部分时间", "score": 2},
        {"text": "相当多时间", "score": 3},
        {"text": "绝大部分或全部时间", "score": 4}
      ]
    },
    {
      "number": 9,
      "text": "我心跳比平时快",
      "options": [
        {"text": "没有或很少时间", "score": 1},
        {"text": "小部分时间", "score": 2},
        {"text": "相当多时间", "score": 3},
        {"text": "绝大部分或全部时间", "score": 4}
      ]
    },
    {
      "number": 10,
      "text": "我无缘无故感到疲乏",
      "options": [
        {"text": "没有或很少时间", "score": 1},
        {"text": "小部分时间", "score": 2},
        {"text": "相当多时间", "score": 3},
        {"text": "绝大部分或全部时间", "score": 4}
      ]
    },
    {
      "number": 11,
      "text": "我的头脑和平时一样清楚",
      "options": [
        {"text": "没有或很少时间", "score": 4},
        {"text": "小部分时间", "score": 3},
        {"text": "相当多时间", "score": 2},
        {"text": "绝大部分或全部时间", "score": 1}
      ]
    },
    {
      "number": 12,
      "text": "我觉得经常做的事情并没有困难",
      "options": [
        {"text": "没有或很少时间", "score": 4},
        {"text": "小部分时间", "score": 3},
        {"text": "相当多时间", "score": 2},
        {"text": "绝大部分或全部时间", "score": 1}
      ]
    },
    {
      "number": 13,
      "text": "我觉得不安而平静不下来",
      "options": [
        {"text": "没有或很少时间", "score": 1},
        {"text": "小部分时间", "score": 2},
        {"text": "相当多时间", "score": 3},
        {"text": "绝大部分或全部时间", "score": 4}
      ]
    },
    {
      "number": 14,
      "text": "我对将来抱有希望",
      "options": [
        {"text": "没有或很少时间", "score": 4},
        {"text": "小部分时间", "score": 3},
        {"text": "相当多时间", "score": 2},
        {"text": "绝大部分或全部时间", "score": 1}
      ]
    },
    {
      "number": 15,
      "text": "我比平常容易激动",
      "options": [
        {"text": "没有或很少时间", "score": 1},
        {"text": "小部分时间", "score": 2},
        {"text": "相当多时间", "score": 3},
        {"text": "绝大部分或全部时间", "score": 4}
      ]
    },
    {
      "number": 16,
      "text": "我觉得做出决定是容易的",
      "options": [
        {"text": "没有或很少时间", "score": 4},
        {"text": "小部分时间", "score": 3},
        {"text": "相当多时间", "score": 2},
        {"text": "绝大部分或全部时间", "score": 1}
      ]
    },
    {
      "number": 17,
      "text": "我觉得自己是个有用的人，有人需要我",
      "options": [
        {"text": "没有或很少时间", "score": 4},
        {"text": "小部分时间", "score": 3},
        {"text": "相当多时间", "score": 2},
        {"text": "绝大部分或全部时间", "score": 1}
      ]
    },
    {
      "number": 18,
      "text": "我的生活过得很有意思",
      "options": [
        {"text": "没有或很少时间", "score": 4},
        {"text": "小部分时间", "score": 3},
        {"text": "相当多时间", "score": 2},
        {"text": "绝大部分或全部时间", "score": 1}
      ]
    },
    {
      "number": 19,
      "text": "我认为如果我死了别人会生活的更好些",
      "options": [
        {"text": "没有或很少时间", "score": 1},
        {"text": "小部分时间", "score": 2},
        {"text": "相当多时间", "score": 3},
        {"text": "绝大部分或全部时间", "score": 4}
      ]
    },
    {
      "number": 20,
      "text": "平常感兴趣的事我仍然照样感兴趣",
      "options": [
        {"text": "没有或很少时间", "score": 4},
        {"text": "小部分时间", "score": 3},
        {"text": "相当多时间", "score": 2},
        {"text": "绝大部分或全部时间", "score": 1}
      ]
    }
  ],
  "results": [
    {
      "range": {"min": 0, "max": 40},
      "conclusion": "经过检测您【无抑郁】，希望您继续保持良好的心情。（以上测试结果仅供参考）"
    },
    {
      "range": {"min": 40, "max": 47},
      "conclusion": "经过检测您为【轻微至轻度抑郁】，为了您和家人的幸福，建议您联系我们，进行更专业的检测。（以上测试结果仅供参考）"
    },
    {
      "range": {"min": 48, "max": 55},
      "conclusion": "经过检测您为【中至重度抑郁】，为了您和家人的幸福，建议您联系我们，进行更专业的检测。（以上测试结果仅供参考）"
    },
    {
      "range": {"min": 56, "max": 80},
      "conclusion": "经过检测您为【重度抑郁】，为了您和家人的幸福，建议您联系我们，进行更专业的检测。（以上测试结果仅供参考）"
    }
  ]
}

--------------------------------------------------
文件路径: input\questionnaires\5人际关系综合诊断量表.json
--------------------------------------------------
{
    "title": "人际关系综合诊断量表",
    "description": "这是一份人际关系行为困扰的诊断量表，共28个问题，每个问题做“是”（打 √）或“非”（打×）两种回答。请你根究自己的实际情况如实回答，答案没有对错之分：",
    "questions": [
      {
        "number": 1,
        "text": "关于自己的烦恼有口难言。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 2,
        "text": "和生人见面感觉不自然。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 3,
        "text": "过分地羡慕和妒忌别人。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 4,
        "text": "与异性交往太少。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 5,
        "text": "对连续不断地会谈感到困难。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 6,
        "text": "在社交场合，感到紧张。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 7,
        "text": "时常伤害别人。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 8,
        "text": "与异性来往感觉不自然。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 9,
        "text": "与一大群朋友在一起，常感到孤寂或失落。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 10,
        "text": "极易受窘。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 11,
        "text": "与别人不能和睦相处。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 12,
        "text": "不知道与异性相处如何适可而止。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 13,
        "text": "当不熟悉的人对自己倾诉他的生平遭遇以求同情时，自己常感到不知所措。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 14,
        "text": "担心别人对自己有什么坏印象。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 15,
        "text": "总是尽力使别人赏识自己。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 16,
        "text": "暗自思慕异性。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 17,
        "text": "时常避免表达自己的感受。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 18,
        "text": "对自己的仪表（容貌）缺乏信心。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 19,
        "text": "讨厌某人或被某人所讨厌。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 20,
        "text": "瞧不起异性。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 21,
        "text": "不能专注地倾听。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 22,
        "text": "自己的烦恼无人可倾诉。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 23,
        "text": "受别人排斥与冷漠。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 24,
        "text": "被异性瞧不起。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 25,
        "text": "不能广泛地听取各种各样意见、看法。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 26,
        "text": "自己常因受伤害而暗自伤心。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 27,
        "text": "常被别人谈论、愚弄。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      },
      {
        "number": 28,
        "text": "与异性交往不知如何更好相处。",
        "options": [
          {"text": "是", "score": 1},
          {"text": "非", "score": 0}
        ]
      }
    ],
    "results": [
      {
        "range": {"min": 0, "max": 8},
        "conclusion": "说明你在与朋友相处上的困扰较少。你善于交谈，性格比较开朗，主动，关心别人，你对周围的朋友都比较好，愿意和他们在一起，他们也都喜欢你，你们相处得不错。而且，你能够从与朋友相处中，得到乐趣。你的生活是比较充实而且丰富多彩的，你与异性朋友也相处得比较好。一句话，你不存在或较少存在交友方面的困扰，你善于与朋友相处，人缘很好，获得许多的好感与赞同。"
      },
      {
        "range": {"min": 9, "max": 14},
        "conclusion": "你与朋友相处存在一定程度的困扰。你的人缘很一般，换句话说，你和朋友的关系并不牢固，时好时坏，经常处在一种起伏波动之中。"
      },
      {
        "range": {"min": 15, "max": 28},
        "conclusion": "表明你在同朋友相处上的行为困扰较严重，分数超过20分，则表明你的人际关系困扰程度很严重，而且在心理上出现较为明显得障碍。你可能不善于交谈，也可能是一个性格孤僻的人，不开朗，或者有明显得自高自大、讨人嫌的行为。"
      }
    ]
  }

--------------------------------------------------
文件路径: input\questionnaires\6情绪稳定性测验量表.json
--------------------------------------------------
{
    "title": "情绪稳定性测验量表",
    "description": "请根据您的实际情况回答以下问题。",
    "questions": [
      {
        "number": 1,
        "text": "看到自己最近一次拍摄的照片，你有何想法？",
        "options": [
          {"text": "觉得不称心", "score": 2},
          {"text": "觉得很好", "score": 0},
          {"text": "觉得可以", "score": 1}
        ]
      },
      {
        "number": 2,
        "text": "你是否想到若干年后有什么使自己极为不安的事？",
        "options": [
          {"text": "经常想到", "score": 2},
          {"text": "从来没想到", "score": 0},
          {"text": "偶而想到", "score": 1}
        ]
      },
      {
        "number": 3,
        "text": "你是否被朋友、同事、同学起过绰号、挖苦过？",
        "options": [
          {"text": "这是常有的事", "score": 2},
          {"text": "从来没有", "score": 0},
          {"text": "偶尔有过", "score": 1}
        ]
      },
      {
        "number": 4,
        "text": "你上床以后，是否经常再起来一次，看看门窗是否关好。",
        "options": [
          {"text": "经常如此", "score": 2},
          {"text": "从不如此", "score": 0},
          {"text": "偶尔如此", "score": 1}
        ]
      },
      {
        "number": 5,
        "text": "你对与你关系最密切的人是否满意？",
        "options": [
          {"text": "不满意", "score": 2},
          {"text": "非常满意", "score": 0},
          {"text": "基本满意", "score": 1}
        ]
      },
      {
        "number": 6,
        "text": "你在半夜的时候，是否经常觉得有什么值得害怕的事？",
        "options": [
          {"text": "经常", "score": 2},
          {"text": "从来没有", "score": 0},
          {"text": "极少有这种情况", "score": 1}
        ]
      },
      {
        "number": 7,
        "text": "你是否经常梦见什么可怕的事而惊醒？",
        "options": [
          {"text": "经常", "score": 2},
          {"text": "没有", "score": 0},
          {"text": "极少", "score": 1}
        ]
      },
      {
        "number": 8,
        "text": "你是否曾经多次做同一个梦的情况？",
        "options": [
          {"text": "有", "score": 2},
          {"text": "没有", "score": 0},
          {"text": "记不清", "score": 1}
        ]
      },
      {
        "number": 9,
        "text": "有没有一种食物使你吃后呕吐？",
        "options": [
          {"text": "有", "score": 2},
          {"text": "没有", "score": 0},
          {"text": "记不清", "score": 1}
        ]
      },
      {
        "number": 10,
        "text": "除去看见的世界外，你心理有没有另一种世界？",
        "options": [
          {"text": "有", "score": 2},
          {"text": "没有", "score": 0},
          {"text": "记不清", "score": 1}
        ]
      },
      {
        "number": 11,
        "text": "你心里是否时常觉得你不是现在的父母所生？",
        "options": [
          {"text": "时常", "score": 2},
          {"text": "没有", "score": 0},
          {"text": "偶尔有", "score": 1}
        ]
      },
      {
        "number": 12,
        "text": "你是否曾经觉得有一个人爱你或尊敬你?",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "说不清", "score": 1}
        ]
      },
      {
        "number": 13,
        "text": "你是否常常觉得你的家庭对你不好，但是你又确知他们的确对你好？",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "偶尔", "score": 1}
        ]
      },
      {
        "number": 14,
        "text": "你是否觉得没有人十分了解你？",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "偶尔", "score": 1}
        ]
      },
      {
        "number": 15,
        "text": "你在早晨起来的时候最经常的感觉是什么？",
        "options": [
          {"text": "秋雨霏霏或枯叶遍地", "score": 2},
          {"text": "秋高气爽或艳阳天", "score": 0},
          {"text": "不清楚", "score": 1}
        ]
      },
      {
        "number": 16,
        "text": "你在高处的时候，是否觉得站不稳？",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "有时是这样", "score": 1}
        ]
      },
      {
        "number": 17,
        "text": "你平时是否觉得自己很强健？",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "不清楚", "score": 1}
        ]
      },
      {
        "number": 18,
        "text": "你是否一回家就立刻把门关上？",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "不清楚", "score": 1}
        ]
      },
      {
        "number": 19,
        "text": "你坐在小房间里把门关上后，是否觉得心理不安？",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "偶尔是", "score": 1}
        ]
      },
      {
        "number": 20,
        "text": "当一件事需要你做决定时，你是否觉得很难？",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "偶尔是", "score": 1}
        ]
      },
      {
        "number": 21,
        "text": "你是否常常用抛硬币、玩纸牌、抽签之类的游戏来测凶吉？",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "偶尔", "score": 1}
        ]
      },
      {
        "number": 22,
        "text": "你是否常常因为碰到东西而跌到？",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "偶尔", "score": 1}
        ]
      },
      {
        "number": 23,
        "text": "你是否需要一个多小时才能入睡，或醒的比你希望的早一个小时？",
        "options": [
          {"text": "经常这样", "score": 2},
          {"text": "从不这样", "score": 0},
          {"text": "偶尔这样", "score": 1}
        ]
      },
      {
        "number": 24,
        "text": "你是否经常看到、听到或感觉到别人觉察不到的东西？",
        "options": [
          {"text": "经常这样", "score": 2},
          {"text": "从不这样", "score": 0},
          {"text": "偶尔这样", "score": 1}
        ]
      },
      {
        "number": 25,
        "text": "你是否觉得自己有超越常人的能力？",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "不清楚", "score": 1}
        ]
      },
      {
        "number": 26,
        "text": "你是否曾经觉得因有人跟你走而心理不安？",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "不清楚", "score": 1}
        ]
      },
      {
        "number": 27,
        "text": "你是否觉得有人在注意你的言行？",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "不清楚", "score": 1}
        ]
      },
      {
        "number": 28,
        "text": "当你一个人走夜路时，是否觉得前面潜藏着危险？",
        "options": [
          {"text": "是", "score": 2},
          {"text": "否", "score": 0},
          {"text": "偶尔", "score": 1}
        ]
      },
      {
        "number": 29,
        "text": "你对别人自杀有什么想法？",
        "options": [
          {"text": "可以理解", "score": 2},
          {"text": "不可思议", "score": 0},
          {"text": "不清楚", "score": 1}
        ]
      }
    ],
    "results": [
      {
        "range": {"min": 0, "max": 20},
        "conclusion": "表明你情绪稳定，自信心强，具有较强的美感、道德感和理智感。你有一定的社会活动能力，能理解周围人们的心情，顾全大局。你一定是个性爽朗、受人欢迎得人。"
      },
      {
        "range": {"min": 21, "max": 40},
        "conclusion": "说明你情绪基本稳定，但较为深沉，对事情的考虑过于冷静，处世淡漠消极，不善于发挥自己的个性。你的自信心受到压抑，办事热情忽高忽低，瞻前顾后，踌躇不前。"
      },
      {
        "range": {"min": 41, "max": 58},
        "conclusion": "说明你的情绪极不稳定，日常烦恼太多，使自己的心情处于紧张和矛盾中。如果你得分在50分以上，则是一种危险信号，你务必请心理医生进一步诊断。"
      }
    ]
  }

--------------------------------------------------
文件路径: input\questionnaires\7汉密尔顿抑郁量表HAMD24.json
--------------------------------------------------
{
    "title": "汉密尔顿抑郁量表 (HAMD-24)",
    "description": "汉密顿抑郁量表由汉密尔顿于1960年编制，是临床上评定抑郁状态时应用得最为普遍的量表。本量表有17项、21项和24项等3种版本，这里介绍的是24项版本。这些项目包括抑郁所涉及的各种症状。适用于有抑郁症状的成年病人。下面请根据您一周以来的感受进行选择。",
    "questions": [
      {
        "number": 1,
        "text": "抑郁情绪",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "只在问到时才诉述", "score": 1},
          {"text": "在访谈中自发地表达", "score": 2},
          {"text": "不用言语也可以从表情-姿势-声音或欲哭中流露出这种情绪", "score": 3},
          {"text": "病人的自发言语和非语言表达几乎完全表现为这种情绪", "score": 4}
        ]
      },
      {
        "number": 2,
        "text": "有罪感",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "责备自己,感到自己已连累他人", "score": 1},
          {"text": "认为自己犯了罪,或反复思考以往的过失和错误", "score": 2},
          {"text": "认为目前的疾病,是对自己错误的惩罚,或有罪恶妄想", "score": 3},
          {"text": "罪恶妄想伴有指责或威胁性幻觉", "score": 4}
        ]
      },
      {
        "number": 3,
        "text": "自杀",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "觉得活着没有意义", "score": 1},
          {"text": "希望自己已经死去,或常想到与死有关的事", "score": 2},
          {"text": "消极观念自杀念头", "score": 3},
          {"text": "有严重自杀行为", "score": 4}
        ]
      },
      {
        "number": 4,
        "text": "入睡困难（初段失眠）",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "主诉有入睡困难,上床半小时后仍不能入睡", "score": 1},
          {"text": "主诉每晚均有入睡困难", "score": 2}
        ]
      },
      {
        "number": 5,
        "text": "睡眠不深（中段失眠）",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "睡眠浅,多恶梦", "score": 1},
          {"text": "半夜（晚12点钟以前）曾醒来（不包括上厕所）", "score": 2}
        ]
      },
      {
        "number": 6,
        "text": "早醒（末段失眠）",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "有早醒,比平时早醒1小时,但能重新入睡", "score": 1},
          {"text": "早醒后无法重新入睡", "score": 2}
        ]
      },
      {
        "number": 7,
        "text": "工作和兴趣",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "提问时才诉述", "score": 1},
          {"text": "自发地直接或间接表达对活动-工作或学习失去兴趣", "score": 2},
          {"text": "活动时间减少或成效下降,住院病人每天参加病房劳动或娱乐不满3小时", "score": 3},
          {"text": "因目前的疾病而停止工作,住院者不参加任何活动或者没有他人帮助便不能完成病室日常事务", "score": 4}
        ]
      },
      {
        "number": 8,
        "text": "阻滞（指思维和言语缓慢,注意力难以集中,主动性减退）",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "精神检查中发现轻度阻滞", "score": 1},
          {"text": "精神检查中发现明显阻滞", "score": 2},
          {"text": "精神检查进行困难", "score": 3},
          {"text": "完全不能回答问题木僵", "score": 4}
        ]
      },
      {
        "number": 9,
        "text": "激越",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "检查时有些心神不定", "score": 1},
          {"text": "明显心神不定或小动作多", "score": 2},
          {"text": "不能静坐,检查中曾起立", "score": 3},
          {"text": "搓手、咬手指、扯头发、咬嘴唇", "score": 4}
        ]
      },
      {
        "number": 10,
        "text": "精神性焦虑",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "问及时诉述", "score": 1},
          {"text": "自发地表达", "score": 2},
          {"text": "表情和言谈流露出明显忧虑", "score": 3},
          {"text": "明显惊恐", "score": 4}
        ]
      },
      {
        "number": 11,
        "text": "躯体性焦虑（指焦虑的生理症状）",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "轻度", "score": 1},
          {"text": "中度,有肯定的症状", "score": 2},
          {"text": "重度,症状严重,影响生活或需要处理", "score": 3},
          {"text": "严重影响生活和活动", "score": 4}
        ]
      },
      {
        "number": 12,
        "text": "胃肠道症状",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "食欲减退,但不需他人鼓励便自行进食", "score": 1},
          {"text": "进食需他人催促或请求和需要应用泻药或助消化药", "score": 2}
        ]
      },
      {
        "number": 13,
        "text": "全身症状",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "四肢,背部或颈部沉重感,背痛、头痛、肌肉疼痛、全身乏力或疲倦", "score": 1},
          {"text": "症状明显", "score": 2}
        ]
      },
      {
        "number": 14,
        "text": "性症状（指性欲减退,月经紊乱等）",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "轻度", "score": 1},
          {"text": "重度", "score": 2}
        ]
      },
      {
        "number": 15,
        "text": "疑病",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "对身体过分关注", "score": 1},
          {"text": "反复考虑健康问题", "score": 2},
          {"text": "有疑病妄想", "score": 3},
          {"text": "伴幻觉的疑病妄想", "score": 4}
        ]
      },
      {
        "number": 16,
        "text": "体重减轻:按病史评定",
        "options": [
          {"text": "体重没有减轻", "score": 0},
          {"text": "患者诉述可能有体重减轻", "score": 1},
          {"text": "肯定体重减轻", "score": 2},
          {"text": "一周内体重减轻超过0.5公斤", "score": 1},
          {"text": "一周内体重减轻超过1公斤", "score": 2}
        ]
      },
      {
        "number": 17,
        "text": "自知力",
        "options": [
          {"text": "知道自己有病,表现为抑郁", "score": 0},
          {"text": "知道自己有病,但归咎伙食太差,环境问题,工作过忙,病毒感染或需要休息", "score": 1},
          {"text": "完全否认有病", "score": 2}
        ]
      },
      {
        "number": 18,
        "text": "日夜变化（症状加重情况）",
        "options": [
          {"text": "早晚情绪无区别", "score": 0},
          {"text": "早晨或傍晚轻度加重", "score": 1},
          {"text": "早晨或傍晚严重加重", "score": 2}
        ]
      },
      {
        "number": 19,
        "text": "人格解体或现实解体（指非真实感或虚无妄想）",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "问及时才诉述", "score": 1},
          {"text": "自然诉述", "score": 2},
          {"text": "有虚无妄想", "score": 3},
          {"text": "伴幻觉的虚无妄想", "score": 4}
        ]
      },
      {
        "number": 20,
        "text": "偏执症状",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "有猜疑", "score": 1},
          {"text": "有牵连观念", "score": 2},
          {"text": "有关系妄想或被害妄想", "score": 3},
          {"text": "伴有幻觉的关系妄想或被害妄想", "score": 4}
        ]
      },
      {
        "number": 21,
        "text": "强迫症状（指强迫思维和强迫行为）",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "问及时才诉述", "score": 1},
          {"text": "自发诉述", "score": 2}
        ]
      },
      {
        "number": 22,
        "text": "能力减退感",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "仅于提问时方引出主观体验", "score": 1},
          {"text": "病人主动表示有能力减退感", "score": 2},
          {"text": "需鼓励-指导和安慰才能完成病室日常事务或个人卫生", "score": 3},
          {"text": "穿衣、梳洗、进食、铺床或个人卫生均需他人协助", "score": 4}
        ]
      },
      {
        "number": 23,
        "text": "绝望感",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "有时怀疑情况是否会好转,但解释后能接受", "score": 1},
          {"text": "持续感到没有希望,但解释后能接受", "score": 2},
          {"text": "对未来感到灰心-悲观和失望,解释后不能解除", "score": 3},
          {"text": "自动地反复诉述“我的病好不了啦” 或诸如此类的情况", "score": 4}
        ]
      },
      {
        "number": 24,
        "text": "自卑感",
        "options": [
          {"text": "没有", "score": 0},
          {"text": "仅在询问时诉述有自卑感不如他人", "score": 1},
          {"text": "自动地诉述有自卑感", "score": 2},
          {"text": "病人主动诉述自己一无是处或低人一等", "score": 3},
          {"text": "自卑感达妄想的程度,如“我是废物”等类似情况", "score": 4}
        ]
      }
    ],
    "results": [
      {
        "range": {"min": 0, "max": 7},
        "conclusion": "经过检测您【无抑郁症状】，希望您继续保持良好的心情。 （以上测试结果仅供参考）"
      },
      {
        "range": {"min": 8, "max": 20},
        "conclusion": "经过检测您【可能有抑郁】，为了您和家人的幸福，建议您联系我们，进行更专业的检测。（以上测试结果仅供参考）"
      },
      {
        "range": {"min": 21, "max": 35},
        "conclusion": "经过检测您【肯定有抑郁】，为了您和家人的幸福，建议您联系我们，进行更专业的检测。（以上测试结果仅供参考）"
      },
      {
        "range": {"min": 36, "max": 80},
        "conclusion": "经过检测您为【严重抑郁】，为了您和家人的幸福，建议您联系我们，进行更专业的检测。（以上测试结果仅供参考）"
      }
    ]
  }

--------------------------------------------------
文件路径: input\questionnaires\8艾森克人格问卷EPQ85成人版.json
--------------------------------------------------
{
    "title": "艾森克人格问卷 (EPQ-85成人版)",
    "description": "指导语：请对下列每一题目进行回答，在“是”或“否”字上打“√”，回答无所谓对错，这里没有对你不利的题目，请尽快回答，不要对每题的含义进行过多的思考。每题都要回答。",
    "questions": [
      {
        "number": 1,
        "text": "你是否有广泛的爱好",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 2,
        "text": "再做任何事情之前你是否都要先考虑一番",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 3,
        "text": "你的情绪时常波动么？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 4,
        "text": "当别人做了好事周围人却认为是你做的，你是否感到洋洋得意？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 5,
        "text": "你是一个健谈的人吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 6,
        "text": "你曾经无缘无故的感到自己可怜吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 7,
        "text": "你曾经有过因为贪心而获得分外的物质利益吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 8,
        "text": "晚上你是否会小心的把门锁好？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 9,
        "text": "你认为自己活泼吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 10,
        "text": "当你看到小孩或（动物）受折磨时是否会感到很难受？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 11,
        "text": "你是否时常担心你会说出（或做出）不应该说（或做）的事情？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 12,
        "text": "若你说过要做某件事，是否不管遇到什么困难都要把它做成？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 13,
        "text": "在愉快的聚会中，你是否尽情享受？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 14,
        "text": "你是一位易激怒的人吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 15,
        "text": "你是否有过自己做错了事反倒责备别人的时候？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 16,
        "text": "你喜欢会见陌生人么？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 17,
        "text": "你是否相信参加储蓄是一种好办法？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 18,
        "text": "你的感情是否容易受到伤害？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 19,
        "text": "你想服用有奇特效果或是有危险性的药物么？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 20,
        "text": "你是否时常感到“及其厌烦”？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 21,
        "text": "你曾多占、多得别人的东西(甚至是一针一线)吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 22,
        "text": "如果条件允许，你喜欢经常外出（旅行）么？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 23,
        "text": "对你所喜欢的人，你是否为取乐开过过头的玩笑么？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 24,
        "text": "你是否常因“自罪感”而烦恼？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 25,
        "text": "你是否谈论一些你自己毫无所知的事情？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 26,
        "text": "你是否宁愿看些书而不想去会见别人？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 27,
        "text": "有坏人想要害你么？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 28,
        "text": "你认为自己“神经过敏”吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 29,
        "text": "你的朋友多吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 30,
        "text": "你是个忧虑重重的人吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 31,
        "text": "你在儿童时代是否立即听从大人的吩咐而毫无怨言吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 32,
        "text": "你是一个无忧无虑，逍遥自在的人吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 33,
        "text": "有礼貌、爱整洁对你很重要吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 34,
        "text": "你是否担心将会发生可怕的事情？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 35,
        "text": "在结识新朋友时你通常是主动的吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 36,
        "text": "你觉得自己是一个非常敏感的人吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 37,
        "text": "和别人在一起的时候你是否不常说话？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 38,
        "text": "你是否觉得结婚是个框框，应该废除？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 39,
        "text": "你有时有点自吹自擂吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 40,
        "text": "在一个沉闷的场合，你能给大家添点生气吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 41,
        "text": "慢腾腾开车的司机是否使你讨厌？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 42,
        "text": "你担心自己的健康吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 43,
        "text": "你是否喜欢说笑话和谈论有趣的事情？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 44,
        "text": "你是否觉得大多数事情对你来说都是无所谓的？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 45,
        "text": "你小时候是否有过对待父母鲁莽、无礼的行为吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 46,
        "text": "你喜欢和别人打成一片、整天相处在一起吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 47,
        "text": "你失眠吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 48,
        "text": "你饭前必定先洗手吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 49,
        "text": "当别人问你话时你是否对答如流？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 50,
        "text": "你是否宁愿有富裕时间而早点动身去赴约会吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 51,
        "text": "你经常无缘无故感到疲倦或无精打采吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 52,
        "text": "在游戏或打牌时你经常作弊吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 53,
        "text": "你喜欢紧张的工作吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 54,
        "text": "你时常觉得自己的生活很单调吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 55,
        "text": "你曾经为了自己而利用过别人吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 56,
        "text": "你是否参加的活动太多而超过自己可能分配的时间？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 57,
        "text": "你是否有那么几个人时常躲着你?",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 58,
        "text": "你是否认为人们为保障自己的将来而精打细算勤俭时间太多了？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 59,
        "text": "你是否曾经想过去死吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 60,
        "text": "若你确知不会被发现时你会少付给人家钱吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 61,
        "text": "你能使一个联欢会开成功吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 62,
        "text": "你是否尽力使自己不粗鲁？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 63,
        "text": "一件使你为难的事情过去之后是否会使你烦恼好久？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 64,
        "text": "你是否坚持要按照你的想法办事？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 65,
        "text": "当你去乘火车时，你是否会最后一分钟到达？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 66,
        "text": "你是否神经质？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 67,
        "text": "你常感到寂寞吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 68,
        "text": "你的言行总是一致的吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 69,
        "text": "你有时喜欢玩弄动物吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 70,
        "text": "有人对你或你的工作吹毛求疵时，是否容易伤害你的积极性？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 71,
        "text": "你去赴约会或上班时，曾否迟到？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 72,
        "text": "你是否喜欢在你的周围有许多热闹和高兴的事情？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 73,
        "text": "你愿意让别人怕你吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 74,
        "text": "你是否有时兴致勃勃，有时却很慵懒不想动弹？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 75,
        "text": "你有时会把今天应做的事情拖到明天吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 76,
        "text": "别人是否认为你是生气勃勃的？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 77,
        "text": "别人是否对你说过许多谎话？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 78,
        "text": "你是否对有些事情易性急生气？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 79,
        "text": "若你有错误，是否愿意承认？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 80,
        "text": "你是一个整洁严谨，有条不紊的人吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 81,
        "text": "在公园里或马路上，你是否总把果皮或废纸扔到垃圾箱里？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 82,
        "text": "遇到为难的事情，你是否拿不定主意？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 83,
        "text": "你是否有过随口骂人的时候？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 84,
        "text": "若你乘车或坐飞机外出时，你是否担心会碰撞或出意外？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      },
      {
        "number": 85,
        "text": "你是一个爱交往的人吗？",
        "options": [
          {"text": "是", "value": "yes"},
          {"text": "否", "value": "no"}
        ]
      }
    ],
    "scoring_rules": {
      "E": {
        "yes": [1, 5, 9, 13, 16, 22, 29, 32, 35, 40, 43, 46, 49, 53, 56, 61, 72, 76, 85],
        "no": [26, 37]
      },
      "N": {
        "yes": [3, 6, 11, 14, 18, 20, 24, 28, 30, 34, 36, 42, 47, 51, 54, 59, 63, 66, 67, 70, 74, 78, 82, 84],
        "no": []
      },
      "P": {
        "yes": [19, 23, 27, 38, 41, 44, 57, 58, 65, 69, 73, 77],
        "no": [2, 8, 10, 17, 33, 50, 62, 80]
      },
      "L": {
        "yes": [12, 31, 48, 68, 79, 81],
        "no": [4, 7, 15, 21, 25, 39, 45, 52, 55, 60, 64, 71, 75, 83]
      }
    },
    "interpretation_rules": {
      "E": [
        {"range": {"min": 15, "max": 21}, "conclusion": "人格外向型：可能是好交际，渴望刺激和冒险，情感易于冲动。在多人环境中表现活跃。"},
        {"range": {"min": 0, "max": 8}, "conclusion": "人格内向型：如好静，富于内省，不喜欢刺激，喜欢有秩序的生活方式，情绪比较稳定。"}
      ],
      "N": [
        {"range": {"min": 14, "max": 24}, "conclusion": "高神经质：表示情绪不稳定型，焦虑、忧心忡忡、常郁郁不乐，有强烈情绪反应，甚至出现不够理智的行为。"},
        {"range": {"min": 0, "max": 9}, "conclusion": "情绪稳定型：冷静，平和，情绪调节能力强。"}
      ],
      "P": [
        {"range": {"min": 8, "max": 20}, "conclusion": "高精神质：表示可能是精神质，孤独、不关心他人，攻击性，反社会倾向，缺乏同理心与非传统思维，难以适应外部环境，不近人情，与别人不友好，喜欢寻衅搅扰，喜欢干奇特的事情，并且不顾危险。"},
        {"range": {"min": 0, "max": 7}, "conclusion": "低精神质：温和，富有同情心、遵守规范。"}
      ],
      "L": [
        {"range": {"min": 18, "max": 20}, "conclusion": "高掩饰性：表示被试有掩饰倾向，测验结果可能失真。"}
      ]
    },
    "combinations_interpretation": [
       {"type": "盗窃案-外向不稳定攻击焦虑", "description": "针对盗窃案，外向的，情绪不稳定型，攻击性，焦虑的人的讯问策略..."},
       {"type": "盗窃案-外向稳定温和", "description": "针对盗窃案，外向的，情绪稳定的，情绪调节能力强，温和，富有同情心的人的讯问策略..."},
       {"type": "盗窃案-内向不稳定反社会", "description": "针对盗窃案，内向的，情绪不稳定型，孤独的，反社会倾向，与别人不友好，喜欢寻衅的人的讯问策略..."},
       {"type": "盗窃案-内向稳定温和", "description": "针对盗窃案，内向的，情绪比较稳定，温和，富有同情心，喜欢有秩序的生活方式的人的讯问策略..."},
       {"type": "盗窃案-逃避不稳定攻击焦虑", "description": "针对盗窃案，逃避的，情绪不稳定型，攻击性，焦虑的人的讯问策略..."},
       {"type": "盗窃案-逃避稳定反社会", "description": "针对盗窃案，逃避的，情绪比较稳定，孤独的，反社会倾向，与别人不友好的人的讯问策略..."}
       
    ]
  }

--------------------------------------------------
文件路径: input\questionnaires\9开心测试.json
--------------------------------------------------
{
    "title": "开心测试",
    "questions": [
      {
        "number": 1,
        "text": "你今天开心吗？",
        "options": [
          {
            "text": "开心",
            "score": 1
          },
          {
            "text": "悲伤",
            "score": 0
          }
        ]
      }
    ]
  }

--------------------------------------------------
文件路径: output\descriptions\testpic1_desc.txt
--------------------------------------------------
这张图片是一幅手绘的室内设计草图，展示了一个现代风格的客厅。以下是详细描述：

1. **布局和家具**：
   - 客厅中央有一张长方形的茶几，上面放置了一些装饰品。
   - 茶几两侧各有一张沙发，沙发上摆放着几个靠垫。
   - 沙发后面是一个带有多个隔层的电视柜，柜子上放着一台电视机。

2. **装饰元素**：
   - 左侧墙角有一盆大型绿植，增加了空间的自然气息。
   - 墙上有两扇大窗户，窗外可以看到一些植物，使室内光线充足且明亮。
   - 天花板上悬挂着一组吊灯，由多个小灯组成，提供照明。

3. **墙面和地板**：
   - 墙面简洁，颜色为浅色系，给人一种干净、明亮的感觉。
   - 地板上铺有地毯，地毯的图案与整体风格相协调。

4. **其他细节**：
   - 右侧墙上有一个壁炉，旁边放置了一把单人椅和一个小边桌，营造出一个舒适的阅读或休息角落。
   - 整个房间的设计注重线条感和对称性，体现了现代简约风格。

这幅草图通过细致的线条和阴影表现了空间的层次感和立体感，展示了设计师对空间布局和装饰细节的精心构思。

--------------------------------------------------
文件路径: output\reports\testpic1_report.txt
--------------------------------------------------
### 心理学分析报告

#### **基本信息**
- **被测者姓名**: ss  
- **性别**: 女  
- **年龄**: 43岁  
- **量表类型**: ParentChild（亲子关系量表）  
- **总得分**: 20/20  

---

#### **量表解读**

##### **量表背景**
ParentChild量表主要用于评估个体在亲子关系中的满意度、情感联结以及潜在的冲突或压力。该量表通过多个维度（如沟通质量、情感支持、角色期待等）来衡量亲子关系的健康程度。

##### **得分范围与解释**
- **低分（0-5）**: 表明亲子关系中存在明显的矛盾或不和谐，可能缺乏有效沟通和情感支持。
- **中等分数（6-15）**: 表明亲子关系较为平衡，但可能存在一些需要改进的地方。
- **高分（16-20）**: 表明亲子关系非常和谐，沟通顺畅，情感支持充足，双方对彼此的角色期待基本一致。

**ss的得分（20/20）**属于量表的最高分段，这表明她在亲子关系中体验到了极高的满足感和安全感。她可能认为自己与孩子之间的互动是积极且充满爱的，同时在情感支持、角色分工和沟通方面都达到了理想状态。

---

#### **图片描述的心理学解读**

从手绘室内设计草图中可以提取出以下心理特征：

1. **空间布局与秩序感**：
   - 草图展示了现代简约风格的客厅，注重线条感和对称性。这种对空间布局的精心设计可能反映了ss追求秩序和条理性的生活态度。她可能倾向于创造一个整洁、有序的家庭环境，以促进家庭成员之间的舒适感和归属感。
   
2. **自然元素的引入**：
   - 左侧墙角的大绿植和窗外的植物表明ss重视自然与生活的关系。这可能暗示她希望为家人营造一个放松、温馨的氛围，同时也体现了她对环境保护和生活质量的关注。

3. **功能性与情感性的结合**：
   - 客厅中央的茶几和沙发形成了一个交流区域，而右侧的壁炉角落则提供了一个独立的阅读或休息空间。这种设计既满足了家庭成员共同活动的需求，也兼顾了个人的私密性和独立性。这可能反映出ss在家庭关系中既注重集体互动，也尊重个体差异。

4. **光线与色彩的选择**：
   - 浅色系墙面和充足的自然光使整个空间显得明亮而开放。这可能象征着ss内心的积极情绪和对未来的乐观态度。她可能希望通过这样的设计传递一种轻松、愉悦的家庭氛围。

---

#### **综合心理状态分析**

结合量表得分和图片描述，ss的心理状态可以总结如下：

1. **亲子关系高度和谐**：
   - 高分表明ss在亲子关系中感到极大的满足和幸福。她可能是一个善于倾听、理解并支持孩子的家长，同时也能够清晰地表达自己的期望和需求。
   
2. **追求秩序与平衡**：
   - 图片中的对称性和功能分区显示ss可能在生活中追求一种平衡的状态，既关注家庭的整体氛围，也尊重每个成员的独特需求。

3. **注重情感联结**：
   - 绿植、自然光和舒适的家具选择表明ss重视家庭成员之间的情感联结，并努力为他们创造一个温暖、支持性的环境。

4. **潜在的控制倾向**：
   - 尽管整体表现良好，但过于强调秩序和对称性可能暗示ss有一定的控制倾向。她可能会希望一切按照自己的规划进行，这有时可能导致轻微的压力或焦虑，尤其是在面对不确定性时。

---

#### **建议**

1. **保持开放心态**：
   - 虽然ss在亲子关系中表现出色，但仍需注意避免过度控制。可以尝试更多地倾听孩子的想法和感受，允许他们在某些领域拥有自主权。

2. **灵活应对变化**：
   - 在追求秩序的同时，也要学会接受生活中的不确定性和灵活性。可以通过参与新的活动或尝试不同的生活方式来增强适应能力。

3. **关注自我需求**：
   - 在为家庭付出的同时，不要忽视自己的情感需求。可以通过培养兴趣爱好或与朋友交流来丰富自己的生活。

4. **持续优化沟通**：
   - 即使亲子关系已经非常和谐，也可以通过定期的家庭会议或情感分享活动进一步巩固彼此的联系。

---

#### **总结**

ss展现出一位成熟、负责且富有爱心的家长形象。她的家庭观念强烈，注重细节和情感联结，同时具备较高的心理调适能力。然而，适度放松对秩序的要求、增加灵活性和自我关怀将有助于进一步提升她的幸福感和家庭氛围。

--------------------------------------------------
文件路径: src\ai_utils.py
--------------------------------------------------
# src/ai_utils.py
import os
import json
# 移除了 import sqlite3
from datetime import datetime
import sys
import logging

# --- 路径设置和模块导入 (保持不变) ---
SRC_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT_FROM_SRC = os.path.dirname(SRC_DIR)
if PROJECT_ROOT_FROM_SRC not in sys.path:
    sys.path.insert(0, PROJECT_ROOT_FROM_SRC)

try:
    # 移除了 from .data_handler import DataHandler 的导入，因为此文件不再直接使用它
    # 修正：确认导入路径
    from src.image_processor import ImageProcessor
    from src.report_generator import ReportGenerator
    print("[ai_utils] 成功相对导入 ImageProcessor 和 ReportGenerator。")
except ImportError:
    try:
        # 移除了 from data_handler import DataHandler 的导入
        from image_processor import ImageProcessor
        from report_generator import ReportGenerator
        print("[ai_utils] 成功直接导入 ImageProcessor 和 ReportGenerator (后备)。")
    except ImportError as e:
        print(f"[ai_utils] CRITICAL ERROR: 无法导入必要的同级模块: {e}", file=sys.stderr)
        # 如果在 Celery 任务中，这可能导致任务失败
        raise e

# --- calculate_score_and_interpret 函数 (添加 HappyTest 逻辑) ---
def calculate_score_and_interpret(scale_type, scale_answers, task_logger=None):
    """
    根据量表答案计算分数，并根据量表类型提供基本解释。
    使用提供的 logger 实例进行日志记录。

    Args:
        scale_type (str): 表示量表类型的代码 (例如, 'SAS', 'SDS').
        scale_answers (dict): 问题答案字典 { 'q1': 'score_value', ... }.
        task_logger (logging.Logger, optional): 要使用的 Logger 实例. 默认为 None.

    Returns:
        tuple: (calculated_score, interpretation_string)
    """
    # 获取 logger，如果未提供则使用默认 logger
    current_logger = task_logger or logging.getLogger(__name__)
    if task_logger is None:
        current_logger.warning("未向 calculate_score_and_interpret 提供 task_logger，使用默认日志记录器。")
        # 确保有基本的日志处理器，以防万一
        if not logging.getLogger().hasHandlers():
            logging.basicConfig(level=logging.INFO)

    # 处理没有答案的情况
    if not scale_answers:
        current_logger.info(f"量表类型 '{scale_type}' 的答案为空。")
        return 0, "无量表答案"

    calculated_score = 0
    try:
        # 确保值存在且可以转换为数字
        valid_scores = []
        for key, value in scale_answers.items():
            if value is not None:
                try:
                    # 尝试转换为浮点数，然后转整数（如果可能）
                    score_float = float(value)
                    # 检查是否为整数
                    if score_float.is_integer():
                         valid_scores.append(int(score_float))
                    else:
                         valid_scores.append(score_float)
                except (ValueError, TypeError):
                    current_logger.warning(f"无法将答案 '{key}':'{value}' 转换为数字，已忽略。")

        # 如果没有任何有效分数
        if not valid_scores:
             current_logger.warning(f"在类型 {scale_type} 的量表答案中未找到有效的数字分数: {scale_answers}")
             return 0, f"量表 '{scale_type}' 无有效得分项"

        # 计算总分
        calculated_score = sum(valid_scores)
        current_logger.debug(f"计算得到的分数: {calculated_score} (来自: {valid_scores})")
    except Exception as e:
        current_logger.error(f"从答案 {scale_answers} 计算分数时出错: {e}", exc_info=True)
        # 返回错误标记和信息
        return "计算错误", f"分数计算出错: {e}"

    # --- 量表解释逻辑 ---
    interpretation = f"量表 '{scale_type}' 总得分: {calculated_score}."
    current_logger.info(f"开始为量表 '{scale_type}' (得分: {calculated_score}) 生成解释。")

    try:
        # --- 添加或更新解释逻辑 ---
        if scale_type == 'SAS': # Anxiety Self-Rating Scale
            standard_score = int(calculated_score * 1.25)
            interpretation = f"量表 '{scale_type}' 原始得分: {calculated_score}, 标准分: {standard_score}."
            if standard_score >= 70: interpretation += " (重度焦虑水平)"
            elif standard_score >= 60: interpretation += " (中度焦虑水平)"
            elif standard_score >= 50: interpretation += " (轻度焦虑水平)"
            else: interpretation += " (焦虑水平在正常范围)"
        elif scale_type == 'SDS': # Depression Self-Rating Scale
             standard_score = int(calculated_score * 1.25)
             interpretation = f"量表 '{scale_type}' 原始得分: {calculated_score}, 标准分: {standard_score}."
             if standard_score >= 73: interpretation += " (重度抑郁水平)" # 按量表文件界限
             elif standard_score >= 63: interpretation += " (中度抑郁水平)"
             elif standard_score >= 53: interpretation += " (轻度抑郁水平)"
             else: interpretation += " (抑郁水平在正常范围)"
        elif scale_type == 'ParentChild':
            if calculated_score >= 80: interpretation += " (亲子关系非常和谐)"
            elif calculated_score >= 60: interpretation += " (亲子关系良好)"
            else: interpretation += " (亲子关系可能存在挑战，建议关注)"
        elif scale_type == 'Personality':
            if calculated_score >= 101: interpretation += " (倾向：积极热情)"
            elif calculated_score >= 90: interpretation += " (倾向：领导人特质)"
            elif calculated_score >= 79: interpretation += " (倾向：感性)"
            elif calculated_score >= 60: interpretation += " (倾向：理性&淡定)"
            elif calculated_score >= 40: interpretation += " (倾向：双重&孤寂)"
            else: interpretation += " (倾向：现实&自我)"
            interpretation += " (具体解释需参考原始量表得分范围)"
        elif scale_type == 'InterpersonalRelationship':
            if calculated_score <= 8: interpretation += " (人际关系困扰较少)"
            elif calculated_score <= 14: interpretation += " (人际关系存在一定困扰)"
            else: interpretation += " (人际关系困扰较严重)"
        elif scale_type == 'EmotionalStability':
             if calculated_score <= 20: interpretation += " (情绪稳定，自信心强)"
             elif calculated_score <= 40: interpretation += " (情绪基本稳定，但可能较为深沉或消极)"
             else: interpretation += " (情绪不稳定，可能需要关注)"
        elif scale_type == 'HAMD24':
            if calculated_score >= 36: interpretation += " (重度抑郁)"
            elif calculated_score >= 21: interpretation += " (肯定有抑郁)"
            elif calculated_score >= 8: interpretation += " (可能有抑郁)"
            else: interpretation += " (无抑郁症状)"
        elif scale_type == 'EPQ85':
             interpretation = f"艾森克人格问卷 (EPQ-85)，总得分无直接意义，需分析 P, E, N, L 各维度得分。"
             # 实际分析需要在 generate_report_content 中单独处理 EPQ85
        # +++ 添加 HappyTest 的解释 +++
        elif scale_type == 'HappyTest':
             if calculated_score == 1:
                  interpretation += " (初步判断：用户表示今天很开心。)"
             elif calculated_score == 0:
                  interpretation += " (初步判断：用户表示今天感到悲伤。)"
             else:
                  interpretation += " (得分异常，无法解释。)" # 处理意外得分
        # --- HappyTest 解释结束 ---
        else:
            # 未知量表类型
            current_logger.warning(f"未找到量表类型 '{scale_type}' 的特定解释规则。")
            interpretation += " (无特定解释规则)"

    except Exception as e:
        current_logger.error(f"量表 {scale_type} 解释过程中出错: {e}", exc_info=True)
        interpretation += " (解释规则应用出错)"

    current_logger.info(f"量表 '{scale_type}' 解释完成: '{interpretation}'")
    return calculated_score, interpretation


# --- 重命名并重构核心函数 ---
def generate_report_content(submission_data: dict, config: dict, task_logger: logging.Logger) -> str:
    """
    根据传入的评估数据和配置，生成报告文本。不再直接操作数据库。

    Args:
        submission_data (dict): 从数据库异步加载的评估数据字典.
        config (dict): 应用程序配置字典 (来自 settings.model_dump()).
        task_logger (logging.Logger): 用于记录日志的 logger 实例.

    Returns:
        str: 生成的报告文本或错误信息字符串.
    """
    logger = task_logger
    submission_id = submission_data.get("id", "未知ID")
    logger.info(f"开始为评估 ID: {submission_id} 生成报告内容")

    # --- 提取数据 ---
    image_filename = submission_data.get('image_path') # 这是存储在 DB 中的相对路径或文件名
    image_full_path = None
    if image_filename:
        # 从配置中获取上传目录
        uploads_dir = config.get("UPLOADS_DIR")
        if uploads_dir and os.path.isdir(uploads_dir):
            image_full_path = os.path.join(uploads_dir, image_filename)
            logger.info(f"将使用的图片文件路径: {image_full_path}")
        elif not uploads_dir:
            logger.warning(f"配置中未找到 UPLOADS_DIR，无法定位图片文件: {image_filename}")
        else: # uploads_dir 存在但不是目录
             logger.warning(f"配置的 UPLOADS_DIR '{uploads_dir}' 不是有效目录，无法定位图片文件: {image_filename}")

    scale_type = submission_data.get('questionnaire_type')
    scale_answers_json = submission_data.get('questionnaire_data')
    # 确保 basic_info 包含所有可能的键，并提供默认值
    basic_info = {
        "subject_name": submission_data.get("subject_name", '未提供'),
        "gender": submission_data.get("gender", '未提供'),
        "id_card": submission_data.get("id_card", '未提供'),
        "age": submission_data.get("age", '未提供'),
        "occupation": submission_data.get("occupation", '未提供'),
        "case_name": submission_data.get("case_name", '未提供'),
        "case_type": submission_data.get("case_type", '未提供'),
        "identity_type": submission_data.get("identity_type", '未提供'),
        "person_type": submission_data.get("person_type", '未提供'),
        "marital_status": submission_data.get("marital_status", '未提供'),
        "children_info": submission_data.get("children_info", '未提供'),
        "criminal_record": submission_data.get("criminal_record", 0), # 默认 0 (无)
        "health_status": submission_data.get("health_status", '未提供'),
        "phone_number": submission_data.get("phone_number", '未提供'),
        "domicile": submission_data.get("domicile", '未提供')
    }
    # 确保 'name' 键存在，用于报告模板
    basic_info['name'] = basic_info.get('subject_name', '未知')

    logger.debug(f"用于报告生成的基础信息 (ID {submission_id}): {basic_info}")

    # --- 准备 AI 配置 ---
    ai_config = config.copy()
    # 确保 api_key 存在
    if 'api_key' not in ai_config:
        api_key_from_env = config.get('DASHSCOPE_API_KEY')
        if api_key_from_env:
            logger.info("复制 DASHSCOPE_API_KEY 到 'api_key' 以供 AI 处理器使用。")
            ai_config['api_key'] = api_key_from_env
        else:
             logger.error(f"CRITICAL: AI 处理器的 API Key 未在配置中找到! (ID: {submission_id})")
             # 返回错误信息，因为无法继续
             return "错误：AI 服务配置不完整 (缺少 API Key)"

    # --- 处理图片 ---
    image_description = "未提供图片"
    if image_full_path:
        if os.path.exists(image_full_path):
            logger.info(f"开始处理图片: {image_full_path}")
            try:
                # 使用配置初始化 ImageProcessor
                image_processor = ImageProcessor(ai_config)
                image_description = image_processor.process_image(image_full_path)
                logger.info(f"图片描述生成成功 (ID {submission_id})。描述片段: {image_description[:100]}...")
            except FileNotFoundError:
                 logger.error(f"图片文件在处理时未找到: {image_full_path}")
                 image_description = "图片文件未找到"
            except Exception as img_err:
                logger.error(f"图片处理失败 (ID {submission_id}): {img_err}", exc_info=True)
                image_description = f"图片处理错误: {img_err}"
        else:
            logger.warning(f"图片路径存在但文件在处理时未找到: {image_full_path}")
            image_description = "图片文件未找到"
    else:
        logger.info(f"评估 ID {submission_id} 未提供图片路径。")

    # --- 处理量表数据 ---
    scale_answers = None
    calculated_score = 0 # Default score
    scale_interpretation = "无量表数据" # Default interpretation

    if scale_type and scale_answers_json:
        logger.info(f"开始处理量表数据，类型: {scale_type} (ID {submission_id})")
        try:
            # 尝试解析 JSON 字符串
            scale_answers = json.loads(scale_answers_json) # 期望是字典 {'q1': 'score', ...}

            # 检查解析结果是否为字典
            if isinstance(scale_answers, dict):
                 # 特殊处理 EPQ85，因为它需要计算四个维度
                 if scale_type == 'EPQ85':
                      # TODO: 实现 EPQ85 的计分逻辑
                      # 这需要访问 EPQ85 的 JSON 文件来获取计分规则
                      # 假设有一个辅助函数 `calculate_epq85_scores(scale_answers)`
                      # 返回 {'P': score_p, 'E': score_e, 'N': score_n, 'L': score_l, 'interpretation': '...'}
                      # epq_results = calculate_epq85_scores(scale_answers, logger)
                      # calculated_score = epq_results # 或者只用某个主维度
                      # scale_interpretation = epq_results['interpretation']
                      logger.warning(f"EPQ85 量表计分逻辑尚未在此函数中完全实现 (ID: {submission_id})。")
                      calculated_score = "N/A" # 标记为不适用总分
                      scale_interpretation = "EPQ85 量表结果需单独分析各维度。"
                 else:
                     # 对于其他量表，使用通用计分函数
                     calculated_score, scale_interpretation = calculate_score_and_interpret(
                         scale_type, scale_answers, task_logger=logger
                     )
                 logger.info(f"量表处理完成: Score={calculated_score}, Interpretation='{scale_interpretation}' (ID {submission_id})")
            else:
                 # 如果 JSON 解析后不是字典
                 logger.error(f"解析后的量表答案不是字典类型 (ID {submission_id}): {type(scale_answers)}")
                 scale_interpretation = "量表答案格式错误 (非字典)"
                 scale_answers = None # 重置为 None，以免传递错误类型给报告生成器

        except json.JSONDecodeError as json_err:
            # JSON 字符串本身格式错误
            logger.error(f"量表答案 JSON 解析失败 (ID {submission_id}): {json_err}. JSON: {scale_answers_json[:200]}...")
            scale_interpretation = "量表答案格式错误 (JSON 解析失败)"
            scale_answers = None
        except Exception as scale_err:
            # 其他处理错误（如计分函数内部错误）
            logger.error(f"量表数据处理失败 (ID {submission_id}): {scale_err}", exc_info=True)
            scale_interpretation = f"量表处理错误: {scale_err}"
            scale_answers = None # 重置
    elif scale_type:
         # 有类型但无答案数据
         logger.warning(f"提供了量表类型 '{scale_type}' 但无答案数据 (ID {submission_id}).")
         scale_interpretation = f"量表 '{scale_type}' 未提供答案"
    else:
         # 没有提供量表类型
         logger.info(f"评估 ID {submission_id} 未提供量表类型.")

    # --- 调用 LLM 生成报告 ---
    logger.info(f"开始调用 LLM 生成报告 (ID {submission_id})")
    final_report_text = None
    try:
        # 使用配置初始化 ReportGenerator
        report_generator = ReportGenerator(ai_config)
        final_report_text = report_generator.generate_report(
             description=image_description,
             questionnaire=scale_answers, # 传递解析后的字典或 None
             subject_info=basic_info,
             questionnaire_type=scale_type if scale_type else "未指定", # 提供默认值
             score=calculated_score, # 可能是数字，也可能是 "N/A" (如 EPQ)
             scale_interpretation=scale_interpretation # 使用上面处理后的解释
         )
        # 检查 ReportGenerator 的返回值
        if final_report_text is None:
             # ReportGenerator 应该返回字符串，即使是错误信息
             logger.error(f"报告生成器意外返回了 None (ID: {submission_id})")
             raise ValueError("报告生成器意外返回了 None") # 抛出错误以便捕获
        logger.info(f"LLM 报告生成成功 (ID {submission_id}, 长度: {len(final_report_text)})")

    except Exception as report_err:
        logger.error(f"LLM 报告生成失败 (ID {submission_id}): {report_err}", exc_info=True)
        # 返回具体的错误信息，而不是仅仅标记失败
        final_report_text = f"报告生成错误: {type(report_err).__name__} - {str(report_err)}"

    # --- 返回最终文本 ---
    # 注意：此函数不再负责数据库更新
    return final_report_text

# 移除旧的 process_data_and_generate_report_sync 函数定义（如果它还存在）

--------------------------------------------------
文件路径: src\api_tester.py
--------------------------------------------------
# 文件路径: src/api_tester.py
import base64
import os
import sqlite3
import json
from openai import OpenAI
from src.utils import setup_logging
from src.image_processor import ImageProcessor
from src.report_generator import ReportGenerator

class APITester:
    def __init__(self, config):
        """初始化 API 测试模块"""
        self.logger = setup_logging()
        self.config = config
        
        # 统一客户端配置
        self.api_key = config["api_key"]
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
        )
        
        # 模型名称
        self.vision_model = "qwen-vl-max-latest"
        self.text_model = config["text_model"]  # 从配置中读取，例如 "qwen-plus"
        
        # 初始化数据库路径
        self.db_path = "psychology_analysis.db"
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        # 初始化测试用模块
        self.image_processor = ImageProcessor(config)
        self.report_generator = ReportGenerator(config)

    def setup_test_data(self):
        """为测试准备数据，插入测试用图片路径和量表数据"""
        self.logger.info("准备测试数据...")
        
        # 连接数据库
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # 清空现有数据（仅用于测试）
            cursor.execute("DELETE FROM analysis_data")
            
            # 插入测试数据
            test_image_path = os.path.join(self.project_root, "input", "images", "TestPic.jpg")
            subject_info = {"name": "测试用户", "age": 20, "gender": "男"}
            questionnaire_data = {"q1": "yes", "q2": "no", "q3": "sometimes"}
            
            cursor.execute('''
                INSERT INTO analysis_data (image_path, subject_name, age, gender, questionnaire_data)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                test_image_path,
                subject_info["name"],
                subject_info["age"],
                subject_info["gender"],
                json.dumps(questionnaire_data)
            ))
            
            # 插入只有量表数据的记录（无对应图片）
            cursor.execute('''
                INSERT INTO analysis_data (image_path, subject_name, age, gender, questionnaire_data)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                "no_image.jpg",  # 虚拟图片路径
                "无图片用户",
                25,
                "女",
                json.dumps({"q1": "no", "q2": "yes", "q3": "often"})
            ))
            
            conn.commit()
        self.logger.info("测试数据准备完成")

    def cleanup_test_data(self):
        """清理测试数据"""
        self.logger.info("清理测试数据...")
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("DELETE FROM analysis_data")
            conn.commit()
        self.logger.info("测试数据清理完成")

    def test_vision_api(self):
        """测试图像处理模型的 API 连通性"""
        self.logger.info("开始测试图像处理模型 API...")
        
        # 使用 input/images/TestPic.jpg 进行测试
        test_image_path = os.path.join(self.project_root, "input", "images", "TestPic.jpg")
        
        if not os.path.exists(test_image_path):
            self.logger.error(f"测试图片 {test_image_path} 不存在，请确保文件已放置在正确位置")
            return False
        
        # 将图片转为 base64 编码
        with open(test_image_path, "rb") as image_file:
            image_base64 = base64.b64encode(image_file.read()).decode("utf-8")

        # 构造消息内容
        messages = [
            {"role": "system", "content": [{"type": "text", "text": "You are a helpful assistant."}]},
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}},
                    {"type": "text", "text": "请简要描述这张图片的内容，用于API测试。"}
                ]
            }
        ]

        try:
            completion = self.client.chat.completions.create(
                model=self.vision_model,
                messages=messages,
            )
            description = completion.choices[0].message.content
            self.logger.info("图像模型 API 测试成功！")
            self.logger.info(f"测试图片描述: {description}")
            return True
        except Exception as e:
            self.logger.error(f"图像模型 API 测试失败: {str(e)}")
            return False

    def test_text_api(self):
        """测试文本生成模型的 API 连通性"""
        self.logger.info("开始测试文本生成模型 API...")
        
        # 构造测试消息
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "这是一个API连通性测试，请回复‘测试成功’。"}
        ]

        try:
            completion = self.client.chat.completions.create(
                model=self.text_model,
                messages=messages,
            )
            text_output = completion.choices[0].message.content
            self.logger.info("文本生成模型 API 测试成功！")
            self.logger.info(f"测试输出: {text_output}")
            return True
        except Exception as e:
            self.logger.error(f"文本生成模型 API 测试失败: {str(e)}")
            return False

    def test_report_with_questionnaire_only(self):
        """测试只有量表数据时是否能生成报告"""
        self.logger.info("开始测试只有量表数据时的报告生成...")
        
        # 从数据库加载只有量表数据的记录
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT subject_name, age, gender, questionnaire_data FROM analysis_data WHERE image_path = ?", ("no_image.jpg",))
            result = cursor.fetchone()
            if not result:
                self.logger.error("未找到只有量表数据的测试记录")
                return False
            
            subject_info = {"name": result[0], "age": result[1], "gender": result[2]}
            questionnaire_data = json.loads(result[3]) if result[3] else None
            
            if not questionnaire_data:
                self.logger.error("量表数据为空，无法生成报告")
                return False

        # 使用空描述生成报告
        description = "无图片输入，仅基于量表数据生成报告"
        try:
            report = self.report_generator.generate_report(description, questionnaire_data, subject_info)
            self.logger.info("仅使用量表数据生成报告成功！")
            self.logger.info(f"生成的报告片段: {report[:100]}...")  # 只打印前100个字符
            return True
        except Exception as e:
            self.logger.error(f"仅使用量表数据生成报告失败: {str(e)}")
            return False

    def test_full_pipeline(self):
        """测试完整流程：图像 + 量表数据"""
        self.logger.info("开始测试完整流程（图像 + 量表数据）...")
        
        # 使用 TestPic.jpg 进行测试
        test_image_path = os.path.join(self.project_root, "input", "images", "TestPic.jpg")
        
        if not os.path.exists(test_image_path):
            self.logger.error(f"测试图片 {test_image_path} 不存在，请确保文件已放置在正确位置")
            return False
        
        # 步骤1: 图像识别
        try:
            description = self.image_processor.process_image(test_image_path)
            self.logger.info("图像描述生成成功")
            self.logger.info(f"描述片段: {description[:100]}...")  # 只打印前100个字符
        except Exception as e:
            self.logger.error(f"图像描述生成失败: {str(e)}")
            return False
        
        # 步骤2: 从数据库加载量表数据
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT subject_name, age, gender, questionnaire_data FROM analysis_data WHERE image_path = ?", (test_image_path,))
            result = cursor.fetchone()
            if not result:
                self.logger.error(f"图片 {test_image_path} 无对应量表数据")
                return False
            
            subject_info = {"name": result[0], "age": result[1], "gender": result[2]}
            questionnaire_data = json.loads(result[3]) if result[3] else None
            
            if not questionnaire_data:
                self.logger.error("量表数据为空，无法生成报告")
                return False

        # 步骤3: 生成报告
        try:
            report = self.report_generator.generate_report(description, questionnaire_data, subject_info)
            self.logger.info("完整流程生成报告成功！")
            self.logger.info(f"生成的报告片段: {report[:100]}...")  # 只打印前100个字符
            return True
        except Exception as e:
            self.logger.error(f"完整流程生成报告失败: {str(e)}")
            return False

    def run_all_tests(self):
        """运行所有 API 测试"""
        self.logger.info("开始运行所有 API 测试...")
        
        # 准备测试数据
        self.setup_test_data()
        
        # 执行所有测试
        vision_result = self.test_vision_api()
        text_result = self.test_text_api()
        questionnaire_only_result = self.test_report_with_questionnaire_only()
        full_pipeline_result = self.test_full_pipeline()
        
        # 清理测试数据
        self.cleanup_test_data()
        
        # 汇总测试结果
        if all([vision_result, text_result, questionnaire_only_result, full_pipeline_result]):
            self.logger.info("所有 API 测试通过！")
            return True
        else:
            self.logger.error("部分或全部 API 测试未通过，请检查日志")
            self.logger.error(f"测试结果 - 图像API: {vision_result}, 文本API: {text_result}, 仅量表: {questionnaire_only_result}, 完整流程: {full_pipeline_result}")
            return False

if __name__ == "__main__":
    import yaml
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    config_path = os.path.join(project_root, "config", "config.yaml")
    with open(config_path, "r", encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    tester = APITester(config)
    tester.run_all_tests()

--------------------------------------------------
文件路径: src\data_entry.py
--------------------------------------------------
# 文件路径: src/data_entry.py
import os
import sys

# 获取项目根目录（PsychologyAnalysis/）
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
# 将项目根目录添加到模块搜索路径
sys.path.append(project_root)

from src.data_handler import DataHandler

def main():
    # 初始化 DataHandler
    handler = DataHandler(db_path=os.path.join(project_root, "psychology_analysis.db"))

    # 定义测试数据
    data_entries = [
        {
            "image_path": os.path.join(project_root, "input\images\image1.jpg"),
            "subject_info": {"name": "张三", "age": 13, "gender": "男"},
            "questionnaire_data": {"q1": "yes", "q2": "no", "q3": "sometimes"}
        },
        {
            "image_path": os.path.join(project_root, "input\images\TestPic.jpg"),
            "subject_info": {"name": "李四", "age": 15, "gender": "女"},
            "questionnaire_data": {"q1": "no", "q2": "yes", "q3": "often"}
        },
        {
            "image_path": "no_image.jpg",  # 没有对应图片，仅量表数据
            "subject_info": {"name": "王五", "age": 20, "gender": "男"},
            "questionnaire_data": {"q1": "yes", "q2": "yes", "q3": "rarely"}
        }
    ]

    # 录入数据
    for entry in data_entries:
        image_path = entry["image_path"]
        subject_info = entry["subject_info"]
        questionnaire_data = entry["questionnaire_data"]

        # 检查图片文件是否存在（如果 image_path 不是虚拟路径）
        if image_path != "no_image.jpg" and not os.path.exists(image_path):
            print(f"警告: 图片文件 {image_path} 不存在，跳过录入")
            continue

        try:
            handler.save_data(image_path, subject_info, questionnaire_data)
            print(f"成功录入数据: 图片路径 {image_path}, 被测者 {subject_info['name']}")
        except Exception as e:
            print(f"录入数据失败: 图片路径 {image_path}, 错误: {str(e)}")

if __name__ == "__main__":
    main()
    from src.data_handler import check_db_content
    check_db_content(os.path.join(project_root, "psychology_analysis.db"))
    
    

--------------------------------------------------
文件路径: src\data_handler.py
--------------------------------------------------
# 文件路径: src/data_handler.py
import sqlite3
import json
import os
from datetime import datetime

class DataHandler:
    def __init__(self, db_path="psychology_analysis.db"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """Initializes the database schema more robustly."""
        print(f"Initializing database schema at: {self.db_path}")
        required_columns_analysis = {
            # 列名: 列类型 (不包含约束和复杂默认值，这些在CREATE TABLE中处理)
            "image_path": "TEXT",
            "subject_name": "TEXT",
            "age": "INTEGER",
            "gender": "TEXT",
            "questionnaire_type": "TEXT",
            "questionnaire_data": "TEXT", # JSON
            "report_text": "TEXT",
            "updated_at": "TIMESTAMP", # 类型即可，默认值由触发器处理
            "id_card": "TEXT", # 类型即可，UNIQUE在CREATE TABLE中处理
            "occupation": "TEXT",
            "case_name": "TEXT",
            "case_type": "TEXT",
            "identity_type": "TEXT",
            "person_type": "TEXT",
            "marital_status": "TEXT",
            "children_info": "TEXT",
            "criminal_record": "INTEGER",
            "health_status": "TEXT",
            "phone_number": "TEXT",
            "domicile": "TEXT"
        }
        # questionnaire_questions 列
        required_columns_questions = {
            "scale_name": "TEXT"
        }


        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # --- Handle analysis_data Table ---
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='analysis_data';")
            table_exists = cursor.fetchone()

            if not table_exists:
                print("Table 'analysis_data' does not exist. Creating new table with full schema...")
                # Create table with all columns and constraints if it doesn't exist
                create_table_sql = """
                CREATE TABLE analysis_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    image_path TEXT,
                    subject_name TEXT,
                    age INTEGER,
                    gender TEXT,
                    questionnaire_type TEXT,
                    questionnaire_data TEXT,
                    report_text TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, -- Initial value
                    id_card TEXT , 
                    occupation TEXT,
                    case_name TEXT,
                    case_type TEXT,
                    identity_type TEXT,
                    person_type TEXT,
                    marital_status TEXT,
                    children_info TEXT,
                    criminal_record INTEGER DEFAULT 0,
                    health_status TEXT,
                    phone_number TEXT,
                    domicile TEXT
                );
                """
                cursor.execute(create_table_sql)
                print("Table 'analysis_data' created successfully.")
            else:
                print("Table 'analysis_data' exists. Checking for missing columns...")
                # If table exists, check and add missing columns without problematic constraints/defaults
                cursor.execute("PRAGMA table_info(analysis_data)")
                existing_columns = {info[1] for info in cursor.fetchall()}

                for col_name, col_type in required_columns_analysis.items():
                    if col_name not in existing_columns:
                        try:
                            # Add column with only the type, no complex defaults or UNIQUE constraints here
                            cursor.execute(f"ALTER TABLE analysis_data ADD COLUMN {col_name} {col_type}")
                            print(f"Added column '{col_name}' to analysis_data table.")
                        except sqlite3.OperationalError as e:
                            print(f"Warning: Could not add column '{col_name}': {e}")

            # --- Handle questionnaire_questions Table ---
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='questionnaire_questions';")
            q_table_exists = cursor.fetchone()
            if not q_table_exists:
                 print("Table 'questionnaire_questions' does not exist. Creating new table...")
                 cursor.execute('''CREATE TABLE questionnaire_questions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    questionnaire_type TEXT NOT NULL,
                    question_number INTEGER NOT NULL,
                    question_text TEXT NOT NULL,
                    options TEXT NOT NULL, -- JSON string
                    scale_name TEXT, -- User-friendly name
                    UNIQUE(questionnaire_type, question_number)
                )''')
                 print("Table 'questionnaire_questions' created successfully.")
            else:
                 print("Table 'questionnaire_questions' exists. Checking for missing columns...")
                 cursor.execute("PRAGMA table_info(questionnaire_questions)")
                 existing_q_columns = {info[1] for info in cursor.fetchall()}
                 for col_name, col_type in required_columns_questions.items():
                      if col_name not in existing_q_columns:
                           try:
                                cursor.execute(f"ALTER TABLE questionnaire_questions ADD COLUMN {col_name} {col_type}")
                                print(f"Added column '{col_name}' to questionnaire_questions table.")
                           except sqlite3.OperationalError as e:
                                print(f"Warning: Could not add column '{col_name}' to questionnaire_questions: {e}")


            # --- Ensure Triggers Exist ---
            # Trigger to update 'updated_at' timestamp (safe to run even if exists)
            try:
                 cursor.execute('''
                    CREATE TRIGGER IF NOT EXISTS update_analysis_data_updated_at
                    AFTER UPDATE ON analysis_data
                    FOR EACH ROW
                    WHEN OLD.updated_at = NEW.updated_at OR OLD.updated_at IS NULL -- Avoid infinite loops if trigger itself updates
                    BEGIN
                        UPDATE analysis_data SET updated_at = CURRENT_TIMESTAMP WHERE id = OLD.id;
                    END;
                ''')
                 print("Ensured 'updated_at' trigger exists.")
            except sqlite3.OperationalError as e:
                 print(f"Warning: Could not create/verify 'updated_at' trigger: {e}")


            conn.commit()
        print("Database schema initialization process completed.")
    def normalize_path(self, path):
        """Normalizes file path for consistency, returns None if path is None."""
        if path is None:
            return None
        return os.path.normpath(path).replace('\\', '/')

    def save_data(self, image_path, basic_info, scale_type, scale_answers_json):
        """Saves comprehensive data to the analysis_data table."""
        normalized_image_path = self.normalize_path(image_path) # Can be None

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            # Prepare column names and placeholders dynamically based on basic_info keys
            # Always include mandatory fields
            columns = ['image_path', 'questionnaire_type', 'questionnaire_data']
            placeholders = ['?', '?', '?']
            values = [normalized_image_path, scale_type, scale_answers_json]

            # Add fields from basic_info
            for key, value in basic_info.items():
                # Map form names to DB column names if necessary
                db_key = key # Assume direct mapping for now
                # Handle specific fields like subject_name, age, gender if they are part of basic_info
                if db_key == "name": db_key = "subject_name"

                # Ensure the key is a valid column name (check against required_columns keys)
                # This is a basic check; more robust validation might be needed
                valid_columns = { "subject_name", "age", "gender", "id_card", "occupation", "case_name", "case_type", "identity_type", "person_type", "marital_status", "children_info", "criminal_record", "health_status", "phone_number", "domicile"}
                if db_key in valid_columns:
                    columns.append(db_key)
                    placeholders.append('?')
                    # Special handling for criminal_record (assuming 1 for Yes, 0 for No from form)
                    if db_key == 'criminal_record':
                        values.append(1 if str(value).lower() in ['1', 'yes', 'true'] else 0)
                    else:
                        values.append(value)

            columns_str = ", ".join(columns)
            placeholders_str = ", ".join(placeholders)

            sql = f'''INSERT INTO analysis_data ({columns_str})
                      VALUES ({placeholders_str})'''

            try:
                cursor.execute(sql, values)
                submission_id = cursor.lastrowid
                conn.commit()
                print(f"Data saved successfully. Submission ID: {submission_id}")
                return submission_id
            except sqlite3.IntegrityError as e:
                 print(f"Error saving data: {e}. Possible duplicate entry (e.g., ID card)?")
                 # Depending on requirements, you might want to update instead of insert
                 # Or simply report the error
                 raise e # Re-raise the exception
            except Exception as e:
                 print(f"An unexpected error occurred during save: {e}")
                 raise e


    def load_data_by_id(self, submission_id):
        """Loads a submission's data by its ID, returning a dictionary."""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row # Return rows as dictionary-like objects
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM analysis_data WHERE id = ?", (submission_id,))
            result = cursor.fetchone()
            if result:
                return dict(result) # Convert Row object to a standard dictionary
            return None

    def update_report_text(self, submission_id, report_text):
        """Updates the report_text for a given submission ID."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("UPDATE analysis_data SET report_text = ? WHERE id = ?",
                           (report_text, submission_id))
            conn.commit()
            print(f"Report text updated for submission ID: {submission_id}")

    def load_questions_by_type(self, questionnaire_type_code):
        """Loads questions based on the questionnaire type code (e.g., 'SAS')."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''SELECT question_number, question_text, options
                            FROM questionnaire_questions
                            WHERE questionnaire_type = ?
                            ORDER BY question_number''', (questionnaire_type_code,))
            results = cursor.fetchall()
            questions = []
            if not results:
                 print(f"Warning: No questions found for type code '{questionnaire_type_code}'")
                 return None # Return None or empty list based on how you want to handle this
            for row in results:
                try:
                    options_list = json.loads(row[2])
                    # Ensure options have 'text' (or 'name') and 'score' keys
                    formatted_options = [
                        {"text": opt.get("text", opt.get("name", "N/A")), "score": opt.get("score", 0)}
                        for opt in options_list
                    ]
                    question = {
                        "number": row[0],
                        "text": row[1],
                        "options": formatted_options
                    }
                    questions.append(question)
                except json.JSONDecodeError:
                    print(f"Warning: Could not decode options for question {row[0]} of type {questionnaire_type_code}")
                except Exception as e:
                     print(f"Error processing question {row[0]} options: {e}")
            return questions


    def insert_question(self, questionnaire_type, question_number, question_text, options_json_str, scale_name=None):
        """Inserts a single question into the database."""
        # The options should already be a JSON string here if coming from import_questions
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            try:
                cursor.execute('''INSERT OR REPLACE INTO questionnaire_questions
                                (questionnaire_type, question_number, question_text, options, scale_name)
                                VALUES (?, ?, ?, ?, ?)''', (
                    questionnaire_type,
                    question_number,
                    question_text,
                    options_json_str, # Store as JSON string
                    scale_name if scale_name else questionnaire_type # Default scale_name to type if not provided
                ))
                conn.commit()
                # print(f"Inserted/Replaced question: {questionnaire_type} - Q{question_number}")
            except Exception as e:
                 print(f"Error inserting question {questionnaire_type}-Q{question_number}: {e}")

    def get_all_scale_types(self):
        """Retrieves distinct scale types (code and name) from the database."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            # Use COALESCE to provide the type code if name is NULL
            cursor.execute('''SELECT DISTINCT questionnaire_type, COALESCE(scale_name, questionnaire_type) as display_name
                            FROM questionnaire_questions
                            ORDER BY questionnaire_type''')
            results = cursor.fetchall()
            # Return as a list of dictionaries
            return [{"code": row[0], "name": row[1]} for row in results]


def check_db_content(db_path="psychology_analysis.db"):
    """Utility function to print the content of the analysis_data table."""
    print(f"\n--- Checking content of {db_path} ---")
    if not os.path.exists(db_path):
        print("Database file does not exist.")
        return

    try:
        with sqlite3.connect(db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            print("\n[analysis_data Table Content]")
            try:
                cursor.execute("SELECT * FROM analysis_data LIMIT 10") # Limit output for brevity
                rows = cursor.fetchall()
                if not rows:
                    print("Table is empty.")
                else:
                    # Print header
                    print(" | ".join(rows[0].keys()))
                    print("-" * (len(" | ".join(rows[0].keys())) + 10))
                    # Print rows
                    for row in rows:
                        print(" | ".join(map(str, row)))
            except sqlite3.OperationalError as e:
                print(f"Error querying analysis_data: {e}")


            print("\n[questionnaire_questions Table Content]")
            try:
                cursor.execute("SELECT DISTINCT questionnaire_type, scale_name FROM questionnaire_questions")
                scales = cursor.fetchall()
                if not scales:
                    print("Table is empty or contains no distinct scales.")
                else:
                    print("Available Scales (Type | Name):")
                    for scale in scales:
                        print(f"{scale['questionnaire_type']} | {scale['scale_name']}")

                # Optionally print a few questions per scale
                if scales:
                     print("\nSample Questions:")
                     for scale in scales[:2]: # Limit to first 2 scales for brevity
                         print(f"--- Scale: {scale['questionnaire_type']} ---")
                         cursor.execute("SELECT question_number, question_text FROM questionnaire_questions WHERE questionnaire_type = ? ORDER BY question_number LIMIT 3", (scale['questionnaire_type'],))
                         questions = cursor.fetchall()
                         for q in questions:
                             print(f"  Q{q['question_number']}: {q['question_text'][:50]}...") # Truncate long text

            except sqlite3.OperationalError as e:
                print(f"Error querying questionnaire_questions: {e}")

    except sqlite3.Error as e:
        print(f"An error occurred connecting to or reading the database: {e}")
    print("--- End of content check ---\n")


# Example usage (can be run directly to check DB)
if __name__ == "__main__":
     # Assume db is in the project root relative to this file's location (src/)
     project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
     db_file_path = os.path.join(project_root, "psychology_analysis.db")
     # Initialize schema first (important if DB or tables don't exist)
     print("Initializing DataHandler to ensure schema exists...")
     try:
         handler = DataHandler(db_path=db_file_path)
         print("DataHandler initialized.")
         # Now check content
         check_db_content(db_file_path)
         print("\nAvailable scale types:")
         print(handler.get_all_scale_types())
     except Exception as e:
          print(f"Error during DataHandler initialization or check: {e}")

--------------------------------------------------
文件路径: src\guidance_generator.py
--------------------------------------------------
# FILE: src/guidance_generator.py (修正版)
import logging
from typing import Optional, Dict, Any # <--- 添加 Any 导入
from openai import OpenAI
import os

# --- 获取 logger 和配置 ---
# 假设 logger 和 settings 已在某处初始化并可访问
# 如果在 Celery 任务中调用，确保 logger 正确传递或获取
# from app.core.config import settings # 可能需要调整路径或传递配置
# logger = logging.getLogger(settings.APP_NAME) # 或者传递 logger 实例

# 临时获取 logger 的方法 (如果未通过参数传递)
logger = logging.getLogger("GuidanceGenerator")
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO)

# --- AI 客户端初始化 ---
# 确保 API Key 可用
DASHSCOPE_API_KEY = os.environ.get("DASHSCOPE_API_KEY") # 优先从环境变量获取
# if not DASHSCOPE_API_KEY:
#     # 尝试从可能的 settings 对象获取 (如果在这个上下文可用)
#     try:
#         from app.core.config import settings
#         DASHSCOPE_API_KEY = settings.DASHSCOPE_API_KEY
#     except ImportError:
#         pass # settings 不可用

if not DASHSCOPE_API_KEY:
    logger.error("CRITICAL: Dashscope API Key 未配置，指导方案生成功能将失败！")
    # 在实际应用中，可能需要更健壮的处理方式，例如返回错误或禁用功能
    # 对于原型，这里仅记录错误

# 初始化 OpenAI 客户端 (如果 API Key 可用)
ai_client = None
if DASHSCOPE_API_KEY:
    try:
        # --- 使用安全 Headers (与 report_generator.py 保持一致) ---
        safe_headers = {
            "User-Agent": "MyPsychologyApp-GuidanceGenerator/1.0",
            "Accept": "application/json",
        }
        ai_client = OpenAI(
            api_key=DASHSCOPE_API_KEY,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", # 替换为实际 Dashscope 地址
            default_headers=safe_headers
        )
        logger.info("Guidance Generator AI client initialized.")
    except Exception as e:
        logger.error(f"Failed to initialize OpenAI client for Guidance Generator: {e}", exc_info=True)
        ai_client = None # 标记为不可用

# --- 指导方案 Prompt 设计 ---
# 可以将这些 Prompt 存储在 config.yaml 或数据库中以方便管理
PROMPT_TEMPLATES: Dict[str, str] = {
    "petitioner": """
    **任务:** 你是一位经验丰富的警务心理疏导专家。请基于以下心理评估报告，为该上访户制定一份简洁、实用、具有针对性的情绪疏导与沟通建议方案。方案应侧重于理解其核心诉求背后的心理动机，识别潜在的情绪风险点，并提供具体的沟通技巧和安抚策略，旨在缓和其激动情绪，建立信任，引导其理性表达诉求。请直接输出疏导方案内容。

    **心理评估报告摘要:**
    ```
    {report_text}
    ```

    **情绪疏导与沟通建议方案:**
    """,

    "juvenile": """
    **任务:** 你是一位青少年心理辅导专家，专长于未成年人行为矫正与心理成长。请根据下方提供的心理评估报告，为这名未成年人设计一份初步的心理辅导概要方案。方案应关注报告中揭示的风险因素和保护性因素，提出符合其年龄特点的干预目标和策略，例如提升自我认知、情绪管理能力、社交技能、家庭关系改善等。请着重考虑预防其再次触犯法律的可能性。请直接输出辅导方案概要。

    **心理评估报告摘要:**
    ```    {report_text}
    ```

    **未成年人心理辅导方案概要:**
    """,

    "police": """
    **任务:** 你是一位专门为警务人员提供心理支持的专家。请分析以下民（辅）警的心理评估报告，并据此提供一份保密、实用、富有同理心的心理调适与关怀建议。建议应聚焦于报告中反映出的压力来源、潜在的职业倦怠风险或心理困扰，提出具体的自我调适方法（如压力应对技巧、情绪调节练习）、寻求支持的途径（如内部心理服务、外部资源）以及组织层面可以提供的关怀措施建议。请直接输出调适与关怀建议。

    **心理评估报告摘要:**
    ```
    {report_text}
    ```

    **民（辅）警心理调适与关怀建议:**
    """
}

def generate_guidance(report_text: str, scenario: str, model_name: str = "qwen-plus") -> Optional[str]:
    """
    根据评估报告和场景生成指导方案。

    Args:
        report_text (str): 评估报告的核心内容。
        scenario (str): 场景类型 ('petitioner', 'juvenile', 'police')。
        model_name (str): 使用的 Dashscope 模型。

    Returns:
        Optional[str]: 生成的指导方案文本，或在错误时返回包含错误信息的字符串。
    """
    global logger, ai_client # 引用全局（模块级）变量

    logger.info(f"Guidance Gen: 开始为场景 '{scenario}' 生成指导方案")
    if ai_client is None:
        logger.error("Guidance Gen: AI 客户端未初始化，无法生成指导方案。")
        return f"错误：AI 服务未配置，无法生成场景 '{scenario}' 的指导方案。"

    if scenario not in PROMPT_TEMPLATES:
        logger.error(f"Guidance Gen: 未知的指导方案场景类型: '{scenario}'")
        return f"错误：未知的指导方案场景类型 '{scenario}'。"

    # 格式化 Prompt
    try:
        prompt = PROMPT_TEMPLATES[scenario].format(report_text=report_text)
    except KeyError as e:
        logger.error(f"Guidance Gen: Prompt 模板格式化错误，缺少键 '{e}'。模板: {PROMPT_TEMPLATES[scenario][:100]}...")
        return f"错误：内部模板格式错误，无法生成场景 '{scenario}' 的指导方案。"
    except Exception as fmt_e:
        logger.error(f"Guidance Gen: Prompt 格式化时发生意外错误: {fmt_e}", exc_info=True)
        return f"错误：准备请求时出错，无法生成场景 '{scenario}' 的指导方案。"


    messages = [
        # 可以添加一个通用的 System Prompt
        {"role": "system", "content": "你是一位专业的心理与策略顾问，请根据提供的报告和任务要求，生成具体、可行的建议方案。"},
        {"role": "user", "content": prompt}
    ]

    try:
        logger.debug(f"Guidance Gen: 调用模型 '{model_name}'，场景: {scenario}")
        completion = ai_client.chat.completions.create(
            model=model_name,
            messages=messages,
            max_tokens=1000, # 允许生成较长的方案
            temperature=0.7 # 允许一定的创造性
        )
        # 检查是否有有效的响应内容
        if completion.choices and completion.choices[0].message and completion.choices[0].message.content:
            guidance_text = completion.choices[0].message.content.strip()
            if not guidance_text:
                 logger.warning(f"Guidance Gen: 模型返回空内容，场景: {scenario}")
                 return "AI 未能生成有效的指导方案。" # 返回提示信息
            logger.info(f"Guidance Gen: 成功为场景 '{scenario}' 生成指导方案。")
            return guidance_text
        else:
            logger.error(f"Guidance Gen: 模型响应无效或内容为空，场景: {scenario}。响应对象: {completion}")
            return "错误：AI 模型返回了无效的响应。"

    except Exception as e:
        logger.error(f"Guidance Gen: 调用 AI 模型时出错 (场景: {scenario}): {e}", exc_info=True)
        return f"错误：生成指导方案时发生错误 ({type(e).__name__})"

# --- 示例用法 (注释掉，因为通常在其他模块调用) ---
# if __name__ == "__main__":
#     mock_report = """
#     评估对象李四，男，16岁。评估显示其冲动控制能力较弱，有轻生的念头表达，
#     家庭支持系统薄弱，近期涉及盗窃案件。绘画分析显示内心压抑，渴望关注。
#     SDS 量表得分较高，提示中度抑郁风险。
#     """
#     if not DASHSCOPE_API_KEY:
#         print("请设置 DASHSCOPE_API_KEY 环境变量进行测试")
#     else:
#         print("\n--- 生成未成年人辅导方案 ---")
#         juvenile_guidance = generate_guidance(mock_report, "juvenile")
#         print(juvenile_guidance)

#         print("\n--- 生成上访户疏导方案 (使用相同报告模拟) ---")
#         petitioner_guidance = generate_guidance(mock_report, "petitioner")
#         print(petitioner_guidance)

#         print("\n--- 生成民警调适建议 (使用相同报告模拟) ---")
#         police_guidance = generate_guidance(mock_report, "police")
#         print(police_guidance)

--------------------------------------------------
文件路径: src\image_processor.py
--------------------------------------------------
# src/image_processor.py
import os
import base64
from openai import OpenAI
import logging # Use logging

# Get the logger instance setup in app.py or utils.py
logger = logging.getLogger("PsychologyAnalysis")

class ImageProcessor:
    def __init__(self, config):
        """Initializes ImageProcessor with configuration and explicit safe headers."""
        self.api_key = config.get("api_key")
        if not self.api_key:
             logger.warning("API Key not found in config for ImageProcessor.")
             # Attempt to get from environment as a fallback
             self.api_key = os.environ.get('DASHSCOPE_API_KEY')
             if not self.api_key:
                  raise ValueError("API Key missing for ImageProcessor.")

        self.model = config.get("vision_model", "qwen-vl-plus") # Use config
        self.base_url = config.get("base_url", "https://dashscope.aliyuncs.com/compatible-mode/v1")

        # --- Explicitly set safe default headers ---
        safe_headers = {
            "User-Agent": "MyPsychologyApp-ImageProcessor/1.0", # Example ASCII User-Agent
            "Accept": "application/json",
            # Add other standard headers if needed, but keep values ASCII safe
            # Avoid adding headers derived from potentially non-ASCII user input here
        }
        # --- End of safe headers ---

        try:
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                default_headers=safe_headers # <--- Add explicit safe headers
            )
            logger.info(f"ImageProcessor OpenAI client initialized. Base URL: {self.base_url}. Using explicit safe headers.")
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client in ImageProcessor: {e}", exc_info=True)
            raise

    def process_image(self, image_path):
        """Processes an image using the configured vision model."""
        logger.info(f"Processing image: {image_path}")
        try:
            # 将图片转为 base64 编码
            with open(image_path, "rb") as image_file:
                image_base64 = base64.b64encode(image_file.read()).decode("utf-8")
        except FileNotFoundError:
             logger.error(f"Image file not found: {image_path}")
             raise FileNotFoundError(f"图片文件未找到: {image_path}")
        except Exception as e:
             logger.error(f"Error reading or encoding image {image_path}: {e}", exc_info=True)
             raise Exception(f"读取或编码图片时出错: {e}") from e

        # 构造消息内容
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": "You are a helpful assistant focused on image description."}] # System prompt
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"} # Assume JPEG, adjust if needed
                    },
                    # Keep the prompt focused on description
                    {"type": "text", "text": "请详细描述这张图片的内容，包括物体、人物（如有）、场景氛围、颜色和构图等。"}
                ]
            }
        ]

        try:
            # 调用 API
            logger.debug(f"Calling vision model '{self.model}' for image {os.path.basename(image_path)}")
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
            )
            description = completion.choices[0].message.content
            logger.info(f"Image description received successfully for {os.path.basename(image_path)}.")
            return description
        except Exception as e:
            # Log the specific error during the API call
            logger.error(f"Error calling vision API for {os.path.basename(image_path)}: {type(e).__name__} - {e}", exc_info=True)
            # Re-raise a user-friendly exception (the original traceback still points here)
            raise Exception(f"图像识别失败: {str(e)}") from e

--------------------------------------------------
文件路径: src\import_questions.py
--------------------------------------------------
# 文件路径: src/import_questions.py
import os
import json
import sqlite3
# Correct import assuming data_handler.py is in the same directory (src)
try:
    from data_handler import DataHandler
except ImportError:
    print("Error: data_handler.py not found in the same directory.")
    # Fallback for running directly from PsychologyAnalysis root
    try:
        # 修正：从 src 包导入
        from src.data_handler import DataHandler
    except ImportError:
         print("Error: Could not import DataHandler. Make sure you run this script correctly.")
         exit(1)


def import_questions_from_json():
    """Loads scale questions from JSON files in input/questionnaires/ into the SQLite DB."""
    # 获取项目根目录（假设此脚本在 src 下，根目录是上一级）
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    db_path = os.path.join(project_root, "psychology_analysis.db") # 数据库文件在根目录
    handler = DataHandler(db_path=db_path)
    questionnaire_dir = os.path.join(project_root, "input", "questionnaires") # 问卷目录

    print(f"Scanning for JSON questionnaires in: {questionnaire_dir}")

    # 确保目录存在
    if not os.path.isdir(questionnaire_dir):
        print(f"Error: Questionnaire directory not found: {questionnaire_dir}")
        return

    json_files = [f for f in os.listdir(questionnaire_dir) if f.endswith('.json')]
    if not json_files:
        print(f"No .json files found in {questionnaire_dir}")
        return

    imported_count = 0
    error_count = 0

    # --- 更新映射 ---
    # Define mapping from filename (or title) to scale type code and name
    # Prioritize title if available, otherwise use filename mapping
    scale_mapping = {
        "1测你性格最真实的一面.json": {"code": "Personality", "name": "测你性格最真实的一面"},
        "2亲子关系问卷量表.json": {"code": "ParentChild", "name": "亲子关系问卷量表"},
        "3焦虑症自评量表 (SAS).json": {"code": "SAS", "name": "焦虑症自评量表 (SAS)"},
        "4标准量表：抑郁症自测量表 (SDS).json": {"code": "SDS", "name": "抑郁症自测量表 (SDS)"},
        "5人际关系综合诊断量表.json": {"code": "InterpersonalRelationship", "name": "人际关系综合诊断量表"},
        "6情绪稳定性测验量表.json": {"code": "EmotionalStability", "name": "情绪稳定性测验量表"},
        "7汉密尔顿抑郁量表HAMD24.json": {"code": "HAMD24", "name": "汉密尔顿抑郁量表 (HAMD-24)"},
        "8艾森克人格问卷EPQ85成人版.json": {"code": "EPQ85", "name": "艾森克人格问卷 (EPQ-85成人版)"},
        # +++ 添加新量表的映射 +++
        "9开心测试.json": {"code": "HappyTest", "name": "开心测试"},
        # Add more mappings as needed
    }
    # 也可以基于 title 映射，如果 JSON 文件中有 title 字段
    title_mapping = {
        "测你性格最真实的一面": {"code": "Personality", "name": "测你性格最真实的一面"},
        "亲子关系问卷量表": {"code": "ParentChild", "name": "亲子关系问卷量表"},
        "焦虑症自评量表 (SAS)": {"code": "SAS", "name": "焦虑症自评量表 (SAS)"},
        "标准量表：抑郁症自测量表 (SDS)": {"code": "SDS", "name": "抑郁症自测量表 (SDS)"},
        "人际关系综合诊断量表": {"code": "InterpersonalRelationship", "name": "人际关系综合诊断量表"},
        "情绪稳定性测验量表": {"code": "EmotionalStability", "name": "情绪稳定性测验量表"},
        "汉密尔顿抑郁量表HAMD24": {"code": "HAMD24", "name": "汉密尔顿抑郁量表 (HAMD-24)"}, # 移除 .json
        "艾森克人格问卷EPQ85成人版": {"code": "EPQ85", "name": "艾森克人格问卷 (EPQ-85成人版)"}, # 移除 .json
        # +++ 添加新量表的 title 映射 +++
        "开心测试": {"code": "HappyTest", "name": "开心测试"},
    }
    # --- 映射结束 ---


    for json_file in json_files:
        file_path = os.path.join(questionnaire_dir, json_file)
        print(f"\nProcessing file: {json_file}")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Determine scale type code and name
            title = data.get("title")
            scale_info = None
            # 优先使用 title 映射
            if title and title in title_mapping:
                scale_info = title_mapping[title]
            # 其次使用文件名映射
            elif json_file in scale_mapping:
                 scale_info = scale_mapping[json_file]
            # 最后使用后备方案
            else:
                # Fallback: use filename without extension as code and title as name
                scale_code = os.path.splitext(json_file)[0]
                scale_name = title if title else scale_code # Use title if present, else filename part
                scale_info = {"code": scale_code, "name": scale_name}
                print(f"  Warning: No specific mapping found for file '{json_file}' or title '{title}'. Using fallback code='{scale_code}', name='{scale_name}'.")

            questionnaire_type = scale_info["code"]
            scale_display_name = scale_info["name"]

            print(f"  Identified as: Code='{questionnaire_type}', Name='{scale_display_name}'")

            # Clear old questions for this type before inserting new ones (optional but recommended)
            with sqlite3.connect(handler.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("DELETE FROM questionnaire_questions WHERE questionnaire_type = ?", (questionnaire_type,))
                conn.commit()
            print(f"  Cleared existing questions for type '{questionnaire_type}'.")


            # Insert questions
            questions_in_file = data.get("questions", [])
            if not questions_in_file:
                 print(f"  Warning: No 'questions' array found in {json_file}")
                 error_count += 1
                 continue

            for question in questions_in_file:
                try:
                    q_num = question["number"]
                    q_text = question["text"]
                    # Prepare options as JSON string, ensuring 'text' and 'score' keys exist
                    # Provide defaults if keys are missing in the source JSON
                    options_list = []
                    raw_options = question.get("options", [])
                    if isinstance(raw_options, list): # Check if options is actually a list
                        options_list = [
                            {"text": opt.get("text", "N/A"), "score": opt.get("score", 0)}
                            for opt in raw_options
                            if isinstance(opt, dict) # Ensure each option is a dictionary
                        ]
                    else:
                        print(f"    Warning: 'options' for question {q_num} is not a list, skipping options.")

                    options_json = json.dumps(options_list, ensure_ascii=False)

                    # Use the enhanced insert_question method
                    handler.insert_question(
                        questionnaire_type,
                        q_num,
                        q_text,
                        options_json, # Pass JSON string directly
                        scale_display_name # Pass the scale name
                        )
                    imported_count += 1
                except KeyError as ke:
                     print(f"    Error processing question in {json_file}: Missing key {ke}")
                     error_count += 1
                except TypeError as te:
                    print(f"    Error processing options for question {question.get('number', 'N/A')} in {json_file}: Likely not a list of dicts. Details: {te}")
                    error_count += 1
                except Exception as e_inner:
                     print(f"    Error processing question {question.get('number', 'N/A')} in {json_file}: {e_inner}")
                     error_count += 1

        except json.JSONDecodeError as jde:
            print(f"  Error decoding JSON from {json_file}: {jde}")
            error_count += 1
        except Exception as e_outer:
            print(f"  Error processing file {json_file}: {e_outer}")
            error_count += 1

    print(f"\nImport finished. Successfully imported {imported_count} questions.")
    if error_count > 0:
        print(f"Encountered {error_count} errors during import.")

if __name__ == "__main__":
    import_questions_from_json()
    # Optional: Check DB content after import
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    db_file_path = os.path.join(project_root, "psychology_analysis.db")
    try:
        # 修正：从 src 包导入
        from src.data_handler import check_db_content
        check_db_content(db_file_path)
    except ImportError:
         print("\nRun `python src/data_handler.py` to check database content.")
    except Exception as e:
        print(f"\nError checking DB content after import: {e}")

--------------------------------------------------
文件路径: src\interrogation_ai.py
--------------------------------------------------
import logging
from typing import List, Dict, Any, Optional
from openai import OpenAI
import os
import json

# --- 获取 logger 和配置 ---
logger = logging.getLogger("InterrogationAI")
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO)

# --- AI 客户端初始化 ---
DASHSCOPE_API_KEY = os.environ.get("DASHSCOPE_API_KEY")
if not DASHSCOPE_API_KEY:
    logger.error("CRITICAL: Dashscope API Key 未配置，审讯建议功能将失败！")

ai_client = None
if DASHSCOPE_API_KEY:
    try:
        safe_headers = {
            "User-Agent": "MyPsychologyApp-InterrogationAI/1.0",
            "Accept": "application/json",
        }
        ai_client = OpenAI(
            api_key=DASHSCOPE_API_KEY,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            default_headers=safe_headers
        )
        logger.info("Interrogation AI client initialized.")
    except Exception as e:
        logger.error(f"Failed to initialize OpenAI client for Interrogation AI: {e}", exc_info=True)
        ai_client = None

def format_history_for_prompt(history: List[Dict[str, str]]) -> str:
    """将问答历史格式化为适合 Prompt 的字符串"""
    formatted = ""
    for i, qa in enumerate(history):
        formatted += f"Q{i+1}: {qa.get('q', '无问题文本')}\nA{i+1}: {qa.get('a', '无回答文本')}\n\n"
    return formatted.strip()

def suggest_next_question(
    basic_info: Dict[str, Any],
    history: List[Dict[str, str]],
    model_name: str = "qwen-plus",
    num_suggestions: int = 2
) -> List[str]:
    """
    根据基本信息和问答历史，调用 AI 生成下一个建议的审讯问题。

    Args:
        basic_info (Dict[str, Any]): 被讯问人的基本信息字典。
        history (List[Dict[str, str]]): 当前的问答历史列表，每个元素是 {'q': '...', 'a': '...'}。
        model_name (str): 要使用的 Dashscope 模型名称。
        num_suggestions (int): 希望 AI 生成的建议问题数量。

    Returns:
        List[str]: 包含建议问题字符串的列表，如果出错则返回空列表。
    """
    global logger, ai_client

    logger.info(f"AI Suggest: 开始为基本信息 {basic_info.get('person_name', '未知姓名')} 生成建议问题")
    logger.debug(f"AI Suggest: 当前问答历史条数: {len(history)}")

    if ai_client is None:
        logger.error("AI Suggest: AI 客户端未初始化，无法生成建议。")
        return ["错误：AI 服务未配置"]

    system_prompt = f"""你是一位经验丰富的预审警官，正在协助进行审讯。
你的任务是根据已有的被讯问人基本信息和当前的问答记录，提出 {num_suggestions} 个最相关、最有助于推进审讯的关键问题。
问题应具有引导性、针对性，能够挖掘更深层次的信息或澄清疑点。
请直接输出这 {num_suggestions} 个建议的问题，每个问题占一行，不要包含任何其他解释或编号。"""

    history_str = format_history_for_prompt(history)
    basic_info_str = json.dumps(basic_info, ensure_ascii=False, indent=2)

    user_prompt = f"""
**被讯问人基本信息:**
```json
{basic_info_str}
```

**当前问答记录:**
{history_str if history_str else "（暂无问答记录）"}

**你的任务:**
请基于以上信息，提出 {num_suggestions} 个最合适的下一个审讯问题。
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    try:
        logger.debug(f"AI Suggest: 调用模型 '{model_name}'...")
        completion = ai_client.chat.completions.create(
            model=model_name,
            messages=messages,
            max_tokens=150 * num_suggestions,
            temperature=0.6,
            n=1
        )

        response_content = completion.choices[0].message.content
        logger.debug(f"AI Suggest: 模型原始响应:\n{response_content}")

        suggestions = [line.strip() for line in response_content.strip().split('\n') if line.strip()]
        if not suggestions:
            logger.warning("AI Suggest: 模型未返回有效的建议问题。")
            return ["AI 未能生成建议问题"]

        suggestions = suggestions[:num_suggestions]
        logger.info(f"AI Suggest: 成功生成 {len(suggestions)} 条建议问题。")
        return suggestions

    except Exception as e:
        logger.error(f"AI Suggest: 调用 AI 模型时出错: {e}", exc_info=True)
        return [f"错误：获取 AI 建议失败 ({type(e).__name__})"]

if __name__ == "__main__":
    mock_basic_info = {
        "person_name": "张三",
        "person_gender": "男",
        "person_id_type_number": "211402...",
        "case_type": "帮信"
    }
    mock_history = [
        {"q": "你是否知道这是违法行为？", "a": "我不太清楚，只是帮朋友忙。"},
        {"q": "帮哪个朋友？他叫什么名字？", "a": "他叫李四，我们网上认识的。"}
    ]
    if not DASHSCOPE_API_KEY:
        print("请设置 DASHSCOPE_API_KEY 环境变量进行测试")
    else:
        suggested_questions = suggest_next_question(mock_basic_info, mock_history)
        print("\n--- AI 建议的问题 ---")
        for i, q in enumerate(suggested_questions):
            print(f"{i+1}. {q}")

--------------------------------------------------
文件路径: src\report_generator.py
--------------------------------------------------
# src/report_generator.py
from openai import OpenAI
import json
import os
import logging # Use logging

# Get the logger instance setup in app.py or utils.py
logger = logging.getLogger("PsychologyAnalysis")

class ReportGenerator:
    # --- 确保 __init__ 方法定义正确，包含 self ---
    def __init__(self, config):
        """Initializes ReportGenerator with configuration and explicit safe headers."""
        # --- 使用 self. 引用实例变量 ---
        self.config = config # Store config if needed elsewhere
        self.api_key = config.get("api_key")
        if not self.api_key:
             logger.warning("API Key not found in config for ReportGenerator.")
             self.api_key = os.environ.get('DASHSCOPE_API_KEY')
             if not self.api_key:
                  raise ValueError("API Key missing for ReportGenerator.")

        self.model = config.get("text_model", "qwen-plus")
        self.base_url = config.get("base_url", "https://dashscope.aliyuncs.com/compatible-mode/v1")

        # --- Explicitly set safe default headers ---
        safe_headers = {
            "User-Agent": "MyPsychologyApp-ReportGenerator/1.0", # Example ASCII User-Agent
            "Accept": "application/json",
        }
        # --- End of safe headers ---

        try:
             # --- 使用 self. 引用实例变量 ---
             self.client = OpenAI(
                 api_key=self.api_key,
                 base_url=self.base_url,
                 default_headers=safe_headers # <--- Add explicit safe headers
             )
             logger.info(f"ReportGenerator OpenAI client initialized. Base URL: {self.base_url}. Using explicit safe headers.")
        except Exception as e:
             logger.error(f"Failed to initialize OpenAI client in ReportGenerator: {e}", exc_info=True)
             raise

        # --- Default prompt template definition (moved inside init for clarity) ---
        self.default_prompt_template = """
请根据以下信息生成一份详细的心理分析报告，服务于警务工作场景。

**I. 被测者基础信息:**
姓名: {subject_info[name]}
性别: {subject_info[gender]}
身份证号: {subject_info[id_card]}
年龄: {subject_info[age]}
职业: {subject_info[occupation]}
案件名称: {subject_info[case_name]}
案件类型: {subject_info[case_type]}
人员身份: {subject_info[identity_type]}
人员类型: {subject_info[person_type]}
婚姻状况: {subject_info[marital_status]}
子女情况: {subject_info[children_info]}
有无犯罪前科: {criminal_record_text}
健康情况: {subject_info[health_status]}
手机号: {subject_info[phone_number]}
归属地: {subject_info[domicile]}

**II. 绘画分析 (基于AI对图片的描述):**
{description}

**III. 量表分析:**
量表类型: {questionnaire_type}
量表得分: {score}
量表答案详情 (JSON):
{questionnaire}
初步解释: {scale_interpretation}

**IV. 综合心理状态分析与建议:**
请结合以上所有信息（基础信息、绘画分析、量表结果），进行深入的心理状态评估，提取关键人格特征，并针对警务工作场景（如未成年人犯罪预防、在押人员管理、上访户调解、民辅警关怀等，根据人员类型判断侧重点）给出具体的风险评估、干预建议或沟通策略。分析需专业、客观、有条理。报告应直接开始分析内容，无需重复引言。

--- 分析报告正文 ---
"""
        # --- 使用 self. 引用实例变量 ---
        # Use template from config if provided and is a string, otherwise use the default
        config_template = config.get("REPORT_PROMPT_TEMPLATE")
        if isinstance(config_template, str) and config_template.strip():
             self.prompt_template = config_template
             logger.debug("Using prompt template from config.")
        else:
             if config_template is not None: # Log if it existed but wasn't valid
                  logger.warning("REPORT_PROMPT_TEMPLATE in config is not a valid string. Using default.")
             else:
                  logger.debug("REPORT_PROMPT_TEMPLATE not found in config. Using default.")
             self.prompt_template = self.default_prompt_template


    # 确保 generate_report 方法也正确包含 self 和所有需要的参数
    def generate_report(self, description, questionnaire, subject_info, questionnaire_type, score, scale_interpretation):
        """Generates the report by formatting the prompt and calling the LLM API."""
        logger.info(f"Generating report for subject: {subject_info.get('name', 'N/A')}")

        # --- 在这里计算 criminal_record_text ---
        criminal_record_text = '是' if subject_info.get('criminal_record', 0) == 1 else '否'
        # ------------------------------------

        # Safely format questionnaire data (which should be a dictionary passed from ai_utils)
        questionnaire_str = "N/A"
        if questionnaire and isinstance(questionnaire, dict): # Check if it's a dict
            try:
                # Dump the dictionary to a pretty JSON string for the prompt
                questionnaire_str = json.dumps(questionnaire, ensure_ascii=False, indent=2)
            except Exception as json_err:
                 logger.warning(f"Could not dump questionnaire dict to JSON: {json_err}. Using raw dict string.")
                 questionnaire_str = str(questionnaire) # Fallback
        elif isinstance(questionnaire, str): # If it's already a string (e.g., JSON string)
            questionnaire_str = questionnaire
        elif questionnaire:
            logger.warning(f"Unexpected type for questionnaire data: {type(questionnaire)}. Using raw string.")
            questionnaire_str = str(questionnaire)


        # --- 构建 prompt_context，包含所有模板需要的键 ---
        prompt_context = {
            'description': description if description else "无",
            'questionnaire': questionnaire_str, # Use formatted string
            'subject_info': subject_info if subject_info else {}, # Ensure it's a dict
            'questionnaire_type': questionnaire_type if questionnaire_type else "未知",
            'score': score if score is not None else "N/A",
            'scale_interpretation': scale_interpretation if scale_interpretation else "无",
            'criminal_record_text': criminal_record_text # <--- 添加计算出的文本
        }
        # ---------------------------------------------

        # Ensure all required keys for the template exist in subject_info context
        default_keys = ["name", "gender", "id_card", "age", "occupation", "case_name", "case_type", "identity_type", "person_type", "marital_status", "children_info", "criminal_record", "health_status", "phone_number", "domicile"]
        # Ensure subject_info itself is a dict before iterating
        if isinstance(prompt_context['subject_info'], dict):
             for key in default_keys:
                 prompt_context['subject_info'].setdefault(key, '未提供') # Set default if key missing
        else: # If subject_info is somehow not a dict, create a default one
             logger.warning(f"subject_info was not a dictionary (type: {type(prompt_context['subject_info'])}). Creating default context.")
             prompt_context['subject_info'] = {key: '未提供' for key in default_keys}


        try:
            # --- 使用 self.prompt_template 和构建好的 context 格式化 ---
            final_prompt = self.prompt_template.format(**prompt_context)
            logger.debug(f"Formatted Prompt (first 500 chars): {final_prompt[:500]}...")
        except KeyError as e:
             logger.error(f"Prompt template formatting error: Missing key {e}. Context keys available: {list(prompt_context.keys())}", exc_info=True)
             # Check if the missing key is expected in subject_info
             if str(e).strip("'") in default_keys:
                  logger.error(f"Missing key '{e}' likely expected within subject_info dictionary: {prompt_context.get('subject_info')}")
             raise KeyError(f"Prompt template formatting error: Missing key {e}") from e
        except Exception as e_fmt:
             logger.error(f"Prompt template formatting error: {e_fmt}", exc_info=True)
             raise Exception(f"Prompt template formatting error: {e_fmt}") from e_fmt


        messages = [
            # Refined system prompt
            {"role": "system", "content": "你是一位专业的心理分析师。请根据用户提供的多维度信息（基础信息、绘画描述、量表结果与解释），结合心理学知识和警务场景，生成一份结构清晰、分析深入、建议具体的综合心理评估报告。"},
            {"role": "user", "content": final_prompt}
        ]

        try:
            logger.debug(f"Calling text model '{self.model}'...")
             # --- 使用 self.client 和 self.model 调用 API ---
            completion = self.client.chat.completions.create(
                model=self.model, # 使用 self.model
                messages=messages,
            )
            report_content = completion.choices[0].message.content
            logger.info("Report content received successfully.")
            return report_content
        except Exception as e:
            logger.error(f"Error calling text generation API: {type(e).__name__} - {e}", exc_info=True)
            # Re-raise the exception so ai_utils can catch it
            raise Exception(f"调用大模型 API 时出错 - {str(e)}") from e

--------------------------------------------------
文件路径: src\utils.py
--------------------------------------------------
# src/utils.py
import logging
import os
import sys
from logging.handlers import RotatingFileHandler # (可选) 使用轮转日志

# --------------------------------------------------------------------------
# 重要: 这个文件现在属于 'src' 包，
# 它将被 'app/main.py' 导入。
# 'app/main.py' 已经将项目根目录添加到了 sys.path,
# 所以这里理论上可以直接访问 'app' 包，但最好避免循环导入。
# 因此，日志配置的参数（如 level, dir）应该由调用者传入。
# --------------------------------------------------------------------------

# 获取项目根目录 (PsychologyAnalysis/)
# __file__ 指向当前文件 (utils.py)
# os.path.dirname(__file__) 指向 src 目录
# os.path.dirname(os.path.dirname(__file__)) 指向 PsychologyAnalysis 目录
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

def setup_logging(log_level_str: str = "INFO", log_dir_name: str = "logs", logger_name: str = "QingtingzheApp"):
    """
    配置应用程序的日志记录。

    Args:
        log_level_str (str): 日志级别字符串 (e.g., "DEBUG", "INFO", "WARNING").
        log_dir_name (str): 相对于项目根目录的日志文件夹名称.
        logger_name (str): 要配置的日志记录器的名称.
    """
    # --- 1. 获取日志级别 ---
    log_level = getattr(logging, log_level_str.upper(), logging.INFO)
    print(f"[Logging Setup] Setting log level to: {logging.getLevelName(log_level)} ({log_level_str})")

    # --- 2. 计算日志文件路径 ---
    log_directory = os.path.join(PROJECT_ROOT, log_dir_name)
    try:
        os.makedirs(log_directory, exist_ok=True)
        print(f"[Logging Setup] Ensured log directory exists: {log_directory}")
    except OSError as e:
        print(f"[Logging Setup] Error creating log directory {log_directory}: {e}", file=sys.stderr)
        # 如果目录创建失败，可能无法写入文件日志，但控制台日志仍应工作
        log_directory = None # 标记目录不可用

    log_file_path = os.path.join(log_directory, "app.log") if log_directory else None
    print(f"[Logging Setup] Log file path set to: {log_file_path}")


    # --- 3. 获取或创建 Logger 实例 ---
    # 使用传入的 logger_name，而不是固定的 "PsychologyAnalysis"
    # 这样可以更容易地区分来自不同模块的日志（如果需要的话）
    # 但对于简单应用，使用根 logger 或一个统一的 app logger 也可以
    logger = logging.getLogger(logger_name)
    logger.setLevel(log_level) # 设置 Logger 的基础级别

    # --- 4. 清除旧的 Handlers (防止重复添加) ---
    # 如果多次调用 setup_logging，这可以防止日志重复输出
    if logger.hasHandlers():
        print("[Logging Setup] Clearing existing handlers for logger:", logger_name)
        logger.handlers.clear()

    # --- 5. 创建 Formatter ---
    log_format = "%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s"
    formatter = logging.Formatter(log_format)

    # --- 6. 创建并添加 Handlers ---

    # a) 控制台 Handler (总是添加)
    console_handler = logging.StreamHandler(sys.stdout) # 输出到标准输出
    console_handler.setLevel(log_level) # 控制台 Handler 也遵循设定的级别
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    print(f"[Logging Setup] Added Console Handler (Level: {logging.getLevelName(console_handler.level)})")

    # b) 文件 Handler (如果路径有效)
    if log_file_path:
        try:
            # 可选：使用 RotatingFileHandler 实现日志轮转
            # maxBytes=10MB, backupCount=5 (保留5个旧日志文件)
            file_handler = RotatingFileHandler(log_file_path, maxBytes=10*1024*1024, backupCount=5, encoding='utf-8')
            # 或者使用你原来的 FileHandler:
            # file_handler = logging.FileHandler(log_file_path, encoding='utf-8')

            file_handler.setLevel(log_level) # 文件 Handler 也遵循设定的级别
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
            print(f"[Logging Setup] Added Rotating File Handler (Level: {logging.getLevelName(file_handler.level)})")
        except Exception as e:
            print(f"[Logging Setup] Failed to create/add file handler for {log_file_path}: {e}", file=sys.stderr)
            logger.error(f"Failed to set up file logging to {log_file_path}: {e}")

    # --- 7. (可选) 配置特定库的日志级别 ---
    # 减少某些库的冗余输出
    logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING) # httpx 日志可能很多
    # logging.getLogger("sqlalchemy.engine").setLevel(logging.INFO) # 查看 SQL

    print(f"[Logging Setup] Configuration for logger '{logger_name}' complete.")
    # setup_logging 不再返回 logger 实例，因为它配置的是指定名称的 logger
    # 在其他模块中，通过 logging.getLogger(logger_name) 获取即可
    # 或者如果配置的是根 logger (logging.basicConfig)，则直接使用 logging.info() 等


# --- 使用方式说明 ---
# 在你的 app/main.py 中:
#
# import logging
# from app.core.config import settings
# from src.utils import setup_logging
#
# # 在创建 FastAPI app 实例之前或之后调用
# setup_logging(log_level_str=settings.LOG_LEVEL,
#               log_dir_name=os.path.basename(settings.LOGS_DIR), # 从完整路径获取目录名
#               logger_name=settings.APP_NAME) # 使用 App 名称作为 Logger 名称
#
# # 获取 logger 实例以在 main.py 中使用
# logger = logging.getLogger(settings.APP_NAME)
# logger.info("FastAPI application starting...")
#
# # 在其他模块 (e.g., app/routers/some_router.py or src/data_handler.py) 中:
# import logging
# from app.core.config import settings # 如果需要配置中的 logger 名称
#
# # 获取在 main.py 中配置好的同名 logger
# logger = logging.getLogger(settings.APP_NAME)
# # 或者，如果决定所有模块都用同一个名字:
# # logger = logging.getLogger("QingtingzheApp") # 使用 setup_logging 时传入的固定名字
#
# logger.info("This log message comes from another module.")

--------------------------------------------------
文件路径: 文档部分\倾听者 心理学项目需求分析 (2).md
--------------------------------------------------


**“倾听者” AI 智能警务分析评估应用系统 - 需求分析 (V2.0 - 基于确认的技术栈和 AI 路径)**

**一、 项目概述与目标**

“倾听者”系统利用 AI 技术（结合自研 OCR、**预训练图像模型特征提取**、通义千问 VL 及大语言模型）和心理学评估方法（绘画、量表），对特定警务相关人群进行心理状态分析与评估。系统包含：

1.  **安卓 App (原生 Kotlin):** 用于数据采集。
2.  **后端 API 服务 (FastAPI + Python):** 处理业务逻辑、数据持久化、执行 AI 分析、提供接口。
3.  **数据库 (MySQL):** 存储所有系统数据。
4.  **后台管理系统 (Vue.js):** 用于数据查看、管理、分析和 AI 辅助功能交互。

**目标:** 为警务工作在未成年人犯罪预防、在押人员管理、上访户调解、民辅警关怀等方面提供数据驱动的决策支持、风险预警和智能辅助。

**二、 需求分析**

**A. 前端 - 安卓 App (原生 Kotlin)**

*   **用户认证 (FR-APP-01, 02):**
    *   提供符合 Material Design 的登录界面 (用户名/密码)。
    *   调用后端认证 API (`/auth/login`)，使用 Retrofit/OkHttp。
    *   安全存储 JWT Token (如 EncryptedSharedPreferences)。
*   **主界面 (FR-APP-03):**
    *   包含“分析评估”、“心理百科”（若实现）入口。
    *   采用 Bottom Navigation 或 Navigation Drawer 导航。
*   **分析评估模块 (核心流程):**
    *   **基本信息采集 (FR-APP-04):**
        *   使用标准 Android 控件 (EditText, Spinner, RadioButton, CheckBox) 构建表单。
        *   采集字段：姓名、性别、身份证号、年龄、职业、案件名称、案件类型（Spinner，数据源自 API `/lookups/case-types`）、人员身份（Spinner，数据源自 API `/lookups/identity-types`）、人员类型（Spinner，数据源自 API `/lookups/person-types`）、婚姻状况、子女情况、有无犯罪前科、健康情况、手机号、归属地（Spinner，数据源自 API `/lookups/domiciles`）。
        *   实现客户端输入校验。
    *   **绘画上传 (FR-APP-05):**
        *   集成 CameraX 或原生相机 Intent 拍照。
        *   集成 Activity Result API/相册 Intent 选择图片。
        *   提供图片预览。
    *   **量表测量 (FR-APP-06):**
        *   根据选择的量表类型，调用 API (`/scales/{scale_type}/questions`) 获取问题列表。
        *   使用 RecyclerView 动态展示问题和选项 (RadioGroup)。
        *   记录用户答案。
    *   **数据提交 (FR-APP-07):**
        *   将基础信息、量表答案（JSON 格式）、图片文件打包。
        *   调用后端数据提交 API (`/assessments/submit`)，使用 Retrofit/OkHttp 处理 `multipart/form-data` 上传。
        *   显示上传进度和结果反馈。
    *   **测评报告查看 (FR-APP-08):**
        *   提供查询接口（可能基于评估记录 ID 或用户关联记录）。
        *   调用后端报告查询 API (`/reports/{assessment_id}` 或类似接口)。
        *   解析并展示报告内容。

**B. 后端 - API 服务器 (FastAPI + Python)**

*   **API 接口 (RESTful, OpenAPI/Swagger 自动文档):**
    *   认证: `/auth/login`, `/auth/refresh`
    *   基础数据查询: `/lookups/{type}` (e.g., `/lookups/case-types`)
    *   量表: `/scales/{scale_type}/questions`
    *   评估提交: `POST /assessments/submit` (接收 App 数据)
    *   报告查询: `GET /reports`, `GET /reports/{id}`, `GET /reports/search`
    *   数据分析: `GET /analysis/stats` (带多维度查询参数)
    *   AI 辅助: `POST /ai/{assistance_type}` (e.g., `/ai/mediation_guidance`)
    *   用户管理: `GET /users`, `POST /users`, `PUT /users/{id}`, `DELETE /users/{id}` (及角色相关接口)
*   **核心业务逻辑:**
    *   **用户认证与授权 (FR-BE-CORE-01):** JWT 认证，基于角色的路径操作依赖项。
    *   **数据存储 (FR-BE-CORE-02):**
        *   使用 SQLAlchemy ORM 操作 MySQL 数据库。
        *   实现适配新 Schema 的数据保存逻辑（**重写 Demo 逻辑**）。
        *   图片存储至服务器指定路径或对象存储，路径存入数据库。
    *   **异步任务处理 (FR-BE-CORE-03):**
        *   使用 Celery + Redis/RabbitMQ。
        *   `/assessments/submit` 接口接收数据后，验证并存入数据库（标记状态为“处理中”），然后触发异步任务。
    *   **AI 分析与报告生成 (异步任务 - FR-BE-CORE-04):**
        *   **Input:** Assessment ID
        *   **Steps:**
            1.  从数据库加载评估数据 (图片路径、量表答案等)。
            2.  **本地图像处理 (运行于 CPU):**
                *   调用 OCR 服务/库获取图片文字信息。
                *   加载 **预训练轻量级图像模型 (如 MobileNetV2/V3, EfficientNet-B0)**，提取绘画图像的 **特征向量**。
            3.  调用 **通义千问 VL API**，可能结合 OCR 结果，获取图像的深层理解。
            4.  **量表处理:** 计算得分，根据规则库解释。
            5.  **整合分析:** 融合 OCR 结果、图像特征向量、VL 分析结果、量表解释。
            6.  **人格特征提取:** 实现基于整合分析结果生成人格特征的逻辑。
            7.  **报告生成:**
                *   构造包含所有分析结果的详细 **Prompt**。
                *   调用 **通义千问 LLM API** 生成报告文本。
                *   **持续进行 Prompt 调优** 以提升报告质量。
            8.  将生成的报告、特征向量、分析结果等存入数据库，更新评估记录状态为“完成”。
    *   **数据统计与分析 (FR-BE-CORE-05):** 实现基于 SQLAlchemy 的复杂查询和聚合逻辑。
    *   **AI 辅助模块 (FR-BE-CORE-06):**
        *   查询报告 -> 构造 Prompt -> 调用 LLM API -> 返回结果 -> 记录日志。
    *   **用户管理 (FR-BE-CORE-07):** 实现用户和角色的 CRUD 逻辑。

**C. 前端 - 后台管理系统 (Vue.js)**

*   **用户认证 (FR-ADM-01, 02):**
    *   登录页面，调用 `/auth/login`。
    *   使用 Axios 发送请求，Token 存入 localStorage/sessionStorage。
    *   配置 Vue Router 路由守卫。
*   **主界面/布局 (FR-ADM-03):**
    *   SPA 布局，使用 Element Plus / Naive UI / Ant Design Vue 组件库。
    *   侧边栏导航对应 7 大模块。
*   **模块 1: 查看测试报告 (FR-ADM-04, 05, 06):**
    *   查询表单 (姓名/身份证号)。
    *   使用 Table 组件展示分页结果 (调用 `/reports/search`)。
    *   点击查看详情 (Dialog/新页面，调用 `/reports/{id}`)。
*   **模块 2: 数据分析 (FR-ADM-07, 08):**
    *   筛选表单 (日期、多选框等)。
    *   调用 `/analysis/stats` API。
    *   使用 ECharts 或类似库展示图表，结合 Table。
*   **模块 3-6: AI 辅助功能 (FR-ADM-09 ~ 12 类推):**
    *   查询人员报告 -> 触发对应 AI 辅助 API (`/ai/{type}`) -> 展示返回结果。
*   **模块 7: 用户管理 (FR-ADM-13 ~ 16):**
    *   Table 展示用户列表 (调用 `/users`)。
    *   Dialog/页面 实现新增/编辑表单 (调用 `POST/PUT /users`)。
    *   操作按钮实现删除、角色分配 (调用 `DELETE /users`, `/users/{id}/role` 等)。

**三、 数据库数据结构 (MySQL + SQLAlchemy)**

*   采用上一轮分析中定义的表结构，使用 SQLAlchemy 定义数据模型 (Models)。
*   包括：`User`, `Role`, `Subject`, `Assessment`, `Scale`, `Question`, `Answer`, `Report`, `PersonalityTrait`, `AIAssistanceLog`, `Domicile`, `PersonType`, `IdentityType`, `CaseType` 等。
*   `Report` 表需要增加字段存储提取的图像特征向量 (可能序列化后存 TEXT/BLOB 或存文件路径) 和中间分析结果。
*   设计合适的索引以优化查询性能。
*   考虑敏感信息加密。

**四、 非功能性需求 (关键)**

*   **安全性:** API 权限控制、数据加密、防攻击、日志审计，满足警务系统安全标准。
*   **性能:** API 响应快，App 流畅，后台任务高效 (FastAPI, Kotlin Native, Celery 助力)。CPU 上的图像特征提取需确保性能可接受。
*   **可靠性:** 系统稳定，数据库备份恢复，AI 结果有一定稳定性。
*   **易用性:** App 和后台界面简洁直观，符合用户习惯。
*   **准确性:** 量表计分准确，**AI 分析（绘画、人格、辅助建议）的准确性和有效性是核心价值**。
*   **可维护性:** 代码结构清晰，FastAPI/Vue 模块化，类型提示，自动文档。
*   **合规性:** 遵守相关法律法规和警务规范。

**五、 基础设施要求**

*   需要服务器（云或本地）运行 FastAPI 应用、Celery Worker、MySQL 数据库、Redis/RabbitMQ 缓存/消息队列。
*   服务器 CPU 需要能承担轻量级图像模型（如 MobileNet）的推理负载（非实时）。
*   需要考虑网络带宽（图片上传、API 调用）。
*   需要部署 Nginx 等 Web 服务器/反向代理。
*   需要监控、日志、备份方案。

**六、 主要挑战与风险**

1.  **AI 效果:** 绘画分析（多模态融合）的准确性、人格特征提取的有效性、AI 辅助建议的实用性。
2.  **数据安全与合规:** 满足警务系统的高标准。
3.  **开发资源:** 团队需高效利用框架特性，合理安排优先级，进行迭代开发。
4.  **Prompt 工程:** 需要持续投入时间和精力优化 LLM 的 Prompt 以获得高质量输出。
5.  **本地模型性能:** 确保 CPU 上的图像特征提取性能满足后台任务需求。



--------------------------------------------------
文件路径: 文档部分\倾听者 心理学项目需求分析.md
--------------------------------------------------
好的，我们来整合所有讨论和决策，重新生成一份更精确、面向后续开发的需求分析文档。

**“倾听者” AI 智能警务分析评估应用系统 - 需求分析 (V2.0 - 基于确认的技术栈和 AI 路径)**

**一、 项目概述与目标**

“倾听者”系统利用 AI 技术（结合自研 OCR、**预训练图像模型特征提取**、通义千问 VL 及大语言模型）和心理学评估方法（绘画、量表），对特定警务相关人群进行心理状态分析与评估。系统包含：

1.  **安卓 App (原生 Kotlin):** 用于数据采集。
2.  **后端 API 服务 (FastAPI + Python):** 处理业务逻辑、数据持久化、执行 AI 分析、提供接口。
3.  **数据库 (MySQL):** 存储所有系统数据。
4.  **后台管理系统 (Vue.js):** 用于数据查看、管理、分析和 AI 辅助功能交互。

**目标:** 为警务工作在未成年人犯罪预防、在押人员管理、上访户调解、民辅警关怀等方面提供数据驱动的决策支持、风险预警和智能辅助。

**二、 需求分析**

**A. 前端 - 安卓 App (原生 Kotlin)**

*   **用户认证 (FR-APP-01, 02):**
    *   提供符合 Material Design 的登录界面 (用户名/密码)。
    *   调用后端认证 API (`/auth/login`)，使用 Retrofit/OkHttp。
    *   安全存储 JWT Token (如 EncryptedSharedPreferences)。
*   **主界面 (FR-APP-03):**
    *   包含“分析评估”、“心理百科”（若实现）入口。
    *   采用 Bottom Navigation 或 Navigation Drawer 导航。
*   **分析评估模块 (核心流程):**
    *   **基本信息采集 (FR-APP-04):**
        *   使用标准 Android 控件 (EditText, Spinner, RadioButton, CheckBox) 构建表单。
        *   采集字段：姓名、性别、身份证号、年龄、职业、案件名称、案件类型（Spinner，数据源自 API `/lookups/case-types`）、人员身份（Spinner，数据源自 API `/lookups/identity-types`）、人员类型（Spinner，数据源自 API `/lookups/person-types`）、婚姻状况、子女情况、有无犯罪前科、健康情况、手机号、归属地（Spinner，数据源自 API `/lookups/domiciles`）。
        *   实现客户端输入校验。
    *   **绘画上传 (FR-APP-05):**
        *   集成 CameraX 或原生相机 Intent 拍照。
        *   集成 Activity Result API/相册 Intent 选择图片。
        *   提供图片预览。
    *   **量表测量 (FR-APP-06):**
        *   根据选择的量表类型，调用 API (`/scales/{scale_type}/questions`) 获取问题列表。
        *   使用 RecyclerView 动态展示问题和选项 (RadioGroup)。
        *   记录用户答案。
    *   **数据提交 (FR-APP-07):**
        *   将基础信息、量表答案（JSON 格式）、图片文件打包。
        *   调用后端数据提交 API (`/assessments/submit`)，使用 Retrofit/OkHttp 处理 `multipart/form-data` 上传。
        *   显示上传进度和结果反馈。
    *   **测评报告查看 (FR-APP-08):**
        *   提供查询接口（可能基于评估记录 ID 或用户关联记录）。
        *   调用后端报告查询 API (`/reports/{assessment_id}` 或类似接口)。
        *   解析并展示报告内容。

**B. 后端 - API 服务器 (FastAPI + Python)**

*   **API 接口 (RESTful, OpenAPI/Swagger 自动文档):**
    *   认证: `/auth/login`, `/auth/refresh`
    *   基础数据查询: `/lookups/{type}` (e.g., `/lookups/case-types`)
    *   量表: `/scales/{scale_type}/questions`
    *   评估提交: `POST /assessments/submit` (接收 App 数据)
    *   报告查询: `GET /reports`, `GET /reports/{id}`, `GET /reports/search`
    *   数据分析: `GET /analysis/stats` (带多维度查询参数)
    *   AI 辅助: `POST /ai/{assistance_type}` (e.g., `/ai/mediation_guidance`)
    *   用户管理: `GET /users`, `POST /users`, `PUT /users/{id}`, `DELETE /users/{id}` (及角色相关接口)
*   **核心业务逻辑:**
    *   **用户认证与授权 (FR-BE-CORE-01):** JWT 认证，基于角色的路径操作依赖项。
    *   **数据存储 (FR-BE-CORE-02):**
        *   使用 SQLAlchemy ORM 操作 MySQL 数据库。
        *   实现适配新 Schema 的数据保存逻辑（**重写 Demo 逻辑**）。
        *   图片存储至服务器指定路径或对象存储，路径存入数据库。
    *   **异步任务处理 (FR-BE-CORE-03):**
        *   使用 Celery + Redis/RabbitMQ。
        *   `/assessments/submit` 接口接收数据后，验证并存入数据库（标记状态为“处理中”），然后触发异步任务。
    *   **AI 分析与报告生成 (异步任务 - FR-BE-CORE-04):**
        *   **Input:** Assessment ID
        *   **Steps:**
            1.  从数据库加载评估数据 (图片路径、量表答案等)。
            2.  **本地图像处理 (运行于 CPU):**
                *   调用 OCR 服务/库获取图片文字信息。
                *   加载 **预训练轻量级图像模型 (如 MobileNetV2/V3, EfficientNet-B0)**，提取绘画图像的 **特征向量**。
            3.  调用 **通义千问 VL API**，可能结合 OCR 结果，获取图像的深层理解。
            4.  **量表处理:** 计算得分，根据规则库解释。
            5.  **整合分析:** 融合 OCR 结果、图像特征向量、VL 分析结果、量表解释。
            6.  **人格特征提取:** 实现基于整合分析结果生成人格特征的逻辑。
            7.  **报告生成:**
                *   构造包含所有分析结果的详细 **Prompt**。
                *   调用 **通义千问 LLM API** 生成报告文本。
                *   **持续进行 Prompt 调优** 以提升报告质量。
            8.  将生成的报告、特征向量、分析结果等存入数据库，更新评估记录状态为“完成”。
    *   **数据统计与分析 (FR-BE-CORE-05):** 实现基于 SQLAlchemy 的复杂查询和聚合逻辑。
    *   **AI 辅助模块 (FR-BE-CORE-06):**
        *   查询报告 -> 构造 Prompt -> 调用 LLM API -> 返回结果 -> 记录日志。
    *   **用户管理 (FR-BE-CORE-07):** 实现用户和角色的 CRUD 逻辑。

**C. 前端 - 后台管理系统 (Vue.js)**

*   **用户认证 (FR-ADM-01, 02):**
    *   登录页面，调用 `/auth/login`。
    *   使用 Axios 发送请求，Token 存入 localStorage/sessionStorage。
    *   配置 Vue Router 路由守卫。
*   **主界面/布局 (FR-ADM-03):**
    *   SPA 布局，使用 Element Plus / Naive UI / Ant Design Vue 组件库。
    *   侧边栏导航对应 7 大模块。
*   **模块 1: 查看测试报告 (FR-ADM-04, 05, 06):**
    *   查询表单 (姓名/身份证号)。
    *   使用 Table 组件展示分页结果 (调用 `/reports/search`)。
    *   点击查看详情 (Dialog/新页面，调用 `/reports/{id}`)。
*   **模块 2: 数据分析 (FR-ADM-07, 08):**
    *   筛选表单 (日期、多选框等)。
    *   调用 `/analysis/stats` API。
    *   使用 ECharts 或类似库展示图表，结合 Table。
*   **模块 3-6: AI 辅助功能 (FR-ADM-09 ~ 12 类推):**
    *   查询人员报告 -> 触发对应 AI 辅助 API (`/ai/{type}`) -> 展示返回结果。
*   **模块 7: 用户管理 (FR-ADM-13 ~ 16):**
    *   Table 展示用户列表 (调用 `/users`)。
    *   Dialog/页面 实现新增/编辑表单 (调用 `POST/PUT /users`)。
    *   操作按钮实现删除、角色分配 (调用 `DELETE /users`, `/users/{id}/role` 等)。

**三、 数据库数据结构 (MySQL + SQLAlchemy)**

*   采用上一轮分析中定义的表结构，使用 SQLAlchemy 定义数据模型 (Models)。
*   包括：`User`, `Role`, `Subject`, `Assessment`, `Scale`, `Question`, `Answer`, `Report`, `PersonalityTrait`, `AIAssistanceLog`, `Domicile`, `PersonType`, `IdentityType`, `CaseType` 等。
*   `Report` 表需要增加字段存储提取的图像特征向量 (可能序列化后存 TEXT/BLOB 或存文件路径) 和中间分析结果。
*   设计合适的索引以优化查询性能。
*   考虑敏感信息加密。

**四、 非功能性需求 (关键)**

*   **安全性:** API 权限控制、数据加密、防攻击、日志审计，满足警务系统安全标准。
*   **性能:** API 响应快，App 流畅，后台任务高效 (FastAPI, Kotlin Native, Celery 助力)。CPU 上的图像特征提取需确保性能可接受。
*   **可靠性:** 系统稳定，数据库备份恢复，AI 结果有一定稳定性。
*   **易用性:** App 和后台界面简洁直观，符合用户习惯。
*   **准确性:** 量表计分准确，**AI 分析（绘画、人格、辅助建议）的准确性和有效性是核心价值**。
*   **可维护性:** 代码结构清晰，FastAPI/Vue 模块化，类型提示，自动文档。
*   **合规性:** 遵守相关法律法规和警务规范。

**五、 基础设施要求**

*   需要服务器（云或本地）运行 FastAPI 应用、Celery Worker、MySQL 数据库、Redis/RabbitMQ 缓存/消息队列。
*   服务器 CPU 需要能承担轻量级图像模型（如 MobileNet）的推理负载（非实时）。
*   需要考虑网络带宽（图片上传、API 调用）。
*   需要部署 Nginx 等 Web 服务器/反向代理。
*   需要监控、日志、备份方案。

**六、 主要挑战与风险**

1.  **AI 效果:** 绘画分析（多模态融合）的准确性、人格特征提取的有效性、AI 辅助建议的实用性。
2.  **数据安全与合规:** 满足警务系统的高标准。
3.  **开发资源:** 团队需高效利用框架特性，合理安排优先级，进行迭代开发。
4.  **Prompt 工程:** 需要持续投入时间和精力优化 LLM 的 Prompt 以获得高质量输出。
5.  **本地模型性能:** 确保 CPU 上的图像特征提取性能满足后台任务需求。

这份 V2.0 的需求分析文档整合了所有已知信息和决策，应该可以作为后续开发工作的可靠依据。

