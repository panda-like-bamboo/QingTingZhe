Full Project Directory Tree:
C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis
    .env
    .gitignore
    alembic.ini
    code2txt.py
    Combiner.py
    create_initial_user.py
    create_structure.bat
    find _token.py
    main.py
    project_documentation.txt
    project_text.txt
    psychology_analysis.db
    README.md
    requirements.txt
    run_celery_worker.py
    .git
        COMMIT_EDITMSG
        config
        description
        HEAD
        index
        hooks
            applypatch-msg.sample
            commit-msg.sample
            fsmonitor-watchman.sample
            post-update.sample
            pre-applypatch.sample
            pre-commit.sample
            pre-merge-commit.sample
            pre-push.sample
            pre-rebase.sample
            pre-receive.sample
            prepare-commit-msg.sample
            push-to-checkout.sample
            update.sample
        info
            exclude
        logs
            HEAD
            refs
                heads
                    main
                    feature
                        add-SQLite
                remotes
                    origin
                        main
                        feature
                            add-SQLite
        objects
            00
                1ad4bb4a59cbdb0ac1ddf134f9503ee4eb941d
                f1f9a338f4ecda0461e2b1e919d3679f88b02c
            01
                b512c1b975ae8ed48bec78d2f2a797aee04175
            03
                5c221979f77795910253f051d9d03dff9c4085
                a64fae01884fb7377a512d0118eac53cfe9345
            04
                eb1421784606bd918c72ce21ee7f0278bebf15
            0c
                dec50e08b0ab11ba7d53f7b989a6c614c31abb
            0d
                2b14ced5984d5fcbd7e0c3d4b8824f31c1fa86
                e0b6d0d203267db04218b4aa21ed5542ab10d7
            0f
                8f77aaad74eba52509e01efc00b4bd4f2db5a0
                be2c23a32171ca42eed24e8b48eb6ae70e50bc
            10
                59cc4ab69a96f51712a550fe291d6dd4fdb2b7
            12
                170485ae2e5af27f6a5612b1af82aa1aca1d16
            13
                14ce251df032572faa5b4b1c54741b762c2859
                50414ac811c9720f85d6c3236bbd79d7d0a0af
            14
                755b8a01c4534431afaa63e9a9e69855fe9693
            15
                530d58381cfaa592f432d06d0e3b4d75979fa1
            16
                428857e8476be65ef739ea56109654e7d44ed0
            17
                9554ed9b3425e2cedc13865bd9a73f5cbe290a
            18
                0028419bd3d3c73905a966756a8c23bd13192b
            19
                0edb5d839e6ba4394ea560eb1aacf1e0ff2ba7
                9a7229ad454705cff2987ca2c9aa8844d6e672
            1a
                33b1dc7c13e7687022f838a0701e0beecc73ec
                62c857ba278139810ddc9efba3815ce8a475ac
                6f5248e38415868d72b959451bca579a80cc0a
            1b
                19c678ab390732d2693b1bc7ddd31c48e9986c
                a8056d26b4e150beab2d9c4cb520985febb136
            1c
                17b125e8414f73946d0502609d81a080166e1e
                21834c10f9c325ff3f5f8afaf7ac14706952be
                f1118dba58a12d8abdf85a1264f9302dc989ce
            1e
                ae7d3c4e6aa8867a3921270d822ec7492a23d7
            1f
                11b9b2ac4c648475275a6821b3e2de1a44bd37
            20
                336ee91d617c0235f9d18d07e6a6c9f47aafad
            22
                59b8381631ec194eeadb805f64296cfadb01fc
            23
                1c188fbd04c8f681e6fa84ca53015c009785a7
            25
                54701d69ac6a2ca0ad95cff5cfdc419216e794
                57f71c9ab0fca990b555b577551c4d8388a2b8
            26
                27167eeb025b72af9103bbf8120620c63f1bb5
                ed667e9c00a416625d8a9f03dd8f6b26be218d
            27
                c2f6dc9369ae95547ad0780469b687901bb84f
            2b
                8466c8c4b97b5b581473c6ec56ebf403bb0b18
                8d7c06000aedc326c3645fc1dda5d8acc245b9
            2d
                0cae5aa5df3e38bbc4c530a43b2e5eaf23f3b3
            2f
                09c0b9da126a06729c3e25128161354403c93f
            34
                06ce50c42fef949edef25cdf5ac8becec27af3
                120dd588c96631e266483790d2cd3f883f66f4
                c2e7bdb2747a29de75088cac02a29c10dc3bbc
            35
                13a5221a131ea18fa6e04ec98ab8e306d6757c
                5f2230940baa069ffb5627a42923445941d06f
            36
                2dfbf805088266f80261136a0d3e8803898796
            38
                267654cd80ba5e7e549b8385688d0dc8ef81ff
            39
                584dae458b91ca9abdca53f9c8cc5a484b97c0
                c637c889413858d81833e91447fbd86a105c31
                ddaa0a1662a16d4bed68b5f00eb789376af67c
            3a
                28b5c3dcc80e82689df7622b9d73041a737b36
                882a9b62d80bf5ca77eccd47a0da644b092347
            3c
                6c57c7a5a0bf6998193136db26f5459bb9171b
                a15b11440a25f49e320de5f5f0bc04e81c7cc6
            3d
                18e559d61ec6bc9fec6f549a7890a43aedf304
                e427da87f0a7d2cdb8af4ccb16cf7f73d4c401
            3e
                4dafd7691ec3dd1bde9bd59fcdf28f4098af60
                e972b88be666ad384fcdb25249c2cdd74260d6
            41
                9845770072fcc3e254b3a41f080426265f0eda
                9f3b0d4b1483339db94525c683a6663afac1e1
            42
                0298caf723214e4ede35e42ad7d56bbf7be4b8
                415ba3df9187024b32f5667ecced22a4a51f32
                595efb99cc2f55484cfda26bcec2cdf02219f9
            43
                deef336344327856d1d860ffb3407000840513
            44
                a22b5940ed22e2a04d6093dd30ed791c441592
            45
                a1421ecb1fb92f95faca87c8748c2648a58afd
                ea234cb91b3420333e1752b0c8f14dc4e08c9c
            46
                a29fd2a2ef9d52e6ec457931cb5e874d24bd8a
            48
                0b130d632ca677c11f23d9fe82cf4014d15e0c
                b8f448bd5c8cace7fa482e9281b1f836bdc640
            4c
                db36f62668e963b26a42e3030fcc299fe3f55a
            4d
                d738d6463eac7b454eeb9fa96bcf1e2f486a4f
            4f
                994894eb2c7d18d16bfe83c88d60b0740d8802
                edcf43b25bd44e4e0fbb06954ba5cc9451c22a
            52
                d7c1a18ef930c1c1c98bf248a2e842d15379ff
            53
                07e0809921b46189f314499d02be4faa6772e7
                26d681a358ad0f30e0e6137763b25ed3813393
            56
                028992b3e0f68e084b289347cfdb3e9453e9dd
            57
                bd8279b9e982261ccff432113f3fae4f0ac51c
            58
                9c9062493fa10a00760ab9a98485bacf0bcfb6
            5e
                eae694ae1c7c16bc977e84ccd415c2113efea2
            5f
                2411d567b115a13565f5354f5e9a162f24a345
            61
                29feb5c75b4413880e15b4e76e31dee7b45f89
                efc86772780c5190adcbc73eb739b3d292bfa7
            62
                907838025ec3e0009b09ea0c383a67b3ed6680
            65
                9d2f11fde57f3cc3c2a8d5d60f8fe5479b92ff
            67
                021574883946a5e8a244453fa4869d2547f19a
                39dd22283de3c81abc1959398b87adb215b5b3
            6d
                865406aaaea9879f077b737ac203306abb9e8f
                ea44ab256461f55f054b63f1e82379251af10e
            6e
                0865849ffaffe3c16a97d8df0428a813ef43c1
                e1fa34f83ee5d6685c30b0684793c0d406aab0
            72
                308b59813d15637bdb4462ee2e5705b354bb22
                32676ad66a4d556998272eebb32fb4568d502e
                63f477640e2b25706b08db2b0ab34e45fc6b38
                dfd07de33c644e5be7185e0111ec4727cf9467
            74
                76edb315d4a8bb670a333a8a0f102ce2105405
            76
                50be1332e30e87a4d56178ef6db340065aff2f
                696ae0f5c0350e25cc62f892b8c8e03a61ce66
                a61f3fd9dc64dd502566996a7c6f9cbda4e122
            77
                907e45232406ba49a0de1019cd672740518b52
            79
                3ab6a06621a5827dabf94dc303366b409e7d78
                5aeef71f00d4c48a4315bf8eced76dc2d3a56f
            7a
                205d911dae4499a5957c11f70426eceec86139
            7c
                72e7dbefae2a2dd9f5b26bab1a194f0ae0700f
            7d
                0d7c2b67e6a4b5ab3c01f5417c717d1f51e52a
                7d6bab7cd95ea4af27fd1781fd4e57a4e7bdd7
                bcf05e56e4a378de1f43bf544fc13fc05a1ad5
            7f
                92744e31f87d08096844b04acbf5fb45c62efc
            80
                575028454e49098fcac7da363c615ab6838d00
            82
                f741be7012214e29cb1b7e452aa701fd37c75d
            83
                5c1af93b81cc26e7c3d9403fc96f1c60de7515
            84
                2798011a3f194239dda371cd92b3b606418abb
            86
                d2e82f7f33c7ce049891c2387e47d0c282c3b0
                eecf0e40e7e01e022163d14e7f557482c6a1d6
            89
                ccd8534dbf253b68b8ab392bed537085641801
            8a
                69f975adb32451e3c1090b62857d1e27cd3b04
                f0f895f24c72be1493fa46fa26ba80ed4bd0c1
            8b
                d66ba2773e5362cb3d2578eff8dba1cc8a0354
            8c
                d0beec64208d1801b02eb3d5732570f716708e
            8d
                94250928a83dbb62e59823cbf273ec14178f22
            8e
                66a6c876b707b5559d907d27f61365d24e1f85
            91
                9f90966c62717988fe0c3aacb1a900f1e72cb4
            94
                3f49c6131b8b6d0d063ca6bd7387e62b21de53
            95
                0d92e48c3164e57dc7c01df4e44a6c1c0bc173
            97
                d61d380edd68fe5e76eede90e99cc14e1d857b
            9c
                2e8b4dc8699a85bf6e105b5652418429bba17d
                35ff5a2c8019394506ae45a311b2ad37174ab0
            9e
                eecf53488c2bf7144ceb9f6e123fda2a27e7f2
            9f
                acd9e873402569445133d0b91ac3665a77c761
                dc040050624556464ffa5112dde397ccd792c6
            a1
                9693d4b6c943939f6b36383e9ceaa6491bbd1d
                e7d58650dedca81c92e10a43c6ee9c9f484420
            a2
                7a13e496942e077a2e4392e79a14be597fd0e6
                7ebc9f9b4dfe82cf9cfea0ab6b549aa9c04c6a
            a3
                36e5fe3ad8f45caa601a8c853601ec4c8f0ad4
            a5
                8d010bd1849ea7514bb1d0751c29ca3ba6f2f7
                fb821f3a960767b8861c06a9879fb2535ce7f3
            a6
                3975dc55b149a1531c06adcb3866fad511d1da
                e468cc369ffaf54bc3600c257e772165232b49
            a7
                756130bb1fd01db91b8be0a7bd844faccebe79
                afc0621c9b834d8f139495b113ad430ebdd065
            a8
                210548c126f9a57e25b9fa7bdc3d15d88a152e
            aa
                9674bff49341d7c6b0a6ab8ba34e8ec3372c68
            ab
                7098f5e7d3e65d1dc78736269630ae8873a82b
            ac
                197e9723b0c8d0c03b3e5698343bdcc78fdc93
                77806aed7ad5dc8f60c6c66e8b709b437766ab
                9f9be8794d6e4f9f54185a0942bd88d846ea1f
            ad
                29b33bf959eed8dce123de959454f89e26c2c0
                99bba2735d03e995d5a854935a67d871214c16
            ae
                0a0841941a896a8f3bc32244cfef832aab8225
            af
                1fc84185401155dab36f87f271cb2cb924a958
            b0
                f1d69a4cdfb7075c9fff3c750c173dee824d35
            b1
                725e7c3b8f783170d0479f18e67c416ad4e633
            b2
                3a63d157cf47e7778eca8cc2904d52be305b5d
            b3
                078c8228687749f4723fd9fa35f9f64dbbc242
                e4b3d8822d9208dfb2b5b5dddf512c4b80c844
                f232bdd903e435a62026e1f27119f15961782c
            b4
                0bdefdc1827e0dbcfe065f9350ee23574af4ab
            b7
                44a021db5e8dee616f2a2b299181d4abf5c556
                752a996e380022274105861387fd7e8bf0ea02
            b8
                5d4173ea3a9dcf2599eb5d41406a0cded0eb40
            ba
                68d800e780b486a3488fab3acba5ffb7a5131d
            bb
                3865865a1f62bf3f323712d38f3835c249f185
            be
                19ca58bb5ada17902ef40ae0e7907a44b95ebf
                ae79856ebed36ffcfc2e640570278beb01b8d8
                cf22bf1b11f7e257acb2b1de382a2c40e3e3a6
            bf
                bd8e76e775d579271d141a026c5e3e7abc3cd4
                c3170b74f820792ed9bdc1195795662271f2ef
            c1
                d21363b7bcc1ac97a45405a4c5c8ec3f789d6e
            c2
                906e28dec7b8d1539d30f6cc1dd6cf11cfff08
                a04f00f272a7328de06acb0e4c4331cceff1fd
            c3
                3afe698b93d7de76711ff8cd706821aa722007
                baeee8d9b6021c48fcdf93b3068c20b7ba0b74
            c8
                ad7372a7def273034b6a078fef7ca207f3b6ad
            cf
                aae9373284c74755573f8ee0cfe7d57812d7a9
                c9a6092bfd0b3e43af5256f837a6ab3b43f990
            d0
                830085ecfecaf0a57fcbecea0ba419c9502d37
            d1
                5b66c07e8827fdd0ff4354331ccaa2a00e7de2
                6e97d7914485c17e0395f580442377db5a6658
            d4
                1acbc388a89af3183b27562f3ae301a51ede7c
                369ec968473b131b989bd32ed2431e61002096
                4f8353405ccca528e4d91f00dd71825cf5ef37
            d5
                8de88f57ddaa8d332ac4d3b4bcd776dbd6572a
                99a575e43c7c83e9fe8dadf5982b4514f1a9b3
                e583ccc795e00605e7c75a4eba3162fca04b6a
            d6
                c8f4d9982919fbcc4daf2c64056af136a9b68a
                e9212fe9ba23ffa73805da67ca595142ba353a
            d7
                3c2aa346c4756a3785227ab1fa619dc44e90d9
                6d7d738d3cace730bd2cbcfbaa64dbc3c08941
            d8
                d1ed3944b547bbe2511caa5f1b4bfc1a347766
            d9
                2ec90575ca8351eef4c3c7b7eca228689d0f01
            da
                fba3c24607c591762386d2af6047361a2e041f
            db
                86821b7ebe4cc027e9176c2fa8ccf89d90c207
            dc
                b57f8e7615e87a5d2d391db65f9c50052ab8f8
            de
                8f2375bab107f80db8c9855adc53638dfa95b8
            e2
                612971942d9216aa098999000dbf0599ea74bb
                de9eb013671744299ebd688689ba41db573009
            e4
                23a6d30dd2a4e9a9235da54e88e8af30e90934
                fce11e6093eba1cad00b3d2eb227a99936e0b7
            e5
                0711076ac8770cd864d52f8c9e402c761083be
                25e4aa93a233985efa5c6980965c982da2ebb1
                9a65a16d5731fa0068fbdb27c0aa8b23e77f1c
            e6
                9de29bb2d1d6434b8b29ae775ad8c2e48c5391
            e7
                c459bebff100d6a85bd8fd47478eefd6518774
            e9
                03f469138532261bea6eb9e185fe490403534c
                a63bebc59f897bfcd9dba47ab7d62c44dd4314
            ea
                a6379d7ff568b3928386891be87431e5ff37c2
            eb
                0bd551fbd0728313d2506a006a98dcf3ce2d5d
            ed
                241cfe9cef4bfbdedee9213a46728ed3f5cf9b
            ee
                d71643e8aed4707ef95898270508c15ecb108f
            ef
                a77066fea8765db4e8fd7c7cb90f4552e77afb
            f0
                2431dcbc87e191f13e71aaa2a241a1aaa9a5d4
                c50edbf917d522043159af34a2e6d444df180e
            f1
                e585438b9f9ea720321a283870ca34e9558499
            f2
                1b2295f71004f83504b9b53baa11fcb390cdf6
            f3
                ba9de61264fb775bc62fb9fa6a086230c79af4
            f4
                37427c723f2716bb1e6d54dde5bd532bd632f9
                4e9bbc4bffc0a5955d72b5ec195677dfc8d7ad
            f5
                3cc46fccfb11d32b826605287372f10bcb1e77
            f6
                df3351505ab09ea62622a396aa7cb59eeeeff1
            f7
                0ab3ca71f74ca3687a641251a8624ce3586cdb
            f9
                867422c063e9ea439b567ef2afb06f4eb0b434
            fa
                9820ad9c9fc2e4c97af8c2e2353588587613e2
            fb
                18f9b4ddd9a0db174e5d0a3d453e2344690ff8
                2eb0c874fd0f9196e363705ea8fcdb30117d81
            fc
                57024cbae6ddcf9674749fb76b7cc17bad70c8
            fd
                b19d3a12e64485de2dbf1f567a2b402b7c3316
            fe
                79e7225bcbf1077fff19be1e6048abe838ca9b
                b0a73567855a2ea3e16e2870479f7416dbacf6
            ff
                212f6d63b551ff9b9254bb89d0e0641d7afd3a
                4bc98b7cec201a37c9e5c7f983594e49cba2b4
                e3e680c86868fe4470fe6d7ccd27be6a1b0c44
            info
            pack
        refs
            heads
                main
                feature
                    add-SQLite
            remotes
                origin
                    main
                    feature
                        add-SQLite
            tags
    alembic
        env.py
        README
        script.py.mako
        versions
            72143d5034d8_create_users_table.py
            __pycache__
                72143d5034d8_create_users_table.cpython-311.pyc
        __pycache__
            env.cpython-311.pyc
    app
        main.py
        __init__.py
        core
            celery_app.py
            config.py
            deps.py
            security.py
            __init__.py
            __pycache__
                celery_app.cpython-311.pyc
                config.cpython-311.pyc
                deps.cpython-311.pyc
                security.cpython-311.pyc
                __init__.cpython-311.pyc
        crud
            assessment.py
            user.py
            __init__.py
            __pycache__
                assessment.cpython-311.pyc
                user.cpython-311.pyc
                __init__.cpython-311.pyc
        db
            base_class.py
            init_db.py
            session.py
            __init__.py
            __pycache__
                base_class.cpython-311.pyc
                init_db.cpython-311.pyc
                session.cpython-311.pyc
                __init__.cpython-311.pyc
        models
            assessment.py
            user.py
            __init__.py
            __pycache__
                assessment.cpython-311.pyc
                user.cpython-311.pyc
                __init__.cpython-311.pyc
        routers
            assessments.py
            auth.py
            encyclopedia.py
            reports.py
            scales.py
            __init__.py
            __pycache__
                assessments.cpython-311.pyc
                auth.cpython-311.pyc
                encyclopedia.cpython-311.pyc
                pages.cpython-311.pyc
                reports.cpython-311.pyc
                scales.cpython-311.pyc
                __init__.cpython-311.pyc
        schemas
            assessment.py
            encyclopedia.py
            report.py
            scale.py
            token.py
            user.py
            __init__.py
            __pycache__
                assessment.cpython-311.pyc
                encyclopedia.cpython-311.pyc
                report.cpython-311.pyc
                scale.cpython-311.pyc
                token.cpython-311.pyc
                user.cpython-311.pyc
                __init__.cpython-311.pyc
        tasks
            analysis.py
            __init__.py
            __pycache__
                analysis.cpython-311.pyc
                __init__.cpython-311.pyc
        __pycache__
            main.cpython-311.pyc
            __init__.cpython-311.pyc
    config
        config.yaml
        psychology_encyclopedia.json
    input
        images
            dog_and_girl.jpeg
            image1.jpg
            TestPic.jpg
            testpic1.jpg
            v2-b7ebd7f0a95bb411884521c43f19c846_xld.jpg
        questionnaires
            1测你性格最真实的一面.json
            2亲子关系问卷量表.json
            3焦虑症自评量表 (SAS).json
            4标准量表：抑郁症自测量表 (SDS).json
            5人际关系综合诊断量表.json
            6情绪稳定性测验量表.json
            7汉密尔顿抑郁量表HAMD24.json
            8艾森克人格问卷EPQ85成人版.json
    logs
        app.log
    output
        descriptions
            testpic1_desc.txt
        reports
            testpic1_report.txt
    src
        ai_utils.py
        api_tester.py
        data_entry.py
        data_handler.py
        image_processor.py
        import_questions.py
        report_generator.py
        utils.py
        __pycache__
            ai_utils.cpython-311.pyc
            api_tester.cpython-311.pyc
            data_handler.cpython-311.pyc
            image_processor.cpython-311.pyc
            report_generator.cpython-311.pyc
            utils.cpython-311.pyc
    uploads
        10000000000000_20250416201304_testpic1.jpg
        10000000000000_20250416204749_testpic1.jpg
        10000000000000_20250416210311_testpic1.jpg
        10000000000000_20250416213454_testpic1.jpg
        1111111111111111111111_20250424130717_neu_logo2.png
        1111111111111111111111_20250424130719_neu_logo2.png
        1111111111111111111111_20250424130724_neu_logo2.png
        1111111111111111111111_20250424130725_neu_logo2.png
        1111111111111111111111_20250424130733_neu_logo2.png
        1111111111111111111111_20250424132106_testpic1.jpg
        1111111111111111111111_20250424153455_neu_logo2.png
        111111111111111111120_20250425001629_testpic1.jpg
        111111111111111111121_20250425002213_testpic1.jpg
        1223123123123123_20250417140740_bc3ca98535f261d084345205d7ed53b.jpg
        211401200000000000_20250416173352_testpic1.jpg
        211401200000000001_20250416173600_testpic1.jpg
        211401200000000001_20250416174140_testpic1.jpg
        211401200000000001_20250416174244_testpic1.jpg
        211401200000000001_20250416174317_testpic1.jpg
        211401200000000001_20250416174651_testpic1.jpg
        211401200000000001_20250416175232_testpic1.jpg
        211401200000000001_20250416180044_testpic1.jpg
        211401200000000008_20250416200605_testpic1.jpg
        211401200000000008_20250416222213_testpic1.jpg
        211401200001010001_20250425201116_testpic1.jpg
        211401200001010002_20250425202345_testpic1.jpg
        211401200001010003_20250425205400_testpic1.jpg
        211401200001010004_20250425222951_testpic1.jpg
        string_20250417112455_bc3ca98535f261d084345205d7ed53b.jpg
        string_20250417142126_bc3ca98535f261d084345205d7ed53b.jpg
        string_20250417142942_bc3ca98535f261d084345205d7ed53b.jpg
    __pycache__
        main.cpython-311.pyc
    文档部分
        finding_work.xlsx
        倾听者 心理学项目需求分析 (2).md
        倾听者 心理学项目需求分析.md
        倾听者 心理学项目需求分析.pdf
        倾听者_任务需求分析与技术选型.pdf
        倾听者项目详细流程表_V2.1.xlsx
        绘画分析AI智能解析(4)(1).xlsx
=====

Concatenated File Content (Filtered):
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.env
# .env Example
# --------------------------------------
# API Keys
# --------------------------------------
# 替换为你真实的 Dashscope API Key
DASHSCOPE_API_KEY="sk-0ffc64dd9d614c0088ee28e3a072420a"

# --------------------------------------
# Database Configuration (Example for future MySQL)
# --------------------------------------
# DATABASE_URL="mysql+mysqlclient://user:password@host:port/database_name?charset=utf8mb4"

# --------------------------------------
# Application Settings
# --------------------------------------
# 设置应用名称或环境 (development, staging, production)
ENVIRONMENT="development"
APP_NAME="Qingtingzhe AI Analysis"
# API V1 前缀 (可选)
API_V1_STR="/api/v1"

# --------------------------------------
# Security (Generate strong secrets for production)
# --------------------------------------
# 用于 JWT 签名的密钥，运行 openssl rand -hex 32 生成
SECRET_KEY="01c33c328775c0d492af2a0b1ed692f96347ec4c54d0622560022bb3cf45d2e3"
# JWT 算法和过期时间（秒）
ALGORITHM="HS256"
ACCESS_TOKEN_EXPIRE_MINUTES=30

# --------------------------------------
# CORS Origins (允许的前端地址)
# --------------------------------------
# 开发时可以允许本地地址，生产环境需要精确指定
# 使用逗号分隔，例如 "http://localhost:8080,http://127.0.0.1:8080,https://your-frontend.com"
BACKEND_CORS_ORIGINS=http://localhost,http://localhost:8080,http://127.0.0.1,http://127.0.0.1:8080

# --------------------------------------
# Logging Level
# --------------------------------------
LOG_LEVEL="INFO" # DEBUG, INFO, WARNING, ERROR, CRITICAL

# --- CORS 配置 ---
# 允许来自 FastAPI 服务本身 (运行在 8000 端口) 的请求
# 包含 localhost 和 127.0.0.1 以及模拟器特殊 IP
BACKEND_CORS_ORIGINS_STR="http://localhost:8000,http://127.0.0.1:8000,http://10.0.2.2:8000"
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.gitignore

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\code2txt.py
import os
from datetime import datetime

def generate_project_documentation(root_dir, output_file):
    """
    将项目文件夹内容生成到TXT文件，包括目录结构和文件内容
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        # 写入文档头部
        f.write(f"项目文档生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # 第一部分：项目架构
        f.write("=" * 50 + "\n")
        f.write("第一部分：项目目录结构\n")
        f.write("=" * 50 + "\n\n")
        
        # 遍历目录结构
        for root, dirs, files in os.walk(root_dir):
            # 计算层级深度
            level = root.replace(root_dir, '').count(os.sep)
            indent = "  " * level
            # 写入目录名
            f.write(f"{indent}{os.path.basename(root)}/\n")
            # 写入文件名
            for file in files:
                f.write(f"{indent}  {file}\n")
        f.write("\n\n")
        
        # 第二部分：文件内容
        f.write("=" * 50 + "\n")
        f.write("第二部分：文件内容\n")
        f.write("=" * 50 + "\n\n")
        
        # 遍历所有文件并写入内容
        for root, _, files in os.walk(root_dir):
            for file in files:
                file_path = os.path.join(root, file)
                # 只处理文本类型的文件，可以根据需要扩展
                if file.endswith(('.py', '.txt', '.md', '.json', '.html', '.css', '.js')):
                    f.write("-" * 50 + "\n")
                    # 写入文件相对路径
                    relative_path = os.path.relpath(file_path, root_dir)
                    f.write(f"文件路径: {relative_path}\n")
                    f.write("-" * 50 + "\n")
                    
                    try:
                        with open(file_path, 'r', encoding='utf-8') as source_file:
                            content = source_file.read()
                            f.write(content)
                    except Exception as e:
                        f.write(f"读取文件出错: {str(e)}\n")
                    f.write("\n\n")

def main():
    # 获取当前目录，可以修改为指定目录
    current_dir = os.getcwd()
    output_filename = "project_documentation.txt"
    
    try:
        generate_project_documentation(current_dir, output_filename)
        print(f"文档已生成: {os.path.abspath(output_filename)}")
    except Exception as e:
        print(f"生成文档时出错: {str(e)}")

if __name__ == "__main__":
    main()
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\Combiner.py
import os
import sys
from tqdm import tqdm

# 定义允许被合并内容的文件扩展名
ALLOWED_EXTENSIONS_FOR_CONTENT = ('.py')

# --- 恢复 generate_tree 到原始版本，以包含所有文件 ---
def generate_tree(root_dir):
    """生成类似 tree /f 的目录树字符串 (包含所有文件和目录)"""
    tree_output = [root_dir]
    # os.walk 会自然遍历所有子目录和文件
    for root, dirs, files in os.walk(root_dir):
        level = root.replace(root_dir, '').count(os.sep)
        indent = '    ' * level
        if level > 0:  # 避免重复输出根目录
            tree_output.append(f"{indent}{os.path.basename(root)}")
        
        # --- 在目录树中列出该目录下的所有文件 ---
        sub_indent = indent + '    '
        for file in files:
            # 不进行扩展名过滤，列出所有文件
            tree_output.append(f"{sub_indent}{file}")
            
    return "\n".join(tree_output)

def concatenate_files(root_dir, output_file):
    # 计算总文件数以初始化进度条 (反映遍历的所有文件)
    total_files = sum(len(files) for _, _, files in os.walk(root_dir))

    # 预计算将被合并内容的文件数（可选，如果你想让进度条只反映合并进度）
    # included_files_count = 0
    # for root, dirs, files in os.walk(root_dir):
    #     for file in files:
    #         _, ext = os.path.splitext(file)
    #         if ext.lower() in ALLOWED_EXTENSIONS_FOR_CONTENT:
    #             included_files_count += 1
    # print(f"找到 {included_files_count} 个符合条件的文件将被合并内容。")


    with open(output_file, 'w', encoding='utf-8') as outfile:
        # --- 写入完整的目录树 ---
        outfile.write("Full Project Directory Tree:\n") # 标题明确说明是完整树
        outfile.write(generate_tree(root_dir) + "\n") # 调用未经过滤的 generate_tree
        outfile.write("=====\n\n")
        outfile.write("Concatenated File Content (Filtered):\n") # 明确说明内容是过滤后的

        # 使用 tqdm 显示进度条 (total 仍然是所有文件数，反映遍历进度)
        # 如果使用上面的 included_files_count，这里改为 total=included_files_count
        with tqdm(total=total_files, desc="扫描文件并合并内容", unit="file") as pbar:
            for root, dirs, files in os.walk(root_dir):
                for file in files:
                    file_path = os.path.join(root, file)

                    # --- 核心过滤逻辑：只处理指定扩展名的文件内容 ---
                    _, ext = os.path.splitext(file)

                    # 检查扩展名是否在允许合并内容的列表中
                    if ext.lower() in ALLOWED_EXTENSIONS_FOR_CONTENT:
                        # --- 文件内容合并逻辑 ---
                        try:
                            with open(file_path, 'r', encoding='utf-8') as infile:
                                content = infile.read()
                                outfile.write("FILE: " + file_path + "\n")
                                outfile.write(content + "\n")
                                outfile.write("-----\n")
                            print(f"已合并内容: {file_path} - 成功")
                        except Exception as e:
                            # 即使读取失败，也记录下来
                            outfile.write(f"FILE: {file_path}\n")
                            outfile.write(f"读取文件出错 (尝试合并内容时): {str(e)}\n")
                            outfile.write("-----\n")
                            print(f"尝试合并内容失败: {file_path} - 错误: {str(e)}")
                    else:
                        # --- 文件类型不符，跳过内容合并 ---
                        # 不需要写入文件内容，只在控制台打印信息
                        print(f"已跳过内容合并 (类型不符): {file_path}")

                    # --- 更新进度条 ---
                    # 每个访问的文件都更新进度条
                    pbar.update(1)

if __name__ == "__main__":
    root_dir = sys.argv[1] if len(sys.argv) > 1 else os.getcwd()
    output_file = 'project_text.txt'
    print(f"正在扫描目录: {root_dir}")
    print(f"将生成包含所有文件的目录树，并合并后缀为 {', '.join(ALLOWED_EXTENSIONS_FOR_CONTENT)} 的文件内容到: {output_file}")
    concatenate_files(root_dir, output_file)
    print(f"处理完成。输出文件: {output_file}")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\create_initial_user.py
# create_initial_user.py
import sys
import os
import traceback
import logging
import asyncio # 导入 asyncio 用于运行异步代码
from getpass import getpass # 用于安全地获取密码输入

# --- 设置项目路径 (确保能找到 app 包) ---
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
# --------------------------------------------

# --- 导入必要的模块 ---
try:
    # 从 app.schemas 直接导入 UserCreate
    from app.schemas import UserCreate
    # 导入异步数据库组件
    from sqlalchemy.ext.asyncio import AsyncSession
    from app.db.session import AsyncSessionLocal, async_engine
    # 导入异步 CRUD 函数
    from app.crud.user import get_user_by_username, create_user
    # 导入设置 (如果需要)
    from app.core.config import settings
    # 导入数据库 Base 和 User 模型
    from app.db.base_class import Base
    from app.models.user import User
except ImportError as e:
    print(f"导入应用模块时出错: {e}")
    print("请检查以下几点:")
    print("1. 您是否在 'PsychologyAnalysis' 目录下运行此脚本。")
    print("2. 必要的文件（如 app/db/session.py, app/crud/user.py, app/models/user.py, app/schemas/user.py）是否存在。")
    print("3. requirements.txt 中的所有依赖项是否已在您的环境中安装。")
    print("4. CRUD 函数 (get_user_by_username, create_user) 是否已定义为 'async def'。")
    print("5. app/schemas/__init__.py 是否正确导出了 UserCreate。")
    sys.exit(1)
except Exception as e:
     print(f"导入过程中发生意外错误: {e}")
     sys.exit(1)
# ------------------------

# --- 配置日志 ---
APP_LOGGER_NAME = settings.APP_NAME or "QingtingzheApp"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(APP_LOGGER_NAME)
# ----------------

async def create_tables():
    """异步创建数据库表（如果尚不存在）"""
    logger.info("正在异步初始化数据库表...")
    try:
        async with async_engine.begin() as conn:
            # Base.metadata 包含了所有继承自 Base 的模型信息
            await conn.run_sync(Base.metadata.create_all)
        logger.info("数据库表已确认/创建成功。")
    except Exception as e:
        logger.error(f"创建数据库表时出错: {e}", exc_info=True)
        print(f"错误：无法创建数据库表。正在退出。错误: {e}")
        sys.exit(1)

async def create_user_interactively(db: AsyncSession) -> None:
    """交互式地获取用户信息并异步创建初始用户。"""
    logger.info("--- 正在创建初始用户 ---")

    while True:
        username = input("输入用户名: ").strip()
        if not username:
            print("用户名不能为空。")
            continue
        # 调用异步 CRUD 函数获取用户
        existing_user = await get_user_by_username(db, username=username)
        if existing_user:
            print(f"用户名 '{username}' 已存在。请选择其他用户名。")
        else:
            break

    email_str = input(f"为 {username} 输入邮箱 (可选, 直接按 Enter 跳过): ").strip()
    email = email_str if email_str else None
    full_name = input(f"为 {username} 输入全名 (可选, 直接按 Enter 跳过): ").strip() or None

    while True:
        password = getpass("输入密码 (最少6位): ")
        if len(password) < 6:
            print("密码太短 (最少6位)。")
            continue
        password_confirm = getpass("确认密码: ")
        if password != password_confirm:
            print("两次输入的密码不匹配，请重试。")
        else:
            break

    is_superuser_input = input("是否将此用户设为超级用户? (y/N): ").strip().lower()
    is_superuser = True if is_superuser_input == 'y' else False

    # 使用 Pydantic schema 准备用户数据
    user_in = UserCreate( # <-- 直接使用导入的 UserCreate
        username=username,
        email=email,
        full_name=full_name,
        password=password, # 传递明文密码，哈希在 crud.create_user 中完成
        is_superuser=is_superuser,
        is_active=True # 初始用户通常是激活的
    )

    try:
        logger.info(f"尝试创建用户: {user_in.username} (超级用户: {is_superuser})")
        # --- 关键修改：使用正确的关键字参数名 ---
        # created_user = await create_user(db=db, user=user_in) # 旧的，错误的调用
        created_user = await create_user(db=db, user_in=user_in) # <--- 使用 'user_in='
        # -----------------------------------------
        logger.info(f"成功创建用户: '{created_user.username}' (ID: {created_user.id})")
        print(f"\n用户 '{created_user.username}' 创建成功!")
    except ValueError as ve: # 捕获来自 CRUD 的特定错误 (如重复用户)
         logger.error(f"创建用户 '{username}' 失败: {ve}")
         print(f"\n错误：无法创建用户。{ve}")
    except Exception as e:
        logger.error(f"创建用户 '{username}' 时发生意外错误: {e}", exc_info=True)
        print(f"\n错误：发生意外错误: {e}")


async def main():
    """主异步执行函数"""
    await create_tables() # 先确保表存在

    logger.info("正在创建数据库会话...")
    # 正确使用异步会话上下文管理器
    async with AsyncSessionLocal() as session:
        await create_user_interactively(session)

    logger.info("脚本执行完毕。")


if __name__ == "__main__":
    print("开始执行用户创建脚本...")
    try:
        # 运行主异步函数
        asyncio.run(main())
    except KeyboardInterrupt:
         print("\n用户中断了脚本执行。")
    except Exception as e:
         print(f"\n在顶层发生意外错误: {e}")
         traceback.print_exc() # 打印完整的错误堆栈
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\find _token.py
import dashscope
from dashscope import Account

# 配置您的API Key
dashscope.api_key = 'sk-0ffc64dd9d614c0088ee28e3a072420a'

# 查询账户信息
account = Account()
balance = account.get_balance()
print(balance)
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\main.py
# 文件路径: main.py
import os
import yaml
import sqlite3
import json
from concurrent.futures import ThreadPoolExecutor, as_completed  # 新增导入
from src.image_processor import ImageProcessor
from src.report_generator import ReportGenerator
from src.data_handler import DataHandler
from src.utils import setup_logging

def process_single_task(image_path, image_processor, report_generator, data_handler, logger):
    """处理单个图片和量表数据的分析任务"""
    logger.info(f"开始处理图片: {image_path}")
    try:
        # 处理图片
        description = image_processor.process_image(image_path)
        desc_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 
                               f"output/descriptions/{os.path.splitext(os.path.basename(image_path))[0]}_desc.txt")
        os.makedirs(os.path.dirname(desc_file), exist_ok=True)
        with open(desc_file, "w", encoding='utf-8') as f:
            f.write(description)
        logger.info(f"图片描述已保存至: {desc_file}")

        # 加载数据
        subject_info, questionnaire_type, questionnaire_data = data_handler.load_data_by_image(image_path)
        if not subject_info:
            logger.warning(f"图片 {image_path} 无对应量表数据，仅保存描述")
            return

        # 计算得分并生成报告
        score = calculate_score(questionnaire_type, questionnaire_data)
        report = report_generator.generate_report(description, questionnaire_data, subject_info, questionnaire_type, score)
        report_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 
                                 f"output/reports/{os.path.splitext(os.path.basename(image_path))[0]}_report.txt")
        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        with open(report_file, "w", encoding='utf-8') as f:
            f.write(report)
        logger.info(f"报告已保存至: {report_file}")
    except Exception as e:
        logger.error(f"处理 {image_path} 时出错: {str(e)}")

def main(image_paths=None, max_workers=4):
    logger = setup_logging()
    logger.info("心理学图像和数据分析项目启动（并发模式）")

    project_root = os.path.dirname(os.path.abspath(__file__))
    os.chdir(project_root)
    config_path = os.path.join(project_root, "config/config.yaml")
    with open(config_path, "r", encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # 初始化处理器
    image_processor = ImageProcessor(config)
    report_generator = ReportGenerator(config)
    data_handler = DataHandler(db_path=os.path.join(project_root, "psychology_analysis.db"))

    # 如果没有指定图片路径，处理 input/images 目录下的所有图片
    if not image_paths:
        image_dir = os.path.join(project_root, "input/images")
        image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        logger.info(f"未指定图片路径，将处理目录 {image_dir} 中的所有图片: {len(image_paths)} 张")

    if not image_paths:
        logger.warning("没有找到任何图片需要处理")
        return

    # 使用线程池并发处理
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # 提交所有任务
        future_to_image = {executor.submit(process_single_task, image_path, image_processor, report_generator, data_handler, logger): image_path 
                         for image_path in image_paths}
        
        # 处理完成的任务
        for future in as_completed(future_to_image):
            image_path = future_to_image[future]
            try:
                future.result()  # 获取结果，如果有异常会抛出
                logger.info(f"任务完成: {image_path}")
            except Exception as e:
                logger.error(f"任务 {image_path} 执行失败: {str(e)}")

def calculate_score(questionnaire_type, questionnaire_data):
    """根据量表类型计算总分"""
    scores = [int(value) for value in questionnaire_data.values()]
    total_score = sum(scores)
    return total_score

if __name__ == "__main__":
    # 示例：可以传入特定图片路径列表
    specific_images = [
        os.path.join(os.path.dirname(os.path.abspath(__file__)), "input/images/TestPic.jpg"),
        os.path.join(os.path.dirname(os.path.abspath(__file__)), "input/images/testpic1.jpg")
    ]
    main(image_paths=specific_images, max_workers=4)  # 设置最大线程数为4
    # 或不指定图片路径，处理所有图片
    # main(max_workers=4)
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\run_celery_worker.py
# run_celery_worker.py
import os
import sys

# --- 1. 定位项目根目录 ---
# __file__ 指向这个脚本文件 (run_celery_worker.py)
# os.path.dirname(__file__) 指向项目根目录 (PsychologyAnalysis/)
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
print(f"[Worker Start Script] Project Root detected: {PROJECT_ROOT}")

# --- 2. 将项目根目录添加到 sys.path ---
# 这样 Celery 启动时就能找到 app 和 src 包
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
    print(f"[Worker Start Script] Added {PROJECT_ROOT} to sys.path")
else:
    print(f"[Worker Start Script] {PROJECT_ROOT} already in sys.path")

# --- 3. 导入 Celery App 实例 ---
# 现在可以安全地导入了
try:
    from app.core.celery_app import celery_app
    print("[Worker Start Script] Successfully imported celery_app")
except ImportError as e:
    print(f"[Worker Start Script] CRITICAL ERROR: Could not import celery_app: {e}")
    print("Please ensure app/core/celery_app.py exists and dependencies are installed.")
    sys.exit(1)
except Exception as e:
     print(f"[Worker Start Script] CRITICAL ERROR during celery_app import: {e}")
     sys.exit(1)


# --- 4. (可选) 加载 .env 文件 (如果 Celery 任务需要直接访问环境变量) ---
# Celery worker 默认不一定加载 .env。如果你的任务代码 (如 ai_utils)
# 需要读取 .env 中的变量 (除了 Pydantic Settings 已经加载的)，
# 在这里加载它。
# from dotenv import load_dotenv
# dotenv_path = os.path.join(PROJECT_ROOT, '.env')
# if os.path.exists(dotenv_path):
#     load_dotenv(dotenv_path=dotenv_path)
#     print(f"[Worker Start Script] Loaded environment variables from: {dotenv_path}")
# else:
#     print("[Worker Start Script] .env file not found, skipping dotenv load.")


# --- 5. 准备 Celery Worker 的命令行参数 ---
# 你可以在这里定义参数，或者从命令行读取
worker_args = [
    'worker',             # 命令
    '--loglevel=info',    # 日志级别
    # '-P', 'solo',       # 在 Windows 上需要添加这个参数
    # '-c', '4',          # (可选) 并发数 (Linux/macOS)
    # '--pool=prefork',   # (可选) 进程池类型 (Linux/macOS 默认)
]

# --- 针对 Windows 添加 solo 进程池 ---
if sys.platform == "win32":
     if '-P' not in worker_args and '--pool' not in worker_args:
         print("[Worker Start Script] Windows detected, adding '-P solo' argument.")
         worker_args.extend(['-P', 'solo'])

# --- 6. 执行 Celery Worker 命令 ---
print(f"[Worker Start Script] Starting Celery worker with args: {worker_args}")
# 使用 celery_app.worker_main 来启动 worker，它会处理命令行参数
celery_app.worker_main(argv=worker_args)
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\COMMIT_EDITMSG
update:

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\config
[core]
	repositoryformatversion = 0
	filemode = false
	bare = false
	logallrefupdates = true
	symlinks = false
	ignorecase = true
[remote "origin"]
	url = git@github.com:panda-like-bamboo/PsychologyAnalysis.git
	fetch = +refs/heads/*:refs/remotes/origin/*
[branch "main"]
	remote = origin
	merge = refs/heads/main
[branch "feature/add-SQLite"]
	vscode-merge-base = origin/main
	remote = origin
	merge = refs/heads/feature/add-SQLite

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\description
Unnamed repository; edit this file 'description' to name the repository.

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\HEAD
ref: refs/heads/feature/add-SQLite

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\index
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa3 in position 11: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\info\exclude
# git ls-files --others --exclude-from=.git/info/exclude
# Lines that start with '#' are comments.
# For a project mostly in C, the following would be a good set of
# exclude patterns (uncomment them if you want to use them):
# *.[oa]
# *~

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\logs\HEAD
0000000000000000000000000000000000000000 c2a04f00f272a7328de06acb0e4c4331cceff1fd panda-like-bamboo <dier334824851@163.com> 1742809358 +0800	commit (initial): Initial commit
c2a04f00f272a7328de06acb0e4c4331cceff1fd c2a04f00f272a7328de06acb0e4c4331cceff1fd panda-like-bamboo <dier334824851@163.com> 1742811839 +0800	checkout: moving from main to feature/add-SQLite
c2a04f00f272a7328de06acb0e4c4331cceff1fd 82f741be7012214e29cb1b7e452aa701fd37c75d panda-like-bamboo <dier334824851@163.com> 1742832601 +0800	commit: 本机html可交互demo
82f741be7012214e29cb1b7e452aa701fd37c75d 04eb1421784606bd918c72ce21ee7f0278bebf15 panda-like-bamboo <dier334824851@163.com> 1745515378 +0800	commit: update:

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\logs\refs\heads\main
0000000000000000000000000000000000000000 c2a04f00f272a7328de06acb0e4c4331cceff1fd panda-like-bamboo <dier334824851@163.com> 1742809358 +0800	commit (initial): Initial commit

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\logs\refs\heads\feature\add-SQLite
0000000000000000000000000000000000000000 c2a04f00f272a7328de06acb0e4c4331cceff1fd panda-like-bamboo <dier334824851@163.com> 1742811839 +0800	branch: Created from HEAD
c2a04f00f272a7328de06acb0e4c4331cceff1fd 82f741be7012214e29cb1b7e452aa701fd37c75d panda-like-bamboo <dier334824851@163.com> 1742832601 +0800	commit: 本机html可交互demo
82f741be7012214e29cb1b7e452aa701fd37c75d 04eb1421784606bd918c72ce21ee7f0278bebf15 panda-like-bamboo <dier334824851@163.com> 1745515378 +0800	commit: update:

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\logs\refs\remotes\origin\main
0000000000000000000000000000000000000000 c2a04f00f272a7328de06acb0e4c4331cceff1fd panda-like-bamboo <dier334824851@163.com> 1742810995 +0800	update by push

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\logs\refs\remotes\origin\feature\add-SQLite
0000000000000000000000000000000000000000 04eb1421784606bd918c72ce21ee7f0278bebf15 panda-like-bamboo <dier334824851@163.com> 1745515408 +0800	update by push

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\00\1ad4bb4a59cbdb0ac1ddf134f9503ee4eb941d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 23: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\00\f1f9a338f4ecda0461e2b1e919d3679f88b02c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd0 in position 20: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\01\b512c1b975ae8ed48bec78d2f2a797aee04175
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbd in position 4: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\03\5c221979f77795910253f051d9d03dff9c4085
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcc in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\03\a64fae01884fb7377a512d0118eac53cfe9345
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\04\eb1421784606bd918c72ce21ee7f0278bebf15
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\0c\dec50e08b0ab11ba7d53f7b989a6c614c31abb
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\0d\2b14ced5984d5fcbd7e0c3d4b8824f31c1fa86
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 16: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\0d\e0b6d0d203267db04218b4aa21ed5542ab10d7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 18: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\0f\8f77aaad74eba52509e01efc00b4bd4f2db5a0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\0f\be2c23a32171ca42eed24e8b48eb6ae70e50bc
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8f in position 5: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\10\59cc4ab69a96f51712a550fe291d6dd4fdb2b7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\12\170485ae2e5af27f6a5612b1af82aa1aca1d16
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbd in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\13\14ce251df032572faa5b4b1c54741b762c2859
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdb in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\13\50414ac811c9720f85d6c3236bbd79d7d0a0af
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\14\755b8a01c4534431afaa63e9a9e69855fe9693
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd0 in position 20: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\15\530d58381cfaa592f432d06d0e3b4d75979fa1
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\16\428857e8476be65ef739ea56109654e7d44ed0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 14: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\17\9554ed9b3425e2cedc13865bd9a73f5cbe290a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\18\0028419bd3d3c73905a966756a8c23bd13192b
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\19\0edb5d839e6ba4394ea560eb1aacf1e0ff2ba7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\19\9a7229ad454705cff2987ca2c9aa8844d6e672
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1a\33b1dc7c13e7687022f838a0701e0beecc73ec
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbb in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1a\62c857ba278139810ddc9efba3815ce8a475ac
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1a\6f5248e38415868d72b959451bca579a80cc0a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1b\19c678ab390732d2693b1bc7ddd31c48e9986c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 20: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1b\a8056d26b4e150beab2d9c4cb520985febb136
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1c\17b125e8414f73946d0502609d81a080166e1e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdb in position 6: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1c\21834c10f9c325ff3f5f8afaf7ac14706952be
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb1 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1c\f1118dba58a12d8abdf85a1264f9302dc989ce
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x92 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1e\ae7d3c4e6aa8867a3921270d822ec7492a23d7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb6 in position 20: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\1f\11b9b2ac4c648475275a6821b3e2de1a44bd37
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd0 in position 20: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\20\336ee91d617c0235f9d18d07e6a6c9f47aafad
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\22\59b8381631ec194eeadb805f64296cfadb01fc
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 14: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\23\1c188fbd04c8f681e6fa84ca53015c009785a7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\25\54701d69ac6a2ca0ad95cff5cfdc419216e794
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9c in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\25\57f71c9ab0fca990b555b577551c4d8388a2b8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd0 in position 19: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\26\27167eeb025b72af9103bbf8120620c63f1bb5
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\26\ed667e9c00a416625d8a9f03dd8f6b26be218d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb1 in position 4: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\27\c2f6dc9369ae95547ad0780469b687901bb84f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\2b\8466c8c4b97b5b581473c6ec56ebf403bb0b18
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\2b\8d7c06000aedc326c3645fc1dda5d8acc245b9
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\2d\0cae5aa5df3e38bbc4c530a43b2e5eaf23f3b3
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\2f\09c0b9da126a06729c3e25128161354403c93f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\34\06ce50c42fef949edef25cdf5ac8becec27af3
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\34\120dd588c96631e266483790d2cd3f883f66f4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\34\c2e7bdb2747a29de75088cac02a29c10dc3bbc
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\35\13a5221a131ea18fa6e04ec98ab8e306d6757c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\35\5f2230940baa069ffb5627a42923445941d06f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\36\2dfbf805088266f80261136a0d3e8803898796
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\38\267654cd80ba5e7e549b8385688d0dc8ef81ff
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb1 in position 28: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\39\584dae458b91ca9abdca53f9c8cc5a484b97c0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdf in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\39\c637c889413858d81833e91447fbd86a105c31
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\39\ddaa0a1662a16d4bed68b5f00eb789376af67c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb6 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3a\28b5c3dcc80e82689df7622b9d73041a737b36
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3a\882a9b62d80bf5ca77eccd47a0da644b092347
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3c\6c57c7a5a0bf6998193136db26f5459bb9171b
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdb in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3c\a15b11440a25f49e320de5f5f0bc04e81c7cc6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcb in position 19: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3d\18e559d61ec6bc9fec6f549a7890a43aedf304
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3d\e427da87f0a7d2cdb8af4ccb16cf7f73d4c401
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3e\4dafd7691ec3dd1bde9bd59fcdf28f4098af60
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\3e\e972b88be666ad384fcdb25249c2cdd74260d6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 22: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\41\9845770072fcc3e254b3a41f080426265f0eda
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdb in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\41\9f3b0d4b1483339db94525c683a6663afac1e1
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\42\0298caf723214e4ede35e42ad7d56bbf7be4b8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb0 in position 7: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\42\415ba3df9187024b32f5667ecced22a4a51f32
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\42\595efb99cc2f55484cfda26bcec2cdf02219f9
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\43\deef336344327856d1d860ffb3407000840513
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\44\a22b5940ed22e2a04d6093dd30ed791c441592
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\45\a1421ecb1fb92f95faca87c8748c2648a58afd
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9c in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\45\ea234cb91b3420333e1752b0c8f14dc4e08c9c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\46\a29fd2a2ef9d52e6ec457931cb5e874d24bd8a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\48\0b130d632ca677c11f23d9fe82cf4014d15e0c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\48\b8f448bd5c8cace7fa482e9281b1f836bdc640
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\4c\db36f62668e963b26a42e3030fcc299fe3f55a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\4d\d738d6463eac7b454eeb9fa96bcf1e2f486a4f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbf in position 11: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\4f\994894eb2c7d18d16bfe83c88d60b0740d8802
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\4f\edcf43b25bd44e4e0fbb06954ba5cc9451c22a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdf in position 9: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\52\d7c1a18ef930c1c1c98bf248a2e842d15379ff
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\53\07e0809921b46189f314499d02be4faa6772e7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\53\26d681a358ad0f30e0e6137763b25ed3813393
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\56\028992b3e0f68e084b289347cfdb3e9453e9dd
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\57\bd8279b9e982261ccff432113f3fae4f0ac51c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 15: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\58\9c9062493fa10a00760ab9a98485bacf0bcfb6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\5e\eae694ae1c7c16bc977e84ccd415c2113efea2
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc9 in position 7: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\5f\2411d567b115a13565f5354f5e9a162f24a345
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x90 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\61\29feb5c75b4413880e15b4e76e31dee7b45f89
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xe3 in position 21: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\61\efc86772780c5190adcbc73eb739b3d292bfa7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcb in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\62\907838025ec3e0009b09ea0c383a67b3ed6680
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 14: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\65\9d2f11fde57f3cc3c2a8d5d60f8fe5479b92ff
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\67\021574883946a5e8a244453fa4869d2547f19a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdb in position 6: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\67\39dd22283de3c81abc1959398b87adb215b5b3
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 9-10: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\6d\865406aaaea9879f077b737ac203306abb9e8f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 14: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\6d\ea44ab256461f55f054b63f1e82379251af10e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\6e\0865849ffaffe3c16a97d8df0428a813ef43c1
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\6e\e1fa34f83ee5d6685c30b0684793c0d406aab0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\72\308b59813d15637bdb4462ee2e5705b354bb22
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\72\32676ad66a4d556998272eebb32fb4568d502e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\72\63f477640e2b25706b08db2b0ab34e45fc6b38
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\72\dfd07de33c644e5be7185e0111ec4727cf9467
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 18: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\74\76edb315d4a8bb670a333a8a0f102ce2105405
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\76\50be1332e30e87a4d56178ef6db340065aff2f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\76\696ae0f5c0350e25cc62f892b8c8e03a61ce66
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\76\a61f3fd9dc64dd502566996a7c6f9cbda4e122
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 23: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\77\907e45232406ba49a0de1019cd672740518b52
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\79\3ab6a06621a5827dabf94dc303366b409e7d78
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\79\5aeef71f00d4c48a4315bf8eced76dc2d3a56f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\7a\205d911dae4499a5957c11f70426eceec86139
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\7c\72e7dbefae2a2dd9f5b26bab1a194f0ae0700f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\7d\0d7c2b67e6a4b5ab3c01f5417c717d1f51e52a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb6 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\7d\7d6bab7cd95ea4af27fd1781fd4e57a4e7bdd7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\7d\bcf05e56e4a378de1f43bf544fc13fc05a1ad5
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\7f\92744e31f87d08096844b04acbf5fb45c62efc
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\80\575028454e49098fcac7da363c615ab6838d00
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\82\f741be7012214e29cb1b7e452aa701fd37c75d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\83\5c1af93b81cc26e7c3d9403fc96f1c60de7515
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\84\2798011a3f194239dda371cd92b3b606418abb
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbd in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\86\d2e82f7f33c7ce049891c2387e47d0c282c3b0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 22: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\86\eecf0e40e7e01e022163d14e7f557482c6a1d6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\89\ccd8534dbf253b68b8ab392bed537085641801
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\8a\69f975adb32451e3c1090b62857d1e27cd3b04
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\8a\f0f895f24c72be1493fa46fa26ba80ed4bd0c1
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\8b\d66ba2773e5362cb3d2578eff8dba1cc8a0354
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\8c\d0beec64208d1801b02eb3d5732570f716708e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\8d\94250928a83dbb62e59823cbf273ec14178f22
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd3 in position 6: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\8e\66a6c876b707b5559d907d27f61365d24e1f85
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\91\9f90966c62717988fe0c3aacb1a900f1e72cb4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\94\3f49c6131b8b6d0d063ca6bd7387e62b21de53
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\95\0d92e48c3164e57dc7c01df4e44a6c1c0bc173
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x92 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\97\d61d380edd68fe5e76eede90e99cc14e1d857b
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\9c\2e8b4dc8699a85bf6e105b5652418429bba17d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb7 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\9c\35ff5a2c8019394506ae45a311b2ad37174ab0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\9e\eecf53488c2bf7144ceb9f6e123fda2a27e7f2
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\9f\acd9e873402569445133d0b91ac3665a77c761
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\9f\dc040050624556464ffa5112dde397ccd792c6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x94 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a1\9693d4b6c943939f6b36383e9ceaa6491bbd1d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x88 in position 18: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a1\e7d58650dedca81c92e10a43c6ee9c9f484420
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a2\7a13e496942e077a2e4392e79a14be597fd0e6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a2\7ebc9f9b4dfe82cf9cfea0ab6b549aa9c04c6a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x91 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a3\36e5fe3ad8f45caa601a8c853601ec4c8f0ad4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a5\8d010bd1849ea7514bb1d0751c29ca3ba6f2f7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a5\fb821f3a960767b8861c06a9879fb2535ce7f3
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb1 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a6\3975dc55b149a1531c06adcb3866fad511d1da
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb6 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a6\e468cc369ffaf54bc3600c257e772165232b49
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a7\756130bb1fd01db91b8be0a7bd844faccebe79
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a7\afc0621c9b834d8f139495b113ad430ebdd065
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 14: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\a8\210548c126f9a57e25b9fa7bdc3d15d88a152e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\aa\9674bff49341d7c6b0a6ab8ba34e8ec3372c68
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ab\7098f5e7d3e65d1dc78736269630ae8873a82b
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcd in position 18: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ac\197e9723b0c8d0c03b3e5698343bdcc78fdc93
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ac\77806aed7ad5dc8f60c6c66e8b709b437766ab
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ac\9f9be8794d6e4f9f54185a0942bd88d846ea1f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ad\29b33bf959eed8dce123de959454f89e26c2c0
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xde in position 9: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ad\99bba2735d03e995d5a854935a67d871214c16
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ae\0a0841941a896a8f3bc32244cfef832aab8225
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\af\1fc84185401155dab36f87f271cb2cb924a958
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b0\f1d69a4cdfb7075c9fff3c750c173dee824d35
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b1\725e7c3b8f783170d0479f18e67c416ad4e633
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b2\3a63d157cf47e7778eca8cc2904d52be305b5d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b3\078c8228687749f4723fd9fa35f9f64dbbc242
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 23: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b3\e4b3d8822d9208dfb2b5b5dddf512c4b80c844
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b3\f232bdd903e435a62026e1f27119f15961782c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode bytes in position 2-3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b4\0bdefdc1827e0dbcfe065f9350ee23574af4ab
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xf5 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b7\44a021db5e8dee616f2a2b299181d4abf5c556
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b7\752a996e380022274105861387fd7e8bf0ea02
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbb in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\b8\5d4173ea3a9dcf2599eb5d41406a0cded0eb40
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ba\68d800e780b486a3488fab3acba5ffb7a5131d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\bb\3865865a1f62bf3f323712d38f3835c249f185
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\be\19ca58bb5ada17902ef40ae0e7907a44b95ebf
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\be\ae79856ebed36ffcfc2e640570278beb01b8d8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\be\cf22bf1b11f7e257acb2b1de382a2c40e3e3a6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\bf\bd8e76e775d579271d141a026c5e3e7abc3cd4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\bf\c3170b74f820792ed9bdc1195795662271f2ef
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb6 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\c1\d21363b7bcc1ac97a45405a4c5c8ec3f789d6e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\c2\906e28dec7b8d1539d30f6cc1dd6cf11cfff08
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\c2\a04f00f272a7328de06acb0e4c4331cceff1fd
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\c3\3afe698b93d7de76711ff8cd706821aa722007
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc3 in position 6: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\c3\baeee8d9b6021c48fcdf93b3068c20b7ba0b74
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcd in position 9: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\c8\ad7372a7def273034b6a078fef7ca207f3b6ad
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb2 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\cf\aae9373284c74755573f8ee0cfe7d57812d7a9
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb2 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\cf\c9a6092bfd0b3e43af5256f837a6ab3b43f990
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb0 in position 7: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d0\830085ecfecaf0a57fcbecea0ba419c9502d37
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d1\5b66c07e8827fdd0ff4354331ccaa2a00e7de2
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdf in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d1\6e97d7914485c17e0395f580442377db5a6658
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xef in position 9: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d4\1acbc388a89af3183b27562f3ae301a51ede7c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d4\369ec968473b131b989bd32ed2431e61002096
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb7 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d4\4f8353405ccca528e4d91f00dd71825cf5ef37
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d5\8de88f57ddaa8d332ac4d3b4bcd776dbd6572a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdb in position 6: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d5\99a575e43c7c83e9fe8dadf5982b4514f1a9b3
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d5\e583ccc795e00605e7c75a4eba3162fca04b6a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d6\c8f4d9982919fbcc4daf2c64056af136a9b68a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcd in position 4: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d6\e9212fe9ba23ffa73805da67ca595142ba353a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d7\3c2aa346c4756a3785227ab1fa619dc44e90d9
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xa5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d7\6d7d738d3cace730bd2cbcfbaa64dbc3c08941
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x92 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d8\d1ed3944b547bbe2511caa5f1b4bfc1a347766
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\d9\2ec90575ca8351eef4c3c7b7eca228689d0f01
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\da\fba3c24607c591762386d2af6047361a2e041f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\db\86821b7ebe4cc027e9176c2fa8ccf89d90c207
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb0 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\dc\b57f8e7615e87a5d2d391db65f9c50052ab8f8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb4 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\de\8f2375bab107f80db8c9855adc53638dfa95b8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e2\612971942d9216aa098999000dbf0599ea74bb
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e2\de9eb013671744299ebd688689ba41db573009
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb7 in position 9: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e4\23a6d30dd2a4e9a9235da54e88e8af30e90934
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x95 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e4\fce11e6093eba1cad00b3d2eb227a99936e0b7
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8c in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e5\0711076ac8770cd864d52f8c9e402c761083be
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e5\25e4aa93a233985efa5c6980965c982da2ebb1
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e5\9a65a16d5731fa0068fbdb27c0aa8b23e77f1c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e6\9de29bb2d1d6434b8b29ae775ad8c2e48c5391
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e7\c459bebff100d6a85bd8fd47478eefd6518774
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e9\03f469138532261bea6eb9e185fe490403534c
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc9 in position 22: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\e9\a63bebc59f897bfcd9dba47ab7d62c44dd4314
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xdd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ea\a6379d7ff568b3928386891be87431e5ff37c2
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xf1 in position 22: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\eb\0bd551fbd0728313d2506a006a98dcf3ce2d5d
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xca in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ed\241cfe9cef4bfbdedee9213a46728ed3f5cf9b
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb4 in position 8: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ee\d71643e8aed4707ef95898270508c15ecb108f
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x94 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ef\a77066fea8765db4e8fd7c7cb90f4552e77afb
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f0\2431dcbc87e191f13e71aaa2a241a1aaa9a5d4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd3 in position 6: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f0\c50edbf917d522043159af34a2e6d444df180e
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x8d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f1\e585438b9f9ea720321a283870ca34e9558499
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xda in position 10: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f2\1b2295f71004f83504b9b53baa11fcb390cdf6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 22: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f3\ba9de61264fb775bc62fb9fa6a086230c79af4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f4\37427c723f2716bb1e6d54dde5bd532bd632f9
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xbd in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f4\4e9bbc4bffc0a5955d72b5ec195677dfc8d7ad
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x9d in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f5\3cc46fccfb11d32b826605287372f10bcb1e77
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc9 in position 23: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f6\df3351505ab09ea62622a396aa7cb59eeeeff1
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f7\0ab3ca71f74ca3687a641251a8624ce3586cdb
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xb5 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\f9\867422c063e9ea439b567ef2afb06f4eb0b434
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xed in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fa\9820ad9c9fc2e4c97af8c2e2353588587613e2
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 14: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fb\18f9b4ddd9a0db174e5d0a3d453e2344690ff8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xad in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fb\2eb0c874fd0f9196e363705ea8fcdb30117d81
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fc\57024cbae6ddcf9674749fb76b7cc17bad70c8
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xf1 in position 22: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fd\b19d3a12e64485de2dbf1f567a2b402b7c3316
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd0 in position 3: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fe\79e7225bcbf1077fff19be1e6048abe838ca9b
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xd5 in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\fe\b0a73567855a2ea3e16e2870479f7416dbacf6
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x85 in position 2: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ff\212f6d63b551ff9b9254bb89d0e0641d7afd3a
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xc8 in position 17: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ff\4bc98b7cec201a37c9e5c7f983594e49cba2b4
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0xcd in position 2: invalid continuation byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\objects\ff\e3e680c86868fe4470fe6d7ccd27be6a1b0c44
读取文件出错 (尝试合并内容时): 'utf-8' codec can't decode byte 0x93 in position 3: invalid start byte
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\refs\heads\main
c2a04f00f272a7328de06acb0e4c4331cceff1fd

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\refs\heads\feature\add-SQLite
04eb1421784606bd918c72ce21ee7f0278bebf15

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\refs\remotes\origin\main
c2a04f00f272a7328de06acb0e4c4331cceff1fd

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\.git\refs\remotes\origin\feature\add-SQLite
04eb1421784606bd918c72ce21ee7f0278bebf15

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\alembic\env.py
# FILE: alembic/env.py (Complete: Imports settings, Base, Models, and handles sync URL)
import os
import sys
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

# +++ 添加这部分来设置路径和导入你的应用模块 +++
# 1. 计算项目根目录 (PROJECT_ROOT)
#    确保你的项目结构是 PsychologyAnalysis/alembic/env.py
try:
    # This assumes the env.py file is in PsychologyAnalysis/alembic/
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
except NameError: # Fallback if __file__ is not defined
    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '.'))
    print(f"Warning: __file__ not defined, assuming project root is CWD: {PROJECT_ROOT}")

# 2. 将项目根目录添加到 sys.path，这样 Python 就能找到 'app' 包
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
    print(f"[Alembic env.py] Added project root to sys.path: {PROJECT_ROOT}")
else:
    print(f"[Alembic env.py] Project root already in sys.path: {PROJECT_ROOT}")


# 3. 从你的应用导入 settings, Base, 和所有模型
try:
    # 导入配置
    from app.core.config import settings
    # 导入 SQLAlchemy Base
    from app.db.base_class import Base
    # +++ 显式导入所有需要被 Alembic 管理的模型 +++
    from app.models.user import User           # 导入 User 模型
    from app.models.assessment import Assessment # 导入 Assessment 模型
    # 如果还有其他模型，也在这里导入:
    # from app.models.another_model import AnotherModel
    print("[Alembic env.py] Successfully imported settings, Base, and Models (User, Assessment).")
except ImportError as e:
    print(f"[Alembic env.py] Error importing app modules: {e}")
    print(f"Please ensure PROJECT_ROOT ({PROJECT_ROOT}) is correct and contains the 'app' package "
          f"with core.config, db.base_class, models.user, and models.assessment modules.")
    sys.exit(1)
except Exception as e_import:
     print(f"[Alembic env.py] An unexpected error occurred during import: {e_import}")
     sys.exit(1)
# --- 添加结束 ---


# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    try:
        fileConfig(config.config_file_name)
        print(f"[Alembic env.py] Logging configured from: {config.config_file_name}")
    except Exception as e_log:
        # Avoid crashing if logging config fails, just warn
        print(f"[Alembic env.py] Warning: Could not configure logging from {config.config_file_name}: {e_log}")


# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata

# +++ 设置 Alembic 需要知道的模型元数据 (现在包含了导入的所有模型) +++
print(f"[Alembic env.py] Setting target_metadata from {Base.__module__}.Base")
target_metadata = Base.metadata
# --- 设置结束 ---

try:
    known_tables = list(target_metadata.tables.keys())
    print(f"[Alembic env.py] DEBUG: Tables registered in Base.metadata before comparison: {known_tables}")
except Exception as e_debug:
    print(f"[Alembic env.py] DEBUG: Error getting table names from metadata: {e_debug}")
# +++ 调试代码结束 +++

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def get_sync_database_url() -> str:
    """Gets the database URL from settings and ensures it's sync for Alembic."""
    url = settings.DATABASE_URL
    # Replace async sqlite driver with sync one
    if url and url.startswith("sqlite+aiosqlite"):
        sync_url = url.replace("sqlite+aiosqlite", "sqlite", 1)
        # print(f"[Alembic env.py] Converted DB URL for Alembic: {sync_url}") # Reduced verbosity
        return sync_url
    # Add similar replacements for other async drivers if needed
    # elif url and url.startswith("postgresql+asyncpg"):
    #     return url.replace("postgresql+asyncpg", "postgresql", 1)
    # print(f"[Alembic env.py] Using DB URL as is for Alembic: {url}") # Reduced verbosity
    return url # Return original if no known async driver found


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode."""
    sync_url = get_sync_database_url() # Get the potentially modified sync URL
    print(f"[Alembic env.py] Configuring offline mode with URL: {sync_url}")

    context.configure(
        url=sync_url,
        target_metadata=target_metadata,
        literal_binds=True, # Recommended for script generation
        dialect_opts={"paramstyle": "named"},
        compare_type=True, # Enable type comparison
        compare_server_default=True, # Enable server default comparison
        # render_as_batch=True # Uncomment if needed for SQLite ALTER limitations
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""
    # Get the sync URL
    sync_url = get_sync_database_url()
    # Set the sync URL in the Alembic config object, overriding alembic.ini
    config.set_main_option("sqlalchemy.url", sync_url)
    print(f"[Alembic env.py] Set online mode sqlalchemy.url to: {sync_url}")

    try:
        # Create engine using the sync URL from the config
        connectable = engine_from_config(
            config.get_section(config.config_ini_section, {}),
            prefix="sqlalchemy.",
            poolclass=pool.NullPool, # Use NullPool for migrations
        )
        # print("[Alembic env.py] SYNC Engine created successfully for online mode.") # Reduced verbosity
    except Exception as e_engine:
         print(f"[Alembic env.py] Error creating SYNC engine from config: {e_engine}")
         sys.exit(1)

    # Connect using the synchronous engine
    with connectable.connect() as connection:
        # print("[Alembic env.py] SYNC Connection established for online mode.") # Reduced verbosity
        # Configure the context with the connection and metadata
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
            compare_server_default=True,
            # include_schemas=True, # Uncomment if using PG schemas
            # render_as_batch=True # Often needed for SQLite limitations with ALTER commands
        )
        # print("[Alembic env.py] Context configured for online mode.") # Reduced verbosity

        # Run migrations within a transaction
        try:
            print("[Alembic env.py] Beginning transaction and running migrations...")
            with context.begin_transaction():
                context.run_migrations()
            print("[Alembic env.py] Migrations ran successfully within transaction.")
        except Exception as e_migrate:
             print(f"[Alembic env.py] Error running migrations: {e_migrate}")
             # Consider re-raising if you want the command to fail explicitly
             # raise e_migrate


# --- Determine mode and run ---
if context.is_offline_mode():
    print("[Alembic env.py] Running migrations in offline mode.")
    run_migrations_offline()
else:
    print("[Alembic env.py] Running migrations in online mode.")
    run_migrations_online()

print("[Alembic env.py] Script execution finished.")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\alembic\README
Generic single-database configuration.

1.
alembic revision --autogenerate -m "Create users table and add submitter_id fk"
2.
alembic upgrade head
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\alembic\versions\72143d5034d8_create_users_table.py
"""Create users table

Revision ID: 72143d5034d8
Revises: 
Create Date: 2025-04-24 15:06:06.038114

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '72143d5034d8'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    pass


def downgrade() -> None:
    """Downgrade schema."""
    pass

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\main.py
# FILE: app/main.py (修改后，包含百科路由)
import sys
import os
import logging
import traceback

# --- 添加项目根目录到 sys.path ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
    print(f"[Main App] 已添加 {PROJECT_ROOT} 到 sys.path")
# ------------------------------------

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

# --- 导入配置、解析的 origins 和日志设置 ---
try:
    from app.core.config import settings, parsed_cors_origins
    from src.utils import setup_logging
except ImportError as e:
    print(f"[Main App] 严重错误: 无法导入核心配置或日志设置: {e}")
    traceback.print_exc()
    sys.exit(1)
except Exception as e:
    print(f"[Main App] 严重错误: 在初始导入/配置处理期间发生错误: {e}")
    traceback.print_exc()
    sys.exit(1)
# ---------------------------------------------------------

# --- 设置集中式日志 ---
APP_LOGGER_NAME = settings.APP_NAME or "QingtingzheApp"
try:
    setup_logging(log_level_str=settings.LOG_LEVEL,
                  log_dir_name=os.path.basename(settings.LOGS_DIR),
                  logger_name=APP_LOGGER_NAME)
    logger = logging.getLogger(APP_LOGGER_NAME)
    logger.info("--- 启动 FastAPI 应用 ---")
    logger.info(f"日志记录器 '{APP_LOGGER_NAME}' 配置成功。")
    logger.info(f"应用名称: {settings.APP_NAME}, 环境: {settings.ENVIRONMENT}")
except Exception as e:
    print(f"[Main App] 严重错误: 无法设置日志: {e}")
    traceback.print_exc()
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(APP_LOGGER_NAME)
    logger.error("日志设置失败，使用基础配置。")
# ---------------------------------

# --- 导入 API 路由 ---
try:
    # ******** 修改：添加 encyclopedia 路由导入 ********
    from app.routers import scales, assessments, reports, auth, encyclopedia
    # 如果旧的 tips 路由文件仍然存在，可以不导入它或注释掉：
    # from app.routers import tips
    # **************************************************
    logger.info("成功导入 API 路由。")
except ImportError as e:
    logger.critical(f"严重错误: 无法导入路由: {e}", exc_info=True)
    sys.exit(1)
except Exception as e:
    logger.critical(f"严重错误: 在路由导入期间发生错误: {e}", exc_info=True)
    sys.exit(1)
# -------------------------

# --- 创建 FastAPI 应用实例 ---
app = FastAPI(
    title=settings.APP_NAME,
    description="倾听者 AI 智能警务分析评估应用系统 API",
    version="0.3.1", # 根据需要调整版本号
    openapi_url=f"{settings.API_V1_STR}/openapi.json",
    docs_url="/docs",
    redoc_url="/redoc",
)
logger.info("FastAPI 应用实例已创建。")
# ---------------------------------

# --- CORS 中间件配置 ---
if parsed_cors_origins:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=parsed_cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    logger.info(f"CORS 中间件已为源启用: {parsed_cors_origins}")
else:
    logger.warning("CORS 源列表为空或解析失败。CORS 中间件可能不允许前端访问。")
# ----------------------------------

# --- 挂载静态文件 (已移除) ---
logger.info("静态文件挂载功能已移除。此应用现在专注于 API。")

# --- 提供根前端页面 (已移除) ---
logger.info("根路径 '/' 的 HTML 响应功能已移除。")

# --- 包含 API 路由 ---
logger.info(f"包含 API 路由，前缀: {settings.API_V1_STR}")
app.include_router(auth.router, prefix=settings.API_V1_STR, tags=["认证"]) # Authentication
app.include_router(scales.router, prefix=settings.API_V1_STR, tags=["量表"]) # Scales
app.include_router(assessments.router, prefix=settings.API_V1_STR, tags=["评估"]) # Assessments
app.include_router(reports.router, prefix=settings.API_V1_STR, tags=["报告"]) # Reports
# ******** 修改：添加 encyclopedia 路由包含，移除旧 tips ********
app.include_router(encyclopedia.router, prefix=settings.API_V1_STR, tags=["Encyclopedia"]) # Psychology Encyclopedia
# 如果旧的 tips 路由导入了，需要注释掉或移除：
# app.include_router(tips.router, prefix=settings.API_V1_STR, tags=["Tips"])
# ************************************************************
# -------------------------

# --- 包含页面路由 (已移除) ---
logger.info("页面 (Pages) 路由包含功能已移除。")

# --- 最终启动消息 ---
logger.info("FastAPI 应用已配置并准备就绪 (仅 API)。")
logger.info(f"在 /docs 或 /redoc 访问 API 文档")
# ---------------------------

# --- Uvicorn 运行部分 (如果直接运行) ---
# if __name__ == "__main__":
#     import uvicorn
#     logger.info("启动 Uvicorn 开发服务器...")
#     run_host = settings.YAML_CONFIG.get('HOST', '0.0.0.0')
#     run_port = settings.YAML_CONFIG.get('PORT', 8000)
#     logger.info(f"服务器将在 http://{run_host}:{run_port} 上运行")
#     uvicorn.run("app.main:app", host=run_host, port=run_port, reload=True) # reload=True 仅用于开发
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\__init__.py
# # FILE: app/crud/__init__.py (Corrected)
# from . import user      # 导入 user.py 中的内容
# from . import assessment # +++ 添加这一行，导入 assessment.py 中的内容 +++

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\core\celery_app.py
# app/core/celery_app.py
from celery import Celery
from app.core.config import settings # 导入你的设置

# 配置 Redis 作为 Broker 和 Backend
# 使用 settings 中的配置或默认值
REDIS_URL = "redis://localhost:6379/0" # 默认 Redis 地址和数据库 0
# 你可以在 .env 中添加 REDIS_URL 并从 settings 加载

# 创建 Celery 实例
# main 参数通常是 Celery 应用的入口点名称，这里用 'app' 或项目名
celery_app = Celery(
    "QingtingzheApp", # 与 FastAPI app name 保持一致或自定义
    broker=REDIS_URL,
    backend=REDIS_URL, # 使用 Redis 作为结果存储后端
    include=['app.tasks.analysis'] # 指定包含任务定义的模块列表
)

# 可选：Celery 配置项 (可以放在 settings 或这里)
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],  # Allow json content
    result_serializer='json',
    timezone='Asia/Shanghai', # 设置时区
    enable_utc=True,
    # task_track_started=True, # 如果需要追踪任务开始状态
    # broker_connection_retry_on_startup=True, # 启动时自动重试连接 broker
)

# 可选: 打印确认信息
print(f"[Celery Setup] Celery app configured. Broker: {REDIS_URL}, Backend: {REDIS_URL}")
print(f"[Celery Setup] Included task modules: {celery_app.conf.include}")

# 如果你需要在任务中使用 FastAPI 的依赖项或设置，
# 可以考虑更复杂的设置，但现在保持简单。
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\core\config.py
# app/core/config.py
import os
import sys
import json # <--- 确保导入 json
from typing import List, Union, Optional, Dict, Any
from pydantic_settings import BaseSettings, SettingsConfigDict
import yaml
import logging
import traceback # Import traceback for better error logging

# --- Basic Logging Setup (for config loading itself) ---
# Get a basic logger instance specifically for configuration loading issues
# This avoids potential issues if the main app logger isn't configured yet
config_logger = logging.getLogger("ConfigLoader")
# Set a default level in case main setup hasn't run
config_logger.setLevel(logging.INFO)
# Add a handler if none exist (e.g., running script directly)
if not config_logger.hasHandlers():
    ch = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    config_logger.addHandler(ch)
# ---------------------------------------------------------


# --- Project Root Calculation ---
# __file__ points to this file (config.py)
# os.path.dirname(__file__) points to app/core
# os.path.dirname(os.path.dirname(__file__)) points to app/
# os.path.dirname(os.path.dirname(os.path.dirname(__file__))) points to PsychologyAnalysis/
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
CONFIG_YAML_PATH = os.path.join(PROJECT_ROOT, "config/config.yaml")
DOTENV_PATH = os.path.join(PROJECT_ROOT, ".env")
# +++ 定义百科文件路径 +++
ENCYCLOPEDIA_JSON_PATH = os.path.join(PROJECT_ROOT, "config/psychology_encyclopedia.json")
# --- (可选) 旧的小贴士文件路径 ---
# TIPS_JSON_PATH = os.path.join(PROJECT_ROOT, "config/psychology_tips.json")
# ---------------------------------


class Settings(BaseSettings):
    """
    Application Settings loaded from .env file and YAML.
    .env file takes precedence for overlapping variables.
    """
    # --- Basic App Settings ---
    APP_NAME: str = "Qingtingzhe AI Analysis"
    ENVIRONMENT: str = "development" # Options: development, staging, production
    API_V1_STR: str = "/api/v1"     # Base path for API V1 endpoints
    LOG_LEVEL: str = "INFO"         # Default logging level (DEBUG, INFO, WARNING, ERROR)

    # --- Security Settings (JWT Authentication) ---
    SECRET_KEY: str = "a_very_unsafe_default_secret_key_please_change_in_dotenv"
    ALGORITHM: str = "HS256"        # JWT signing algorithm
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 # 1 day default

    # --- CORS ---
    BACKEND_CORS_ORIGINS_STR: str = "http://localhost:8080,http://127.0.0.1:8080" # Default development origins

    # --- Database ---
    DB_PATH_SQLITE: str = os.path.join(PROJECT_ROOT, "psychology_analysis.db")
    DATABASE_URL: str = f"sqlite+aiosqlite:///{DB_PATH_SQLITE}" # Example for async SQLite

    # --- File Storage ---
    UPLOADS_DIR: str = os.path.join(PROJECT_ROOT, "uploads")
    LOGS_DIR: str = os.path.join(PROJECT_ROOT, "logs")

    # --- Dashscope API ---
    DASHSCOPE_API_KEY: Optional[str] = None

    # --- Settings potentially loaded from config.yaml (as fallbacks or defaults) ---
    TEXT_MODEL: str = "qwen-plus"
    VISION_MODEL: str = "qwen-vl-plus"
    REPORT_PROMPT_TEMPLATE: Optional[str] = None
    YAML_CONFIG: Dict[str, Any] = {}

    # +++ Psychology Encyclopedia Settings +++
    PSYCHOLOGY_ENCYCLOPEDIA_FILE: str = ENCYCLOPEDIA_JSON_PATH # 使用上面定义的路径
    PSYCHOLOGY_ENTRIES: List[Dict[str, str]] = [] # 初始化为空列表

    # --- (可选) 旧的小贴士字段 ---
    # PSYCHOLOGY_TIPS_FILE: str = TIPS_JSON_PATH
    # PSYCHOLOGY_TIPS: List[str] = []

    # Pydantic Settings Configuration
    model_config = SettingsConfigDict(
        env_file=DOTENV_PATH, # Explicitly point to .env file
        extra='ignore'        # Ignore extra fields not defined in Settings
    )

# --- Helper function to load YAML ---
def load_yaml_config(yaml_path: str) -> Dict[str, Any]:
    """Loads configuration from a YAML file."""
    if not os.path.exists(yaml_path):
        config_logger.warning(f"YAML config file not found at {yaml_path}. Returning empty config.")
        return {}
    try:
        with open(yaml_path, 'r', encoding='utf-8') as f:
            config_data = yaml.safe_load(f)
            if config_data is None:
                config_logger.warning(f"YAML config file at {yaml_path} is empty. Returning empty config.")
                return {}
            config_logger.info(f"Successfully loaded configuration from {yaml_path}")
            return config_data
    except Exception as e:
        config_logger.error(f"Error loading YAML config from {yaml_path}: {e}", exc_info=True)
        return {}

# +++ Helper function to load Encyclopedia JSON +++
def load_encyclopedia_from_json(encyclopedia_path: str) -> List[Dict[str, str]]:
    """从 JSON 文件加载百科条目列表"""
    if not os.path.exists(encyclopedia_path):
        config_logger.warning(f"百科文件未找到: {encyclopedia_path}. 返回空列表.")
        return []
    try:
        with open(encyclopedia_path, 'r', encoding='utf-8') as f:
            entries_list = json.load(f)
            if not isinstance(entries_list, list):
                config_logger.error(f"百科文件 {encyclopedia_path} 格式错误，根元素不是列表.")
                return []

            valid_entries = []
            required_keys = {"category", "title", "content"}
            for i, entry in enumerate(entries_list):
                if isinstance(entry, dict) and required_keys.issubset(entry.keys()):
                    # 基本验证：确保值是字符串且不为空
                    if all(isinstance(entry[key], str) and entry[key].strip() for key in required_keys):
                        valid_entries.append({
                            "category": entry["category"].strip(),
                            "title": entry["title"].strip(),
                            "content": entry["content"].strip()
                        })
                    else:
                        config_logger.warning(f"百科文件 {encyclopedia_path} 中第 {i+1} 条记录的值无效 (非字符串或为空)，已跳过。")
                else:
                    config_logger.warning(f"百科文件 {encyclopedia_path} 中第 {i+1} 条记录格式错误或缺少键，已跳过。")

            config_logger.info(f"成功从 {encyclopedia_path} 加载 {len(valid_entries)} 条有效的百科条目。")
            return valid_entries
    except json.JSONDecodeError as e:
        config_logger.error(f"解析百科 JSON 文件 {encyclopedia_path} 时出错: {e}", exc_info=True)
        return []
    except Exception as e:
        config_logger.error(f"加载百科文件 {encyclopedia_path} 时发生意外错误: {e}", exc_info=True)
        return []

# --- (可选) 旧的 tips 加载函数 ---
# def load_tips_from_json(tips_path: str) -> List[str]: ...

# --- Instantiate Settings (Loads from .env) ---
try:
    settings = Settings()
except Exception as e:
    config_logger.critical(f"CRITICAL ERROR during Settings initialization (loading .env): {e}")
    traceback.print_exc()
    sys.exit(1) # Exit if basic settings fail to load

# --- CRITICAL SECURITY CHECK ---
if settings.SECRET_KEY == "a_very_unsafe_default_secret_key_please_change_in_dotenv":
    config_logger.critical("="*30 + " CRITICAL SECURITY WARNING " + "="*30)
    config_logger.critical("SECRET_KEY is using the default unsafe value!")
    config_logger.critical("Please generate a strong secret key and set it in the .env file.")
    config_logger.critical("Example generation: openssl rand -hex 32")
    config_logger.critical("="*80 + "\n")
    # sys.exit(1)
# -----------------------------

# --- Load and merge YAML config ---
yaml_config_data = load_yaml_config(CONFIG_YAML_PATH)
settings.YAML_CONFIG = yaml_config_data

# Override settings from YAML if needed (check if already set by .env)
if settings.DASHSCOPE_API_KEY is None:
    yaml_api_key = yaml_config_data.get("api_key")
    if yaml_api_key:
        settings.DASHSCOPE_API_KEY = yaml_api_key
        config_logger.warning("Loaded DASHSCOPE_API_KEY from YAML (recommend using .env).")
if settings.TEXT_MODEL == "qwen-plus":
    settings.TEXT_MODEL = yaml_config_data.get("text_model", settings.TEXT_MODEL)
if settings.VISION_MODEL == "qwen-vl-plus":
    settings.VISION_MODEL = yaml_config_data.get("vision_model", settings.VISION_MODEL)
if settings.REPORT_PROMPT_TEMPLATE is None:
    settings.REPORT_PROMPT_TEMPLATE = yaml_config_data.get("REPORT_PROMPT_TEMPLATE", settings.REPORT_PROMPT_TEMPLATE)

# --- Load Psychology Encyclopedia ---
settings.PSYCHOLOGY_ENTRIES = load_encyclopedia_from_json(settings.PSYCHOLOGY_ENCYCLOPEDIA_FILE)
if not settings.PSYCHOLOGY_ENTRIES:
    config_logger.warning("未能加载任何心理百科条目，相关功能可能受影响。")

# --- (可选) Load old tips ---
# settings.PSYCHOLOGY_TIPS = load_tips_from_json(settings.PSYCHOLOGY_TIPS_FILE)
# if not settings.PSYCHOLOGY_TIPS:
#     config_logger.warning("未能加载任何心理学小贴士。")

# --- Construct Database URL ---
config_logger.info(f"Using Database URL: {settings.DATABASE_URL}")

# --- Ensure Necessary Directories Exist ---
try:
    os.makedirs(settings.UPLOADS_DIR, exist_ok=True)
    os.makedirs(settings.LOGS_DIR, exist_ok=True)
    config_logger.info(f"Ensured directories exist: Uploads='{settings.UPLOADS_DIR}', Logs='{settings.LOGS_DIR}'")
except OSError as e:
     config_logger.error(f"Error creating necessary directories: {e}", exc_info=True)

# --- Check if essential API key is loaded ---
if not settings.DASHSCOPE_API_KEY:
    config_logger.critical("="*30 + " CRITICAL WARNING " + "="*30)
    config_logger.critical("DASHSCOPE_API_KEY is not set in .env or config.yaml!")
    config_logger.critical("AI functionality will likely fail.")
    config_logger.critical("Please set it in the .env file (recommended) or config/config.yaml.")
    config_logger.critical("="*80 + "\n")

# --- Manually Parse CORS Origins String ---
def parse_cors_origins(origins_str: str) -> List[str]:
    """Parses a comma-separated string of origins into a list of strings."""
    if not origins_str or not isinstance(origins_str, str):
        config_logger.warning(f"BACKEND_CORS_ORIGINS_STR is empty or invalid ('{origins_str}'). Using default: ['http://localhost:8080']")
        return ["http://localhost:8080"]
    try:
        parsed = [origin.strip() for origin in origins_str.split(",") if origin.strip()]
        if not parsed:
             config_logger.warning(f"Parsing BACKEND_CORS_ORIGINS_STR '{origins_str}' resulted in empty list. Using default: ['http://localhost:8080']")
             return ["http://localhost:8080"]
        config_logger.info(f"Successfully parsed CORS origins: {parsed}")
        return parsed
    except Exception as e:
        config_logger.error(f"Error parsing BACKEND_CORS_ORIGINS_STR '{origins_str}': {e}. Using default: ['http://localhost:8080']", exc_info=True)
        return ["http://localhost:8080"]

parsed_cors_origins: List[str] = parse_cors_origins(settings.BACKEND_CORS_ORIGINS_STR)
# --------------------------------------


# --- Log Final Effective Settings ---
config_logger.info("--- Effective Application Settings ---")
config_logger.info(f"APP_NAME: {settings.APP_NAME}")
config_logger.info(f"ENVIRONMENT: {settings.ENVIRONMENT}")
config_logger.info(f"LOG_LEVEL: {settings.LOG_LEVEL}")
config_logger.info(f"API_V1_STR: {settings.API_V1_STR}")
config_logger.info(f"DATABASE_URL: {settings.DATABASE_URL}")
config_logger.info(f"UPLOADS_DIR: {settings.UPLOADS_DIR}")
config_logger.info(f"LOGS_DIR: {settings.LOGS_DIR}")
config_logger.info(f"CORS Origins (Parsed): {parsed_cors_origins}")
config_logger.info(f"JWT Algorithm: {settings.ALGORITHM}")
config_logger.info(f"Token Expire Minutes: {settings.ACCESS_TOKEN_EXPIRE_MINUTES}")
config_logger.info(f"SECRET_KEY Loaded: {'Yes' if settings.SECRET_KEY != 'a_very_unsafe_default_secret_key_please_change_in_dotenv' else 'NO (Using default - UNSAFE!)'}")
config_logger.info(f"DASHSCOPE_API_KEY Loaded: {'Yes' if settings.DASHSCOPE_API_KEY else 'NO - CRITICAL!'}")
config_logger.info(f"Text Model: {settings.TEXT_MODEL}")
config_logger.info(f"Vision Model: {settings.VISION_MODEL}")
config_logger.info(f"Encyclopedia Entries Loaded: {len(settings.PSYCHOLOGY_ENTRIES)}") # <--- Log loaded entry count
# config_logger.info(f"Tips Loaded (Old method): {len(settings.PSYCHOLOGY_TIPS)}") # If keeping old tips
config_logger.info("------------------------------------")

# `settings` instance can now be imported by other modules.
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\core\deps.py
# app/core/deps.py
import logging
from typing import AsyncGenerator, Optional # Use AsyncGenerator for async yield
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from sqlalchemy.ext.asyncio import AsyncSession # Import AsyncSession

# --- Core App Imports ---
from app.core.config import settings
from app.core.security import decode_access_token # Your JWT decoding function

# --- Database and CRUD Imports ---
from app.db.session import AsyncSessionLocal # Import the async session maker
from app import crud, models, schemas # Adjust imports based on your project structure

logger = logging.getLogger(settings.APP_NAME) # Use the main app logger

# --- OAuth2 Scheme Definition ---
# Define the URL where clients will send username/password to get a token.
# This should match the path operation of your login endpoint.
oauth2_scheme = OAuth2PasswordBearer(tokenUrl=f"{settings.API_V1_STR}/auth/token")

# --- Asynchronous Database Session Dependency ---
async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """
    Dependency that provides an asynchronous database session per request.
    It ensures the session is properly closed afterwards.
    """
    async with AsyncSessionLocal() as session:
        try:
            yield session
            # Optional: You could uncomment the next line if you want commits
            # to happen automatically at the end of successful requests,
            # but manual commits in CRUD functions are generally preferred
            # for better control.
            # await session.commit()
        except Exception as e:
            # Rollback in case of exceptions during the request handling
            logger.error(f"Database session error: {e}", exc_info=True)
            await session.rollback()
            # Re-raise the exception so FastAPI can handle it
            raise
        # Session is automatically closed when exiting the 'async with' block

# --- Asynchronous Current User Dependency ---
async def get_current_user(
    db: AsyncSession = Depends(get_db),          # Depend on the async get_db
    token: str = Depends(oauth2_scheme)          # Get token from Authorization header
) -> models.User:                                # Return the SQLAlchemy User model
    """
    Decodes the JWT token, validates it, and retrieves the current user
    from the database asynchronously. Raises HTTPException if invalid.
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )

    # Decode the token using your security utility
    token_data = decode_access_token(token)
    if token_data is None or token_data.username is None:
        logger.warning("Token decoding failed or username missing in token payload.")
        raise credentials_exception

    # Retrieve the user from the database asynchronously using the async CRUD function
    try:
        # Ensure crud.user.get_user_by_username is an async function
        user = await crud.user.get_user_by_username(db, username=token_data.username)
    except Exception as e:
        logger.error(f"Database error while fetching user '{token_data.username}': {e}", exc_info=True)
        # Don't expose internal DB errors directly, raise the standard credentials exception
        raise credentials_exception

    if user is None:
        logger.warning(f"User '{token_data.username}' found in token but not in database.")
        raise credentials_exception

    # Return the validated user object
    return user

# --- Asynchronous Active User Dependency ---
async def get_current_active_user(
    current_user: models.User = Depends(get_current_user), # Depend on get_current_user
) -> models.User:
    """
    Ensures the user retrieved from the token is marked as active.
    """
    if not current_user.is_active:
        logger.warning(f"Authentication attempt by inactive user: {current_user.username}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Inactive user")
    return current_user

# --- Asynchronous Superuser Dependency ---
async def get_current_active_superuser(
    current_user: models.User = Depends(get_current_active_user), # Depend on active user
) -> models.User:
    """
    Ensures the current user is active AND is a superuser.
    Raises HTTPException if not a superuser.
    """
    if not current_user.is_superuser:
        logger.warning(f"Superuser access denied for non-superuser: {current_user.username}")
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="The user doesn't have enough privileges",
        )
    return current_user
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\core\security.py
# app/core/security.py
from datetime import datetime, timedelta, timezone
from typing import Any, Union, Optional
from jose import jwt, JWTError
from passlib.context import CryptContext
from app.core.config import settings
from app.schemas.token import TokenData # 导入令牌数据模式

# 使用 bcrypt 哈希算法
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

ALGORITHM = settings.ALGORITHM
SECRET_KEY = settings.SECRET_KEY
ACCESS_TOKEN_EXPIRE_MINUTES = settings.ACCESS_TOKEN_EXPIRE_MINUTES

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """验证明文密码与哈希密码是否匹配"""
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    """生成密码的哈希值"""
    return pwd_context.hash(password)

def create_access_token(
    subject: Union[str, Any], expires_delta: Optional[timedelta] = None
) -> str:
    """创建 JWT 访问令牌"""
    if expires_delta:
        expire = datetime.now(timezone.utc) + expires_delta
    else:
        # 使用配置中的过期时间
        expire = datetime.now(timezone.utc) + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    # 令牌中至少包含过期时间和主题 (subject, 通常是用户名)
    to_encode = {"exp": expire, "sub": str(subject)}
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

def decode_access_token(token: str) -> Optional[TokenData]:
    """解码访问令牌，验证其有效性"""
    try:
        # 解码 JWT
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        # 提取用户名
        username: str | None = payload.get("sub")
        if username is None:
            # 令牌中没有 'sub' 字段
            return None
        # 你可以在这里添加更多的载荷验证逻辑 (例如：检查 scopes)
        # 返回包含用户名的 TokenData 对象
        return TokenData(username=username)
    except JWTError:
        # 令牌无效 (格式错误、签名不匹配、已过期等)
        return None
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\core\__init__.py

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\crud\assessment.py
# FILE: app/crud/assessment.py

import logging
from typing import Optional, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
# sqlalchemy.future.select is usually needed for async queries, keep if used elsewhere
from sqlalchemy.future import select

# 导入你的 Assessment 模型 和 配置
from app.models.assessment import Assessment
from app.core.config import settings

# 获取日志记录器
logger = logging.getLogger(settings.APP_NAME)

# --- 已有的 get 函数 (示例，确保它存在) ---
async def get(db: AsyncSession, id: int) -> Optional[Assessment]:
    """通过 ID 异步获取评估记录。"""
    logger.debug(f"正在通过 ID 获取评估记录 (CRUD): {id}")
    result = await db.execute(select(Assessment).filter(Assessment.id == id))
    return result.scalar_one_or_none()

# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# +++  确保添加或替换为下面的 async def create 函数定义   +++
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
async def create(db: AsyncSession, **kwargs: Any) -> Assessment:
    """
    异步创建一条新的评估记录。
    接受包含字段的关键字参数，键名应与 Assessment 模型字段匹配。
    """
    logger.info(f"正在为 '{kwargs.get('subject_name', 'N/A')}' 创建新的评估记录 (CRUD)")

    # 直接从关键字参数创建模型实例
    # 路由层已经处理了字段映射，这里直接使用 kwargs
    try:
        db_obj = Assessment(**kwargs)
        logger.debug(f"Assessment 对象创建成功，待存入: {db_obj}")
    except TypeError as te:
        # 如果 kwargs 中的键与 Assessment 模型字段不匹配，这里会抛出 TypeError
        logger.error(f"创建 Assessment 模型实例时字段不匹配: {te}. KWARGS: {kwargs}", exc_info=True)
        # 重新抛出 ValueError，可以被路由层的异常处理捕获
        raise ValueError(f"模型初始化失败，请检查字段: {te}") from te

    # 将新对象添加到数据库会话
    db.add(db_obj)

    try:
        # 提交事务以保存到数据库
        await db.commit()
        # 刷新对象以获取数据库生成的值（如自增 ID, 默认时间戳）
        await db.refresh(db_obj)
        logger.info(f"评估记录创建成功 (CRUD)，ID: {db_obj.id}")
        # 返回创建成功的对象
        return db_obj
    except Exception as e:
        # 如果提交失败，回滚事务
        await db.rollback()
        logger.error(f"创建评估记录时数据库提交失败 (CRUD): {e}", exc_info=True)
        # 重新引发异常，让路由层处理 HTTP 响应
        raise e
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# +++                 create 函数定义结束                   +++
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


# --- 已有的 update_report_text 函数 (示例，确保它存在) ---
async def update_report_text(db: AsyncSession, assessment_id: int, report_text: str) -> Optional[Assessment]:
    """异步更新指定评估记录的 report_text。"""
    logger.debug(f"尝试更新报告文本 (CRUD)，ID: {assessment_id}")
    db_obj = await get(db, id=assessment_id) # 复用 get 函数
    if not db_obj:
        logger.warning(f"尝试更新不存在的评估记录报告 (CRUD)，ID: {assessment_id}")
        return None
    db_obj.report_text = report_text
    # updated_at 应该由数据库触发器或 SQLAlchemy 的 onupdate 自动处理
    db.add(db_obj) # 标记对象已修改
    try:
        await db.commit()
        await db.refresh(db_obj)
        logger.info(f"评估记录 ID {assessment_id} 的报告文本已更新 (CRUD)。")
        return db_obj
    except Exception as e:
        await db.rollback()
        logger.error(f"更新评估记录 ID {assessment_id} 的报告文本时数据库提交失败 (CRUD): {e}", exc_info=True)
        raise e

# 你可以根据需要添加其他的异步 CRUD 函数，比如 get_multi, remove, update 等。
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\crud\user.py
# app/crud/user.py
import logging
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select # For SQLAlchemy 2.0 style selects

# --- Core App Imports ---
from app.core.security import get_password_hash, verify_password # Your security utils
from app.models.user import User # Your SQLAlchemy User model
from app.schemas.user import UserCreate, UserUpdate # Your Pydantic schemas for User
from app.core.config import settings

logger = logging.getLogger(settings.APP_NAME)

# --- Async User CRUD Operations ---

async def get_user(db: AsyncSession, user_id: int) -> User | None:
    """
    Asynchronously retrieves a user by their ID.
    """
    logger.debug(f"Attempting to get user by ID: {user_id}")
    result = await db.execute(select(User).filter(User.id == user_id))
    user = result.scalar_one_or_none()
    if user:
        logger.debug(f"User found by ID: {user_id}")
    else:
        logger.debug(f"User not found by ID: {user_id}")
    return user

async def get_user_by_username(db: AsyncSession, username: str) -> User | None:
    """
    Asynchronously retrieves a user by their username (case-insensitive search recommended).
    """
    logger.debug(f"Attempting to get user by username: {username}")
    # Consider lowercasing username for case-insensitive lookup if needed:
    # username_lower = username.lower()
    # result = await db.execute(select(User).filter(func.lower(User.username) == username_lower))
    result = await db.execute(select(User).filter(User.username == username))
    user = result.scalar_one_or_none()
    if user:
        logger.debug(f"User found by username: {username}")
    else:
        logger.debug(f"User not found by username: {username}")
    return user


async def get_user_by_email(db: AsyncSession, email: str) -> User | None:
    """
    Asynchronously retrieves a user by their email (case-insensitive search recommended).
    """
    logger.debug(f"Attempting to get user by email: {email}")
    # email_lower = email.lower()
    # result = await db.execute(select(User).filter(func.lower(User.email) == email_lower))
    result = await db.execute(select(User).filter(User.email == email))
    user = result.scalar_one_or_none()
    if user:
        logger.debug(f"User found by email: {email}")
    else:
        logger.debug(f"User not found by email: {email}")
    return user

async def create_user(db: AsyncSession, *, user_in: UserCreate) -> User:
    """
    Asynchronously creates a new user in the database.
    """
    logger.info(f"Attempting to create user with username: {user_in.username}")
    # Check if username or email already exists (optional but good practice)
    existing_user_by_username = await get_user_by_username(db, username=user_in.username)
    if existing_user_by_username:
        logger.warning(f"Username '{user_in.username}' already exists.")
        # Consider raising a custom exception or returning None/error indicator
        raise ValueError(f"Username '{user_in.username}' is already registered.")
    if user_in.email:
        existing_user_by_email = await get_user_by_email(db, email=user_in.email)
        if existing_user_by_email:
            logger.warning(f"Email '{user_in.email}' already exists.")
            raise ValueError(f"Email '{user_in.email}' is already registered.")

    # Hash the password
    hashed_password = get_password_hash(user_in.password)

    # Create the SQLAlchemy User model instance
    db_user = User(
        username=user_in.username, # Consider storing username lowercase: .lower()
        email=user_in.email,       # Consider storing email lowercase: .lower()
        full_name=user_in.full_name,
        hashed_password=hashed_password,
        is_active=user_in.is_active if user_in.is_active is not None else True,
        is_superuser=user_in.is_superuser if user_in.is_superuser is not None else False
    )

    # Add the new user object to the session
    db.add(db_user)

    # Commit the transaction to save the user to the database
    try:
        await db.commit()
        logger.info(f"User '{user_in.username}' committed to database.")
    except Exception as e:
        await db.rollback() # Rollback on error
        logger.error(f"Database commit failed while creating user '{user_in.username}': {e}", exc_info=True)
        raise # Re-raise the exception

    # Refresh the object to get ID and defaults set by the database
    await db.refresh(db_user)
    logger.info(f"User '{db_user.username}' created successfully with ID: {db_user.id}")
    return db_user


async def update_user(db: AsyncSession, *, db_user: User, user_in: UserUpdate) -> User:
    """
    Asynchronously updates an existing user's information.
    """
    logger.info(f"Attempting to update user ID: {db_user.id}")
    # Get the dictionary representation of the input schema, excluding unset fields
    update_data = user_in.model_dump(exclude_unset=True) # Use model_dump in Pydantic V2

    # If password is being updated, hash the new one
    if "password" in update_data and update_data["password"]:
        hashed_password = get_password_hash(update_data["password"])
        del update_data["password"] # Remove plain password from update dict
        update_data["hashed_password"] = hashed_password
        logger.info(f"Password updated for user ID: {db_user.id}")
    elif "password" in update_data:
        # Handle case where password might be None or empty string in update schema
        del update_data["password"] # Don't update password if not provided a valid new one

    # Update the user object's attributes
    for field, value in update_data.items():
        setattr(db_user, field, value)

    # Add the updated user object to the session (marks it as dirty)
    db.add(db_user)

    # Commit the changes
    try:
        await db.commit()
        logger.info(f"User ID {db_user.id} changes committed.")
    except Exception as e:
        await db.rollback()
        logger.error(f"Database commit failed while updating user ID {db_user.id}: {e}", exc_info=True)
        raise

    # Refresh the object
    await db.refresh(db_user)
    logger.info(f"User ID {db_user.id} updated successfully.")
    return db_user

# Add other async CRUD functions as needed (e.g., get_multi, remove)
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\crud\__init__.py
# app/crud/__init__.py
from . import user # 导入同目录下的 user.py 文件作为一个模块
from . import assessment # +++ 添加这一行，导入 assessment.py 中的内容 +++
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\db\base_class.py
from sqlalchemy.orm import declarative_base

# 创建所有 ORM 模型将继承的基类
Base = declarative_base()
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\db\init_db.py
# app/db/init_db.py (使用 run_sync 修正)
import logging
import asyncio  # 1. 导入 asyncio 库
from app.db.session import async_engine # 确保你导入的是 async_engine
# 导入所有需要创建表的模型，以及 Base
from app.db.base_class import Base
from app.models.user import User # 导入 User 模型
# from app.models.assessment import Assessment # 如果有其他模型，也导入

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 2. 将 init_db 函数改为异步函数 (async def)
async def init_db() -> None:
    """
    根据 SQLAlchemy 模型异步地创建数据库表。
    """
    logger.info("Attempting to create database tables asynchronously...")
    try:
        # 3. 使用异步上下文管理器获取连接
        async with async_engine.begin() as conn:
            logger.info("Acquired async connection. Running create_all synchronously...")
            # 4. 在异步连接上，使用 run_sync 来执行同步的 create_all 方法
            # 这会将 create_all 的执行委托给事件循环的线程池
            await conn.run_sync(Base.metadata.create_all)
            logger.info("Base.metadata.create_all executed via run_sync.")

        logger.info("Database tables created successfully (if they didn't exist).")
    except Exception as e:
        logger.error(f"Error creating database tables: {e}", exc_info=True)
        raise e
    # finally:
        # 通常不需要手动 dispose，async with 会处理好连接释放
        # 如果需要确保引擎完全关闭（比如脚本结束时），可以取消注释下面两行
        # await async_engine.dispose()
        # logger.info("Async engine disposed.")

if __name__ == "__main__":
    print("Running database initialization...")
    # 5. 使用 asyncio.run() 来运行顶层的异步函数 init_db
    try:
        asyncio.run(init_db())
        print("Database initialization finished successfully.")
    except Exception as e:
        # 捕获在 init_db 中可能重新抛出的异常
        print(f"Database initialization failed: {e}")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\db\session.py
# app/db/session.py
import logging
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession
from app.core.config import settings # 导入你的设置

logger = logging.getLogger(settings.APP_NAME) # Or use a specific logger

# --- Asynchronous Database Engine ---
# Create an asynchronous engine using the DATABASE_URL from settings.
# Ensure settings.DATABASE_URL is like "sqlite+aiosqlite:///path/to/your.db"
logger.info(f"Creating async engine for database: {settings.DATABASE_URL}")
try:
    async_engine = create_async_engine(
        settings.DATABASE_URL,
        # echo=True,  # Uncomment for debugging SQL statements
        future=True  # Enables SQLAlchemy 2.0 style features
        # connect_args can be added here if needed, but typically not for aiosqlite
    )
    logger.info("Async engine created successfully.")
except Exception as e:
    logger.critical(f"Failed to create async engine: {e}", exc_info=True)
    raise e # Re-raise the exception to stop application startup if engine fails

# --- Asynchronous Database Session Maker ---
# Create an asynchronous session factory configured to use the async engine.
# expire_on_commit=False is recommended for FastAPI dependency usage,
# preventing attributes from being expired after commit within a request.
AsyncSessionLocal = async_sessionmaker(
    bind=async_engine,
    class_=AsyncSession,       # Specify the use of AsyncSession
    expire_on_commit=False,    # Keep objects accessible after commit within the session scope
    autocommit=False,          # Standard setting, commits are manual
    autoflush=False            # Standard setting, flushing is manual or on commit
)
logger.info("AsyncSessionLocal (async session maker) configured.")

# Note: You will typically use this AsyncSessionLocal in your dependency
# injection function (e.g., get_db in deps.py) to get session instances.
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\db\__init__.py
# 可以为空，或者导入 Base 和 SessionLocal 以便更容易访问
from .base_class import Base
#from .session import SessionLocal, engine
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\models\assessment.py
# app/models/assessment.py
from sqlalchemy import Column, Integer, String, Boolean, Text, TIMESTAMP, ForeignKey
from sqlalchemy.sql import func
from app.db.base_class import Base

class Assessment(Base):
    __tablename__ = "analysis_data" # 假设这是你现有的表名

    id = Column(Integer, primary_key=True, index=True)
    image_path = Column(Text, nullable=True)
    subject_name = Column(String(200), index=True)
    age = Column(Integer)
    gender = Column(String(10))
    questionnaire_type = Column(String(100), nullable=True)
    questionnaire_data = Column(Text, nullable=True) # 存储 JSON 字符串
    report_text = Column(Text, nullable=True)

    # 时间戳 - 推荐使用数据库默认值或 SQLAlchemy 的 server_default
    created_at = Column(TIMESTAMP, server_default=func.now())
    # 对于 updated_at，SQLAlchemy 的 onupdate 在某些后端可能效果不佳，
    # SQLite 需要触发器，MySQL/PostgreSQL 可以用 onupdate=func.now()
    # 如果使用 SQLite 触发器，这里可以只设置 server_default
    updated_at = Column(TIMESTAMP, server_default=func.now(), onupdate=func.now())

    # 确保包含所有在 data_handler._init_db 中添加的列
    id_card = Column(String(50), unique=True, index=True, nullable=True)
    occupation = Column(String(100), nullable=True)
    case_name = Column(String(200), nullable=True)
    case_type = Column(String(100), nullable=True)
    identity_type = Column(String(100), nullable=True)
    person_type = Column(String(100), nullable=True)
    marital_status = Column(String(50), nullable=True)
    children_info = Column(Text, nullable=True)
    criminal_record = Column(Integer, default=0)
    health_status = Column(Text, nullable=True)
    phone_number = Column(String(50), nullable=True)
    domicile = Column(String(200), nullable=True)

    # 外键，链接到提交此评估的用户
    # 确保 users 表和 User 模型存在
    submitter_id = Column(Integer, ForeignKey("users.id"), nullable=True) # 假设提交者可选

    # (可选) 如果你想在查询 Assessment 时方便地访问 User 对象，可以定义关系
    # from sqlalchemy.orm import relationship
    # from .user import User # 导入 User 模型
    # submitter = relationship("User")

    def __repr__(self):
        return f"<Assessment(id={self.id}, subject='{self.subject_name}', type='{self.questionnaire_type}')>"
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\models\user.py
from sqlalchemy import Column, Integer, String, Boolean
from app.db.base_class import Base # 从我们创建的基类导入

class User(Base):
    __tablename__ = "users" # 数据库中的表名

    id = Column(Integer, primary_key=True, index=True)
    # 用户名，唯一且加索引，不允许为空
    username = Column(String(100), unique=True, index=True, nullable=False)
    # 邮箱，唯一且加索引，可以为空
    email = Column(String(255), unique=True, index=True, nullable=True)
    # 存储哈希后的密码，不允许为空
    hashed_password = Column(String(255), nullable=False)
    # 全名，可以为空
    full_name = Column(String(100), nullable=True)
    # 是否激活，默认为 True
    is_active = Column(Boolean(), default=True, nullable=False)
    # 是否为超级管理员，默认为 False
    is_superuser = Column(Boolean(), default=False, nullable=False)

    # __repr__ 方法用于方便调试时打印对象信息
    def __repr__(self):
        return f"<User(id={self.id}, username='{self.username}', email='{self.email}')>"

# 你可以在这里定义其他模型，如 Assessment, Report 等
# 它们都需要继承自 Base
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\models\__init__.py
# app/models/__init__.py

# 从同级目录下的 user.py 文件中导入 User 类
from .user import User
# 从同级目录下的 assessment.py 文件中导入 Assessment 类 (如果已创建)
from .assessment import Assessment

# (可选，但推荐) 定义 __all__ 列表，明确指定从 'from app.models import *' 时应导入的内容
__all__ = [
    "User",
    "Assessment",
    # 未来添加的其他模型也在此处列出
]
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\routers\assessments.py
# FILE: app/routers/assessments.py (Corrected)
import logging
import os
import json
from datetime import datetime
from fastapi import APIRouter, HTTPException, Depends, File, UploadFile, Form, Request, status
from typing import Dict, Any, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.exc import IntegrityError # Import for potential db errors

# --- Core App Imports ---
from app.core.config import settings
from app.schemas.assessment import AssessmentSubmitResponse
# Ensure your Celery task is importable
try:
    from app.tasks.analysis import run_ai_analysis
except ImportError:
    run_ai_analysis = None # Define as None if import fails
    logging.getLogger(settings.APP_NAME or "FallbackLogger").error("Failed to import Celery task 'run_ai_analysis'. Background processing disabled.")


# --- Authentication & Database Imports ---
from app.core.deps import get_current_active_user, get_db # Use async get_db
from app import models, schemas # Import schemas for potential use
# Import the main crud package (ensure app/crud/__init__.py imports assessment)
from app import crud

# --- Utils ---
try:
    from werkzeug.utils import secure_filename
except ImportError:
    import re
    def secure_filename(filename):
        if not filename: return "invalid_filename"
        # Remove potentially unsafe characters
        filename = re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)
        # Remove leading/trailing dots or underscores
        filename = filename.strip('._')
        # Ensure filename is not empty after stripping
        return filename if filename else "invalid_filename"
    # Get logger safely
    logging.getLogger(settings.APP_NAME or "FallbackLogger").warning("werkzeug not installed. Using basic secure_filename fallback.")


logger = logging.getLogger(settings.APP_NAME) # Use the configured app logger
router = APIRouter()

# --- API Endpoint ---
@router.post(
    "/assessments/submit",
    response_model=AssessmentSubmitResponse,
    tags=["Assessments"],
    status_code=status.HTTP_202_ACCEPTED # Return 202 Accepted on success
)
async def submit_assessment(
    # --- Dependencies ---
    db: AsyncSession = Depends(get_db),
    request: Request = None, # Keep for form data parsing
    current_user: models.User = Depends(get_current_active_user), # Authentication dependency

    # --- Form Fields (Match names used in FormData in JS) ---
    name: str = Form(..., description="姓名"),
    gender: str = Form(..., description="性别"),
    age: int = Form(..., gt=0, description="年龄"),
    id_card: Optional[str] = Form(None, description="身份证号"),
    occupation: Optional[str] = Form(None, description="职业"),
    case_name: Optional[str] = Form(None, description="案件名称"),
    case_type: Optional[str] = Form(None, description="案件类型"),
    identity_type: Optional[str] = Form(None, description="人员身份"),
    person_type: Optional[str] = Form(None, description="人员类型"),
    marital_status: Optional[str] = Form(None, description="婚姻状况"),
    children_info: Optional[str] = Form(None, description="子女情况"),
    criminal_record: Optional[int] = Form(0, ge=0, le=1, description="有无犯罪前科 (0:无, 1:有)"),
    health_status: Optional[str] = Form(None, description="健康情况"),
    phone_number: Optional[str] = Form(None, description="手机号"),
    domicile: Optional[str] = Form(None, description="归属地"),
    scale_type: Optional[str] = Form(None, description="选择的量表代码"),
    # Ensure 'image' matches the name attribute of the file input in HTML/JS
    image: Optional[UploadFile] = File(None, description="上传的绘画图片")
):
    """
    Receives assessment data from an authenticated user.
    Saves the data and queues a background task for AI analysis.
    """
    # +++ Get username and ID *before* potential database errors +++
    submitter_username = current_user.username
    submitter_id = current_user.id
    logger.info(f"用户 '{submitter_username}' (ID: {submitter_id}) 正在提交新的评估，主体姓名: {name}")

    # --- 1. 收集基础信息 (Map Form fields to DB model fields) ---
    basic_info = {
        "subject_name": name, # Maps 'name' from Form to 'subject_name' in DB model
        "gender": gender,
        "age": age,
        "id_card": id_card,
        "occupation": occupation,
        "case_name": case_name,
        "case_type": case_type,
        "identity_type": identity_type,
        "person_type": person_type,
        "marital_status": marital_status,
        "children_info": children_info,
        "criminal_record": criminal_record, # Already int 0 or 1
        "health_status": health_status,
        "phone_number": phone_number,
        "domicile": domicile,
        "submitter_id": submitter_id # Add the authenticated user's ID
    }
    logger.debug(f"收集的基础信息 (待存入数据库): {basic_info}")

    # --- 2. 处理图片上传 ---
    image_relative_path = None
    image_full_path = None # Keep track of full path for saving
    image_was_saved_to_disk = False # Flag for cleanup on error

    if image and image.filename:
        # Sanitize filename
        original_filename = secure_filename(image.filename)
        if original_filename == "invalid_filename":
             logger.warning(f"用户 {submitter_username} 上传了无效的文件名。")
             # Optionally raise HTTPException or proceed without image
             # raise HTTPException(status_code=400, detail="无效的文件名。")
             image = None # Treat as no image uploaded

        if image: # Check again if image is still valid after filename check
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            id_part = secure_filename(id_card if id_card else (name if name else 'UnknownID'))
            base, ext = os.path.splitext(original_filename)
            safe_base = base[:50] # Limit base filename length
            # Standardize extension to lowercase
            ext_lower = ext.lower()
            allowed_extensions = {'.png', '.jpg', '.jpeg', '.gif'} # Example allowed types
            if ext_lower not in allowed_extensions:
                logger.warning(f"用户 {submitter_username} 上传了不允许的文件类型: {ext_lower}")
                raise HTTPException(status_code=400, detail=f"不允许的文件类型: {ext}. 请上传 {', '.join(allowed_extensions)} 文件。")

            # Construct safe filename for saving
            image_filename_to_save = f"{id_part}_{timestamp}_{safe_base}{ext_lower}"
            image_full_path = os.path.join(settings.UPLOADS_DIR, image_filename_to_save)
            # Store relative path (or just the filename) in the DB
            image_relative_path = image_filename_to_save # Or adjust based on how you serve files

            try:
                # Ensure upload directory exists
                os.makedirs(settings.UPLOADS_DIR, exist_ok=True)
                # Read file content asynchronously and save
                file_content = await image.read()
                with open(image_full_path, "wb") as buffer:
                    buffer.write(file_content)
                image_was_saved_to_disk = True # Mark as saved for potential cleanup
                logger.info(f"图片由用户 {submitter_username} 保存至: {image_full_path}")
            except Exception as e:
                logger.error(f"用户 {submitter_username} 保存上传图片至 {image_full_path} 时出错: {e}", exc_info=True)
                # Don't proceed if saving failed, inform user
                raise HTTPException(status_code=500, detail=f"保存上传文件时出错: {e}")
            finally:
                 # Ensure file is closed (UploadFile should handle this, but good practice)
                 await image.close()
    else:
        logger.info(f"用户 {submitter_username} 未上传图片。")


    # --- 3. 收集量表答案 (From dynamic form fields q1, q2...) ---
    scale_answers_dict = {}
    scale_answers_json = None
    if scale_type:
        if request is None:
             logger.error("未注入 Request 对象，无法解析量表答案。")
             # This indicates a server-side setup issue
             raise HTTPException(status_code=500, detail="内部服务器错误: 无法访问请求对象。")
        try:
            # Get all form data asynchronously
            form_data = await request.form()
            for key, value in form_data.items():
                # Check if the key starts with 'q' followed by digits
                if key.startswith('q') and key[1:].isdigit():
                    scale_answers_dict[key] = value

            if scale_answers_dict:
                # Convert the collected answers to a JSON string for DB storage
                scale_answers_json = json.dumps(scale_answers_dict, ensure_ascii=False)
                logger.info(f"用户 {submitter_username} 为量表 '{scale_type}' 收集到的答案: {len(scale_answers_dict)} 条")
                logger.debug(f"量表答案 (JSON): {scale_answers_json}")
            else:
                logger.warning(f"用户 {submitter_username} 提供了量表类型 '{scale_type}', 但未在表单中找到以 'q' 开头的答案。")
                # Depending on requirements, you might raise an error or allow submission without answers
                # raise HTTPException(status_code=400, detail=f"选择了量表 '{scale_type}' 但未提供答案。")
        except Exception as e:
             logger.error(f"用户 {submitter_username} 解析量表答案时出错: {e}", exc_info=True)
             # Proceed without scale data or raise error
             scale_answers_json = None # Ensure it's None if parsing fails
             # raise HTTPException(status_code=400, detail=f"解析量表答案时出错: {e}")

    # --- 4. 使用异步 CRUD 保存初始数据 ---
    assessment_id = None
    try:
        # --- *** Access assessment CRUD via the imported crud package *** ---
        # This now requires app/crud/__init__.py to contain `from . import assessment`
        new_assessment = await crud.assessment.create(
            db=db,
            # Pass collected data using keyword arguments matching Assessment model fields
            **basic_info, # Unpack the dictionary of basic info
            image_path=image_relative_path, # Store relative path/filename
            questionnaire_type=scale_type,
            questionnaire_data=scale_answers_json, # Store the JSON string
            report_text=None # Initial report text is empty
        )

        # Check if creation was successful and we got an ID
        if not new_assessment or not hasattr(new_assessment, 'id'):
             # This indicates an unexpected issue with the CRUD function or DB commit
             raise ValueError("数据保存操作未返回有效的评估对象ID。")

        assessment_id = new_assessment.id
        logger.info(f"评估数据由用户 {submitter_username} 保存成功。评估 ID: {assessment_id}")

    except IntegrityError as ie:
        # Handle specific DB constraint errors (like unique ID card if constraint exists)
        logger.warning(f"用户 {submitter_username} 保存评估数据时发生数据库完整性错误: {ie}", exc_info=True)
        await db.rollback()
        # Cleanup potentially saved image if DB save failed
        if image_was_saved_to_disk and image_full_path and os.path.exists(image_full_path):
            try: os.remove(image_full_path); logger.info(f"因数据库完整性错误清理了文件 {image_full_path}")
            except Exception as rm_err: logger.warning(f"数据库错误后无法移除文件 {image_full_path}: {rm_err}")
        raise HTTPException(status_code=409, detail=f"数据保存冲突: 可能身份证号已存在。") # 409 Conflict is appropriate
    except ValueError as ve:
        # Catch custom errors raised, e.g., from the CRUD function itself
        logger.warning(f"用户 {submitter_username} 保存评估数据时发生值错误: {ve}", exc_info=True)
        await db.rollback()
        if image_was_saved_to_disk and image_full_path and os.path.exists(image_full_path):
            try: os.remove(image_full_path); logger.info(f"因值错误清理了文件 {image_full_path}")
            except Exception as rm_err: logger.warning(f"数据库错误后无法移除文件 {image_full_path}: {rm_err}")
        raise HTTPException(status_code=400, detail=f"数据保存错误: {ve}")
    except Exception as e:
        # --- *** Use the pre-fetched username for logging *** ---
        logger.error(f"用户 {submitter_username} 保存评估数据时发生意外错误: {e}", exc_info=True)
        await db.rollback() # Rollback the session

        # --- Cleanup uploaded image if DB save failed ---
        if image_was_saved_to_disk and image_full_path and os.path.exists(image_full_path):
            try:
                os.remove(image_full_path)
                logger.info(f"因数据库意外错误清理了上传文件 {image_full_path}")
            except Exception as rm_err:
                logger.warning(f"数据库错误后无法移除文件 {image_full_path}: {rm_err}")

        # Re-raise as HTTPException for FastAPI to handle
        error_detail = f"数据库处理错误: {type(e).__name__}"
        # Consider hiding specific error details in production: error_detail = "服务器内部数据库错误"
        raise HTTPException(status_code=500, detail=error_detail)


    # --- 5. 触发 Celery 任务 ---
    task_id = None
    if assessment_id: # Only queue if data was saved successfully
        if run_ai_analysis:
            try:
                # Pass only the ID needed for the task
                task = run_ai_analysis.delay(assessment_id)
                task_id = task.id
                logger.info(f"已为评估 ID: {assessment_id} (提交者: {submitter_username}) 排队 AI 分析任务。任务 ID: {task_id}")
            except Exception as celery_err:
                 # Log the error, but the request itself was successful (data saved)
                 logger.error(f"为评估 ID {assessment_id} (提交者: {submitter_username}) 排队 Celery 任务失败: {celery_err}", exc_info=True)
                 # Don't raise HTTPException here, as the primary action (saving data) succeeded.
                 # The response message will indicate the queueing failure.
        else:
             logger.warning(f"Celery task 'run_ai_analysis' 未加载或不可用。评估 ID: {assessment_id} 的后台处理将不会运行。")


    # --- 6. 构建 API 响应 (Based on successful save and task queuing status) ---
    if assessment_id:
        if task_id:
            message = "评估数据已接收，正在后台进行 AI 分析。"
            status_code_resp = "processing_queued"
        elif run_ai_analysis is None: # Check if task function itself is None
            message = f"评估数据已接收 (ID: {assessment_id})，但后台分析任务未配置或导入失败。"
            status_code_resp = "warning_task_unavailable"
        else: # Task function exists but .delay() failed
            message = f"评估数据已接收 (ID: {assessment_id})，但启动后台处理任务时出错。"
            status_code_resp = "warning_queueing_failed"

        return AssessmentSubmitResponse(
            status=status_code_resp,
            message=message,
            submission_id=assessment_id
            # task_id=task_id # Optionally include task_id in response
        )
    else:
        # This case should ideally not be reached if exceptions are handled correctly above
        logger.error(f"评估 ID 未能生成，但未捕获到明确异常。提交者: {submitter_username}")
        raise HTTPException(status_code=500, detail="数据保存后未能获取评估ID。")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\routers\auth.py
# FILE: app/routers/auth.py
import logging
from datetime import timedelta
from typing import Any

from fastapi import APIRouter, Depends, HTTPException, status, Body # +++ Import Body
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.exc import IntegrityError # +++ Import for catching DB errors

from app.core.config import settings
from app.core import security
from app import crud, models, schemas
from app.core.deps import get_db, get_current_active_user

logger = logging.getLogger(settings.APP_NAME)
router = APIRouter()

# --- Existing Login Endpoint (No changes needed here) ---
@router.post("/auth/token", response_model=schemas.Token, tags=["Authentication"])
async def login_for_access_token(
    db: AsyncSession = Depends(get_db),
    form_data: OAuth2PasswordRequestForm = Depends()
) -> Any:
    """
    OAuth2 compatible token login, get an access token for future requests.
    Accepts standard form data with 'username' and 'password'.
    """
    logger.info(f"User login attempt: {form_data.username}")
    user = await crud.user.get_user_by_username(db, username=form_data.username)

    if not user or not security.verify_password(form_data.password, user.hashed_password):
        logger.warning(f"Login failed for user: {form_data.username}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )

    if not user.is_active:
        logger.warning(f"Inactive user login attempt: {form_data.username}")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Inactive user")

    access_token = security.create_access_token(subject=user.username)
    logger.info(f"User login successful: {form_data.username}")

    return {"access_token": access_token, "token_type": "bearer"}

# +++ NEW Registration Endpoint +++
@router.post("/auth/register", response_model=schemas.User, status_code=status.HTTP_201_CREATED, tags=["Authentication"])
async def register_user(
    *,
    db: AsyncSession = Depends(get_db),
    # Expecting JSON body with username and password
    user_in: schemas.UserCreate = Body(...) # Use UserCreate schema, require body
) -> Any:
    """
    Create new user. Requires username and password.
    """
    logger.info(f"User registration attempt: {user_in.username}")

    # Check if user already exists
    existing_user = await crud.user.get_user_by_username(db, username=user_in.username)
    if existing_user:
        logger.warning(f"Username '{user_in.username}' already registered.")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Username already registered",
        )

    # Ensure password field is present (Pydantic validation should handle this, but double-check)
    if not user_in.password:
         raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="Password is required for registration.",
        )

    try:
        # Create the user using the async CRUD function
        # UserCreate schema already handles default values for optional fields
        # Password hashing happens inside crud.user.create_user
        user = await crud.user.create_user(db=db, user_in=user_in)
        logger.info(f"User '{user.username}' registered successfully with ID: {user.id}")
        # Return the created user data (excluding password) using the User schema
        return user
    except IntegrityError: # Catch potential race conditions or other DB unique constraint errors
        await db.rollback()
        logger.error(f"Database integrity error during registration for {user_in.username}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Username or email might already be registered (database error).",
        )
    except ValueError as ve: # Catch specific errors raised from CRUD (like explicit duplicate check)
         await db.rollback()
         logger.warning(f"Registration failed for {user_in.username}: {ve}")
         raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(ve), # Return the specific error message (e.g., "Username already registered")
         )
    except Exception as e:
        await db.rollback()
        logger.error(f"Unexpected error during registration for {user_in.username}: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An internal error occurred during registration.",
        )


# --- Existing /users/me Endpoint (No changes needed) ---
@router.get("/users/me", response_model=schemas.User, tags=["Users"])
async def read_users_me(
    current_user: models.User = Depends(get_current_active_user)
):
     """
     Get current logged in user's details.
     """
     return current_user
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\routers\encyclopedia.py
# 文件: app/routers/encyclopedia.py
import logging
import random
from typing import Optional, List, Dict
from fastapi import APIRouter, HTTPException, Query, status

from app.core.config import settings
from app.schemas.encyclopedia import EncyclopediaEntry, CategoriesResponse, EntriesResponse # 导入模型

logger = logging.getLogger(settings.APP_NAME)
router = APIRouter()

# --- 辅助函数：从加载的条目中获取数据 ---
def get_all_entries() -> List[Dict[str, str]]:
    """安全地获取配置中的百科条目列表"""
    entries = settings.PSYCHOLOGY_ENTRIES
    if not isinstance(entries, list):
        logger.error("配置中的 PSYCHOLOGY_ENTRIES 不是列表！")
        return []
    return entries

# --- 端点 1: 获取分类 ---
@router.get(
    "/encyclopedia/categories",
    response_model=CategoriesResponse,
    tags=["Encyclopedia"],
    summary="获取所有心理百科分类"
)
async def get_encyclopedia_categories():
    """
    返回所有心理百科条目的唯一分类名称列表。
    """
    all_entries = get_all_entries()
    if not all_entries:
        return CategoriesResponse(categories=[])

    # 提取所有分类并去重，然后排序
    try:
        categories = sorted(list(set(entry.get("category", "未分类") for entry in all_entries)))
        return CategoriesResponse(categories=categories)
    except Exception as e:
         logger.error(f"提取百科分类时出错: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail="处理分类列表时出错")


# --- 端点 2: 获取条目 (包含过滤和随机功能) ---
@router.get(
    "/encyclopedia/entries",
    # 响应模型根据情况可能是列表或单个条目，用 Union 或 Any，或者为随机单独建模型/端点
    # 为清晰起见，我们让它主要返回列表，随机情况特殊处理返回单个条目模型
    response_model=EntriesResponse, # 主要返回列表
    tags=["Encyclopedia"],
    summary="获取心理百科条目"
)
async def get_encyclopedia_entries(
    category: Optional[str] = Query(None, description="按分类过滤条目"),
    random_tip: Optional[bool] = Query(False, description="是否从'心理小贴士'分类中随机获取一条") # 参数名改为 random_tip
    # 可以添加分页参数: page: int = Query(1, ge=1), size: int = Query(10, ge=1, le=100)
):
    """
    获取心理百科条目。
    - 提供 `category` 参数以按分类过滤。
    - 提供 `random_tip=true` 以获取一条随机的“心理小贴士”。(此时忽略 category 参数)
    """
    all_entries = get_all_entries()
    if not all_entries:
        if random_tip:
             # 返回一个默认的 EncyclopediaEntry 结构
             return EntriesResponse(entries=[EncyclopediaEntry(category="心理小贴士", title="提示", content="暂无可用小贴士。")])
        else:
             return EntriesResponse(entries=[]) # 返回空列表

    target_entries = all_entries

    # --- 处理随机小贴士逻辑 ---
    if random_tip:
        tips_category_name = "心理小贴士" # 明确指定分类名称
        tip_entries = [entry for entry in all_entries if entry.get("category") == tips_category_name]

        if not tip_entries:
            logger.warning("请求随机小贴士，但 '心理小贴士' 分类下没有条目。")
            # 返回一个默认的 EncyclopediaEntry 结构，放入列表中
            return EntriesResponse(entries=[EncyclopediaEntry(category=tips_category_name, title="提示", content="暂无可用小贴士。")])

        selected_entry_dict = random.choice(tip_entries)
        # 将选中的字典包装在列表中返回，以匹配 EntriesResponse
        selected_entry_model = EncyclopediaEntry(**selected_entry_dict)
        return EntriesResponse(entries=[selected_entry_model]) # 返回包含单个随机条目的列表

    # --- 处理按分类过滤逻辑 (如果不是请求随机小贴士) ---
    if category:
        target_entries = [entry for entry in all_entries if entry.get("category") == category]
        if not target_entries:
             # 如果指定了分类但找不到，返回空列表是合理的
             logger.info(f"请求分类 '{category}'，但未找到条目。")
             return EntriesResponse(entries=[])

    # --- (可选) 实现分页 ---
    # total = len(target_entries)
    # start = (page - 1) * size
    # end = start + size
    # paged_entries_dicts = target_entries[start:end]

    # --- 将字典列表转换为 Pydantic 模型列表 ---
    # 如果没有分页，直接使用 target_entries
    try:
        result_entries = [EncyclopediaEntry(**entry_dict) for entry_dict in target_entries]
    except Exception as e:
         logger.error(f"将百科条目字典转换为 Pydantic 模型时出错: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail="处理百科条目数据时出错")

    # 返回结果
    return EntriesResponse(entries=result_entries) # , total=total, page=page, size=size) 如果实现分页
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\routers\reports.py
# app/routers/reports.py
import logging
import json
from fastapi import APIRouter, HTTPException, Depends, status
# 导入 AsyncSession
from sqlalchemy.ext.asyncio import AsyncSession
# 导入 Pydantic 验证错误
from pydantic import ValidationError

# --- Core App Imports ---
from app.core.config import settings
# 假设 ReportResponse 结构是 { "report": ReportData | None, "message": str | None }
# 假设 ReportData 是包含报告细节的 Pydantic 模型，且配置了 from_attributes=True
from app.schemas.report import ReportResponse, ReportData

# --- Authentication & Database Imports ---
from app.core.deps import get_current_active_user, get_db
from app import models # 包含 User 和 Assessment 模型
from app import crud   # 包含 assessment CRUD 操作

logger = logging.getLogger(settings.APP_NAME)
router = APIRouter()

@router.get(
    # 注意：从之前的日志看，前端请求的是 /reports/{id}，如果 API 前缀是 /api/v1，
    # 那么这里的路径应该是 "/" 或者 "/{assessment_id}"，取决于 APIRouter 如何包含
    # 假设 APIRouter 前缀是 /api/v1/reports，那么这里是 "/{assessment_id}"
    "/{assessment_id}",
    response_model=ReportResponse, # 响应模型，会过滤掉 ReportData 中未定义的字段
    summary="获取指定评估的分析报告",
    tags=["Reports"],
    responses={ # 添加可能的响应状态码文档
        status.HTTP_200_OK: {"description": "成功获取报告或报告正在生成"},
        status.HTTP_404_NOT_FOUND: {"description": "评估记录未找到"},
        status.HTTP_403_FORBIDDEN: {"description": "无权访问此报告"},
        status.HTTP_500_INTERNAL_SERVER_ERROR: {"description": "服务器内部错误"},
    }
)
async def get_report_by_id(
    assessment_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: models.User = Depends(get_current_active_user) # 添加认证
):
    """
    根据评估 ID 获取单个评估报告的详细信息。
    - 如果报告已生成，返回包含报告内容的 ReportData。
    - 如果报告正在生成中，返回指示信息。
    需要用户已登录。
    """
    logger.info(f"用户 '{current_user.username}' (ID: {current_user.id}) 正在请求评估 ID: {assessment_id} 的报告")

    try:
        # 1. 从数据库异步获取评估记录
        # 假设 crud.assessment.get 是 async 函数
        assessment: models.Assessment | None = await crud.assessment.get(db=db, id=assessment_id)

        # 2. 检查评估是否存在
        if not assessment:
            logger.warning(f"评估 ID {assessment_id} 未找到，请求用户: {current_user.username} (ID: {current_user.id})。")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"评估 ID {assessment_id} 不存在或已被删除。",
            )

        # 3. 权限检查 (示例 - 根据需要取消注释或修改)
        # 假设 Assessment 模型有关联的 submitter_id
        # if not current_user.is_superuser and assessment.submitter_id != current_user.id:
        #     logger.warning(f"用户 '{current_user.username}' (ID: {current_user.id}) 尝试访问不属于自己的评估 {assessment_id}")
        #     raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="您无权访问此报告")

        # 4. 检查报告文本是否已生成
        if not assessment.report_text or assessment.report_text.strip() == "":
            logger.info(f"评估 ID: {assessment_id} 的报告尚未生成或为空。")
            # 返回 200 OK，但在响应体中告知前端报告未就绪
            return ReportResponse(report=None, message="报告正在生成中，请稍后刷新。")

        # 5. 报告已生成，尝试从 ORM 对象创建 ReportData Pydantic 模型
        logger.info(f"评估 ID: {assessment_id} 的报告已找到，正在准备响应数据。")
        try:
            # 假设 ReportData 配置了 from_attributes=True (ORM 模式)
            # 这会尝试从 assessment ORM 对象自动填充 ReportData 的字段
            report_data = ReportData.model_validate(assessment, from_attributes=True)

            # 如果 questionnaire_data 需要特殊处理 (例如从 JSON 字符串解析)
            if isinstance(assessment.questionnaire_data, str):
                try:
                    parsed_q_data = json.loads(assessment.questionnaire_data)
                    # 假设 ReportData 有 questionnaire_data 字段
                    report_data.questionnaire_data = parsed_q_data
                except json.JSONDecodeError:
                    logger.warning(f"无法解码评估 ID {assessment_id} 的 questionnaire_data JSON 字符串。在报告中保留原始字符串或设为错误标记。")
                    # 可以选择保留原始字符串或设置为特定错误标记
                    # report_data.questionnaire_data = {"error": "问卷数据解析失败"}
                    # 或者如果 ReportData 允许字符串：
                    # report_data.questionnaire_data = assessment.questionnaire_data

            # 6. 返回成功的响应
            logger.info(f"成功为评估 ID: {assessment_id} 构建报告响应。")
            return ReportResponse(report=report_data, message="报告获取成功。")

        except ValidationError as pydantic_err:
            # 如果即使配置了 from_attributes=True 仍然出错，说明 ReportData 的字段
            # 与 Assessment 模型字段不匹配，或者数据类型有问题。
            logger.error(f"从评估对象 (ID: {assessment_id}) 创建 ReportData 时发生 Pydantic 验证错误: {pydantic_err}", exc_info=True)
            # 记录原始数据可能有助于调试（注意隐私）
            # logger.debug(f"导致错误的评估对象属性: {assessment.__dict__}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="处理报告数据格式时出错，请联系管理员。"
            )

    except HTTPException as http_exc:
        # 直接重新抛出已知的 HTTP 异常 (如 404, 403)
        raise http_exc
    except Exception as e:
        # 捕捉其他所有意外错误
        logger.exception(f"获取评估 ID {assessment_id} 报告时发生意外服务器错误。用户: {current_user.username} (ID: {current_user.id})", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"获取报告时发生未知错误，请稍后重试或联系支持。",
        )

# 你可能还需要其他端点，例如列出用户自己的报告
# @router.get("/", response_model=list[ReportListItem], tags=["Reports"]) # ReportListItem 是一个简化的 Schema
# async def get_my_reports(
#     db: AsyncSession = Depends(get_db),
#     current_user: models.User = Depends(get_current_active_user),
#     skip: int = 0,
#     limit: int = 100
# ):
#     """获取当前用户提交的所有评估报告列表（简要信息）。"""
#     logger.info(f"用户 '{current_user.username}' 请求他/她自己的报告列表。")
#     # 假设有 crud.assessment.get_multi_by_submitter
#     assessments = await crud.assessment.get_multi_by_submitter(
#         db=db, submitter_id=current_user.id, skip=skip, limit=limit
#     )
#     # 将 assessments 转换为 ReportListItem 列表返回
#     return assessments
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\routers\scales.py
# app/routers/scales.py
import logging
import json # Ensure json is imported
from fastapi import APIRouter, HTTPException, Depends

# Import Pydantic models and other necessary components
from app.schemas.scale import ScaleInfo, ScaleQuestion, ScaleOption, AvailableScalesResponse, ScaleQuestionsResponse
from src.data_handler import DataHandler
from app.core.config import settings

# Get the logger instance configured in main.py
# Ensure the logger name matches the one used in main.py's setup_logging call
logger = logging.getLogger(settings.APP_NAME)

# Create an APIRouter instance
router = APIRouter()

# --- Dependency Injection ---
# def get_data_handler():
#     """Dependency function to get a DataHandler instance."""
#     try:
#         # Use the database path from the application settings
#         return DataHandler(db_path=settings.DB_PATH)
#     except Exception as e:
#         # Log the error and raise an HTTP exception if DataHandler fails to initialize
#         logger.error(f"Failed to initialize DataHandler: {e}", exc_info=True)
#         raise HTTPException(status_code=500, detail="Database handler initialization failed.")
def get_data_handler():
    try:
        # <<< FIX: 使用 DB_PATH_SQLITE >>>
        return DataHandler(db_path=settings.DB_PATH_SQLITE)
    except Exception as e:
        logger.error(f"Failed to initialize DataHandler: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"数据库处理程序初始化失败: {e}")

# --- API Endpoints ---

@router.get(
    "/scales",
    response_model=AvailableScalesResponse, # Use keyword argument for response_model
    tags=["Scales"]                          # Use keyword argument for tags
)
async def get_available_scales(dh: DataHandler = Depends(get_data_handler)):
    """
    Retrieves a list of all available scale types (code and name).
    """
    logger.info("Request received for available scales.")
    try:
        # Fetch scale types from the data handler
        scales = dh.get_all_scale_types() # This should return List[Dict] e.g. [{'code': 'SAS', 'name': '...'}, ...]

        # Convert the list of dictionaries to a list of Pydantic models
        scale_infos = [ScaleInfo(code=s["code"], name=s["name"]) for s in scales]

        # Return the response using the Pydantic response model
        return AvailableScalesResponse(scales=scale_infos)
    except Exception as e:
        # Log any unexpected errors during the process
        logger.error(f"Error fetching available scales: {e}", exc_info=True)
        # Raise a generic 500 error to the client
        raise HTTPException(status_code=500, detail="Failed to fetch scale types from database.")


@router.get(
    "/scales/{scale_code}/questions",
    response_model=ScaleQuestionsResponse, # Use keyword argument
    tags=["Scales"]                          # Use keyword argument
)
async def get_scale_questions(scale_code: str, dh: DataHandler = Depends(get_data_handler)):
    """
    Retrieves all questions for a specific scale based on its code.
    """
    logger.info(f"Request received for questions of scale: {scale_code}")
    try:
        # Load question data using the data handler
        # This method should return List[Dict] or None
        questions_data = dh.load_questions_by_type(scale_code)

        # Handle case where no questions are found for the given scale code
        if questions_data is None:
            logger.warning(f"No questions found for scale code: {scale_code}")
            raise HTTPException(status_code=404, detail=f"Scale with code '{scale_code}' not found or has no questions.")

        questions_list = []
        # Iterate through the raw question data from the database
        for q_data in questions_data:
            # --- Data Validation and Transformation ---
            # Ensure the basic structure of question data is present
            if not all(k in q_data for k in ('number', 'text', 'options')):
                logger.warning(f"Skipping question data due to missing keys: {q_data}")
                continue # Skip this malformed question data

            options_list = []
            options_data_from_db = q_data.get('options', [])

            # Ensure options data is a list before processing
            if not isinstance(options_data_from_db, list):
                logger.warning(f"Options data for Q{q_data['number']} is not a list, skipping options. Data: {options_data_from_db}")
            else:
                # Iterate through options for the current question
                for opt in options_data_from_db:
                    # Ensure option structure is correct
                    if not isinstance(opt, dict) or not all(k in opt for k in ('text', 'score')):
                        logger.warning(f"Skipping invalid option data for Q{q_data['number']}: {opt}")
                        continue # Skip malformed option data

                    text_val = opt['text']
                    score_val_raw = opt['score']
                    score_val_numeric = None

                    # Explicitly convert score to a numeric type (int or float)
                    try:
                        if isinstance(score_val_raw, (int, float)):
                            score_val_numeric = score_val_raw
                        elif isinstance(score_val_raw, str):
                            # Attempt conversion from string
                            try:
                                score_float = float(score_val_raw)
                                score_val_numeric = int(score_float) if score_float.is_integer() else score_float
                            except ValueError:
                                logger.error(f"Could not convert score string '{score_val_raw}' to number for Q{q_data['number']}, option '{text_val}'. Skipping option.")
                                continue # Skip this option if conversion fails
                        else:
                            # Handle unexpected score types
                            logger.warning(f"Unexpected type for score ({type(score_val_raw)}) for Q{q_data['number']}, option '{text_val}'. Using 0 as fallback.")
                            score_val_numeric = 0

                    except Exception as conv_err:
                         logger.error(f"Error converting score '{score_val_raw}' for Q{q_data['number']}, option '{text_val}': {conv_err}", exc_info=True)
                         continue # Skip option on unexpected conversion error

                    # Only create ScaleOption if score conversion was successful
                    if score_val_numeric is not None:
                        try:
                            # Create Pydantic model instance for the option
                            options_list.append(ScaleOption(text=str(text_val), score=score_val_numeric))
                        except Exception as pydantic_option_err:
                            # Catch potential errors during Pydantic model instantiation
                            logger.error(f"Pydantic error creating ScaleOption for Q{q_data['number']}, option '{text_val}': {pydantic_option_err}", exc_info=True)
                            continue # Skip option if it fails validation

            # Create Pydantic model instance for the question
            try:
                questions_list.append(ScaleQuestion(
                    number=int(q_data['number']), # Ensure number is integer
                    text=str(q_data['text']),    # Ensure text is string
                    options=options_list
                ))
            except Exception as pydantic_q_err:
                # Catch potential errors during Pydantic model instantiation
                logger.error(f"Pydantic error creating ScaleQuestion for Q{q_data.get('number', 'N/A')}: {pydantic_q_err}", exc_info=True)
                continue # Skip this question if it fails validation

        # Log successful processing and return the validated response
        logger.info(f"Successfully processed {len(questions_list)} questions for scale {scale_code}.")
        return ScaleQuestionsResponse(questions=questions_list)

    except HTTPException as http_exc:
        # Re-raise known HTTP exceptions (like 404)
        raise http_exc
    except Exception as e:
        # Log unexpected errors and return a generic 500 response
        logger.error(f"Unexpected error fetching/processing questions for scale {scale_code}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to fetch questions for scale {scale_code}.")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\routers\__init__.py

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\assessment.py
#评估提交相关
# app/schemas/assessment.py
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional

# --- 用于 POST /api/assessments/submit 的请求体 (部分数据将来自 Form) ---
# Pydantic 模型通常用于 JSON body，但 FastAPI 也能从 Form 字段映射
# 这里定义基础信息字段，方便验证和文档化，实际接收用 Form(...)
class BasicInfoSubmit(BaseModel):
    name: str = Field(..., description="姓名")
    gender: str = Field(..., description="性别")
    id_card: Optional[str] = Field(None, description="身份证号")
    age: int = Field(..., gt=0, description="年龄")
    occupation: Optional[str] = Field(None, description="职业")
    case_name: Optional[str] = Field(None, description="案件名称")
    case_type: Optional[str] = Field(None, description="案件类型")
    identity_type: Optional[str] = Field(None, description="人员身份")
    person_type: Optional[str] = Field(None, description="人员类型")
    marital_status: Optional[str] = Field(None, description="婚姻状况")
    children_info: Optional[str] = Field(None, description="子女情况")
    criminal_record: Optional[int] = Field(0, ge=0, le=1, description="有无犯罪前科 (0:无, 1:有)")
    health_status: Optional[str] = Field(None, description="健康情况")
    phone_number: Optional[str] = Field(None, description="手机号")
    domicile: Optional[str] = Field(None, description="归属地")
    # 注意：scale_type 和 scale_answers 会作为独立的 Form 字段传入

# --- 用于 POST /api/assessments/submit 的响应 ---
class AssessmentSubmitResponse(BaseModel):
    status: str = "success" # "success" or "error"
    message: str
    submission_id: Optional[int] = None # 成功时返回 ID
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\encyclopedia.py
# 文件: app/schemas/encyclopedia.py
from pydantic import BaseModel
from typing import List, Optional

class EncyclopediaEntry(BaseModel):
    """单个百科条目的结构"""
    category: str
    title: str
    content: str

class CategoriesResponse(BaseModel):
    """获取分类列表的响应"""
    categories: List[str]

class EntriesResponse(BaseModel):
    """获取条目列表的响应"""
    entries: List[EncyclopediaEntry]
    # 可以添加分页信息 (如果需要)
    # total: Optional[int] = None
    # page: Optional[int] = None
    # size: Optional[int] = None

# 可以复用 EncyclopediaEntry 作为随机条目的响应，或者定义一个更简单的
# class RandomTipResponse(BaseModel):
#     tip: str
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\report.py
#报告相关
# app/schemas/report.py
from pydantic import BaseModel
from typing import Optional, Dict, Any
from datetime import datetime

class ReportData(BaseModel):
    """报告详情的响应模型"""
    id: int
    image_path: Optional[str] = None
    subject_name: Optional[str] = None
    age: Optional[int] = None
    gender: Optional[str] = None
    questionnaire_type: Optional[str] = None
    questionnaire_data: Optional[Dict[str, Any]] = None # 解析后的 JSON
    report_text: Optional[str] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    id_card: Optional[str] = None
    occupation: Optional[str] = None
    case_name: Optional[str] = None
    case_type: Optional[str] = None
    identity_type: Optional[str] = None
    person_type: Optional[str] = None
    marital_status: Optional[str] = None
    children_info: Optional[str] = None
    criminal_record: Optional[int] = None
    health_status: Optional[str] = None
    phone_number: Optional[str] = None
    domicile: Optional[str] = None
    # ... 可以添加未来需要的其他报告字段 ...

class ReportResponse(BaseModel):
    """获取报告的 API 响应"""
    report: Optional[ReportData] = None
    error: Optional[str] = None # 如果报告未找到或处理中
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\scale.py
# 量表相关
# app/schemas/scale.py
from pydantic import BaseModel
from typing import List, Dict, Any, Optional

class ScaleOption(BaseModel):
    """量表问题的选项"""
    text: str
    score: int | float # 分数可能是整数或浮点数

class ScaleQuestion(BaseModel):
    """量表问题结构"""
    number: int
    text: str
    options: List[ScaleOption]

class ScaleInfo(BaseModel):
    """量表基本信息 (用于列表显示)"""
    code: str # 量表代码 (e.g., 'SAS', 'Personality')
    name: str # 量表显示名称

# --- 用于 /api/scales/{scale_code}/questions 的响应 ---
class ScaleQuestionsResponse(BaseModel):
    questions: Optional[List[ScaleQuestion]] = None
    error: Optional[str] = None # 如果找不到问题，可以返回错误信息

# --- 用于 /api/scales 的响应 ---
class AvailableScalesResponse(BaseModel):
    scales: List[ScaleInfo]
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\token.py
# app/schemas/token.py
from pydantic import BaseModel
from typing import Optional

class Token(BaseModel):
    """响应模型：登录成功后返回给客户端的令牌"""
    access_token: str
    token_type: str = "bearer" # 标准 OAuth2 类型

class TokenData(BaseModel):
    """内部模型：JWT 令牌载荷 (Payload) 的数据结构"""
    username: Optional[str] = None
    # 你未来可以在这里添加其他需要存储在令牌中的信息，
    # 例如用户 ID 或角色/权限 (scopes)
    # scopes: List[str] = []
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\user.py
# app/schemas/user.py (示例)
from pydantic import BaseModel, EmailStr, Field
from typing import Optional

class UserBase(BaseModel):
    username: str = Field(..., min_length=3, max_length=50)
    email: Optional[EmailStr] = None
    full_name: Optional[str] = Field(None, max_length=100)
    is_active: Optional[bool] = True
    is_superuser: Optional[bool] = False

class UserCreate(UserBase):
    password: str = Field(..., min_length=6) # 创建时接收明文密码

class UserUpdate(UserBase):
    password: Optional[str] = Field(None, min_length=6) # 更新时可选密码

# 用于从数据库读取用户数据的基础模式
class UserInDBBase(UserBase):
    id: int
    # Pydantic V2 使用 from_attributes 替代 orm_mode
    class Config:
        from_attributes = True

# API 返回给客户端的用户信息模式 (不包含密码)
class User(UserInDBBase):
    pass

# 内部使用的用户数据模式 (包含哈希密码)
class UserInDB(UserInDBBase):
    hashed_password: str
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\schemas\__init__.py
# app/schemas/__init__.py

# 导入用户相关的 Schemas
from .user import User, UserCreate, UserUpdate, UserInDB, UserBase

# 导入认证相关的 Schemas
from .token import Token, TokenData

# 导入量表相关的 Schemas
from .scale import ScaleOption, ScaleQuestion, ScaleInfo, ScaleQuestionsResponse, AvailableScalesResponse

# 导入评估提交相关的 Schemas
from .assessment import BasicInfoSubmit, AssessmentSubmitResponse

# 导入报告相关的 Schemas
from .report import ReportData, ReportResponse

# 导入百科相关的 Schemas
from .encyclopedia import EncyclopediaEntry, CategoriesResponse, EntriesResponse 

# (可选) __all__ 列表
__all__ = [
    "User", "UserCreate", "UserUpdate", "UserInDB", "UserBase",
    "Token", "TokenData", # <--- 添加 Token 相关
    "ScaleOption", "ScaleQuestion", "ScaleInfo", "ScaleQuestionsResponse", "AvailableScalesResponse",
    "BasicInfoSubmit", "AssessmentSubmitResponse",
    "ReportData", "ReportResponse",
    "EncyclopediaEntry", "CategoriesResponse", "EntriesResponse",
]
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\tasks\analysis.py
# app/tasks/analysis.py
import logging
import os
import sys
import asyncio

# --- 路径设置 (保持不变) ---
TASK_DIR = os.path.dirname(os.path.abspath(__file__))
APP_ROOT_FROM_TASK = os.path.dirname(TASK_DIR) # app/
PROJECT_ROOT_FROM_TASK = os.path.dirname(APP_ROOT_FROM_TASK) # PsychologyAnalysis/
if PROJECT_ROOT_FROM_TASK not in sys.path:
    sys.path.insert(0, PROJECT_ROOT_FROM_TASK)
    print(f"[Celery Task Init] 已添加 {PROJECT_ROOT_FROM_TASK} 到 sys.path 以供 worker 使用")

# --- 核心导入 ---
from app.core.celery_app import celery_app
from app.core.config import settings
# --- 导入异步 DB 会话和 CRUD ---
from app.db.session import AsyncSessionLocal
# *** 直接导入 assessment 的 CRUD 模块 ***
from app.crud import assessment as crud_assessment

# --- 导入处理逻辑 (现在是 generate_report_content) ---
try:
    # *** 导入修改后的核心处理函数 ***
    from src.ai_utils import generate_report_content
    # ... (日志设置逻辑保持不变) ...
    try:
        from src.utils import setup_logging
        WORKER_LOGGER_NAME = f"{settings.APP_NAME}_Worker"
        # 重新配置日志，以防 worker 启动时未完全初始化
        setup_logging(log_level_str=settings.LOG_LEVEL,
                      log_dir_name=os.path.basename(settings.LOGS_DIR),
                      logger_name=WORKER_LOGGER_NAME)
        logger = logging.getLogger(WORKER_LOGGER_NAME)
        print(f"[Celery Task Init] Logger '{WORKER_LOGGER_NAME}' 配置完成。")
    except Exception as log_setup_err:
        print(f"[Celery Task Init] 日志设置错误: {log_setup_err}. 使用基础日志记录器。")
        logger = logging.getLogger(__name__) # 使用默认 logger
        if not logger.hasHandlers(): # 确保至少有基本配置
            logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

except ImportError as e:
     print(f"[Celery Task Init] CRITICAL: 无法导入 src 模块: {e}", exc_info=True)
     # 定义虚拟函数以便任务优雅失败
     generate_report_content = None
     logger = logging.getLogger(__name__) # 使用基础 logger
     if not logger.hasHandlers():
         logging.basicConfig(level=logging.INFO)
     logger.critical(f"CRITICAL: 核心处理函数 (generate_report_content) 导入失败: {e}")


@celery_app.task(bind=True, name='tasks.run_ai_analysis')
def run_ai_analysis(self, assessment_id: int):
    """
    Celery 任务：异步运行 AI 分析并更新报告。
    1. 异步从数据库加载评估数据。
    2. 调用核心逻辑生成报告文本。
    3. 异步更新数据库中的报告文本。
    """
    # ---- 函数体开始 ----
    global logger
    # 再次确保 logger 可用
    if logger is None:
        print("CRITICAL ERROR: Logger 在 run_ai_analysis 任务中不可用!")
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__) # 最后手段

    task_id_str = f"[Celery Task {self.request.id}]" # 用于日志追踪
    logger.info(f"{task_id_str} 收到任务，评估 ID: {assessment_id}")

    # 检查核心函数是否已加载
    if generate_report_content is None:
         logger.error(f"{task_id_str} 核心处理函数 (generate_report_content) 未加载，中止任务 ID {assessment_id}。")
         # 返回失败状态给 Celery，让调用者知道任务失败
         # 可以通过 task.get() 获取这个结果
         return {"status": "failure", "assessment_id": assessment_id, "error": "核心处理函数导入失败"}

    async def _run_analysis_async():
        """内部异步函数，封装异步操作"""
        nonlocal assessment_id # 确保能访问外部的 assessment_id
        report_text_to_save = "处理失败：未知错误" # 默认错误信息
        final_status = "failure" # 默认状态
        error_detail = None # 存储错误摘要

        # 使用异步会话管理器
        async with AsyncSessionLocal() as session:
            try:
                # 1. 异步加载评估数据
                logger.info(f"{task_id_str} 正在异步加载评估数据 ID: {assessment_id}")
                assessment_record = await crud_assessment.get(db=session, id=assessment_id)

                if not assessment_record:
                    logger.error(f"{task_id_str} 无法找到评估记录 ID: {assessment_id}")
                    error_detail = f"评估记录 ID {assessment_id} 未找到"
                    # 不需要更新数据库，因为记录不存在
                    # 直接返回失败结果
                    return {"status": "failure", "assessment_id": assessment_id, "error": error_detail}

                # 将 SQLAlchemy 模型对象转换为字典，传递给核心逻辑
                # 注意：这是一种简化方式，可能不适用于所有复杂关系。
                # Pydantic 模型验证可能是更健壮的选择。
                submission_data = {}
                for column in assessment_record.__table__.columns:
                    submission_data[column.name] = getattr(assessment_record, column.name)

                logger.debug(f"{task_id_str} 已加载数据，准备调用核心处理函数，ID: {assessment_id}")

                # 2. 调用核心处理逻辑生成报告文本
                generated_text = generate_report_content(
                    submission_data=submission_data,
                    config=settings.model_dump(), # 传递 Pydantic Settings 的字典表示
                    task_logger=logger
                )

                # 检查报告生成结果
                if generated_text is None:
                    logger.error(f"{task_id_str} 核心处理函数 (generate_report_content) 返回 None，ID: {assessment_id}")
                    report_text_to_save = "错误：报告生成意外返回空"
                    error_detail = "报告生成返回空"
                    # final_status 保持 'failure'
                elif "错误" in generated_text or "Error" in generated_text or "失败" in generated_text:
                     logger.error(f"{task_id_str} 核心处理函数返回错误信息，ID {assessment_id}: {generated_text[:200]}...")
                     report_text_to_save = generated_text # 保存具体的错误信息
                     error_detail = f"AI 处理失败: {generated_text[:150]}" # 记录简短错误摘要
                     # final_status 保持 'failure'
                else:
                    # 报告生成成功
                    logger.info(f"{task_id_str} 报告内容生成成功，ID: {assessment_id}")
                    report_text_to_save = generated_text
                    final_status = "success"
                    error_detail = None # 清除错误详情

                # 3. 异步更新数据库 (无论成功失败，都尝试更新 report_text)
                logger.info(f"{task_id_str} 尝试异步更新报告文本 (或错误信息) 到数据库，ID: {assessment_id}")
                # 确保 report_text_to_save 是字符串
                report_to_save_str = str(report_text_to_save)

                updated_record = await crud_assessment.update_report_text(
                    db=session,
                    assessment_id=assessment_id,
                    report_text=report_to_save_str
                )

                if not updated_record:
                     # 这通常意味着 ID 找不到了，但应该在加载步骤就失败了
                     logger.error(f"{task_id_str} 尝试更新数据库时记录 ID {assessment_id} 未找到！")
                     # 如果之前的状态是 success，现在也应该标记为 failure
                     if final_status == "success": final_status = "failure"
                     # 用这个错误覆盖之前的错误，DB 更新失败更关键
                     error_detail = f"数据库更新失败 (更新时 ID: {assessment_id} 未找到)"
                else:
                    logger.info(f"{task_id_str} 数据库报告文本更新成功，ID: {assessment_id}")
                    # session.commit() 和 session.refresh() 应该由 CRUD 函数处理

            except Exception as e:
                # 捕获 _run_analysis_async 内部未处理的异常
                logger.error(f"{task_id_str} 在 _run_analysis_async 中发生意外错误，ID {assessment_id}: {e}", exc_info=True)
                error_message = f"任务执行失败: {type(e).__name__} - {str(e)}"
                report_text_to_save = error_message # 准备将此错误写入 DB
                final_status = "failure"
                error_detail = error_message[:150] # 记录摘要
                # 尝试将这个最终错误写入数据库（仍在 async with session 内）
                try:
                    await crud_assessment.update_report_text(
                         db=session,
                         assessment_id=assessment_id,
                         report_text=report_text_to_save[:2000] # 截断长错误
                     )
                    logger.info(f"{task_id_str} 已将最终错误信息写入数据库，ID {assessment_id}")
                except Exception as db_err_on_fail:
                    logger.error(f"{task_id_str} 在失败处理中写入数据库错误信息也失败了，ID {assessment_id}: {db_err_on_fail}")

        # --- async with session 结束 ---

        # 根据最终状态返回结果字典
        if final_status == "success":
             return {"status": "success", "assessment_id": assessment_id, "report_length": len(report_text_to_save)}
        else:
             # 确保 error_detail 有值
             if not error_detail: error_detail = "处理过程中发生未知错误"
             return {"status": "failure", "assessment_id": assessment_id, "error": error_detail}

    # --- 在同步的 Celery 任务中运行异步代码块 ---
    try:
        # 使用 asyncio.run() 来执行异步函数
        # 这会创建一个新的事件循环来运行 _run_analysis_async
        result = asyncio.run(_run_analysis_async())
        return result
    except RuntimeError as e:
        # 处理可能由 Celery 事件循环引起的嵌套循环错误
        logger.warning(f"{task_id_str} Asyncio 运行时错误 (可能事件循环冲突): {e}。")
        error_msg = f"任务失败: Asyncio 运行时错误 - {e}"
        # 尝试同步方式记录错误（作为最后的手段）
        try:
             # 延迟导入，仅在需要时导入同步 Handler
             from src.data_handler import DataHandler
             sync_db_path = settings.DB_PATH_SQLITE # 从配置获取同步路径
             sync_handler = DataHandler(db_path=sync_db_path)
             sync_handler.update_report_text(assessment_id, error_msg[:2000]) # 截断
             logger.info(f"{task_id_str} 已尝试同步记录 Asyncio 错误到数据库，ID {assessment_id}")
        except Exception as sync_db_err:
             logger.error(f"{task_id_str} 同步记录 Asyncio 错误到数据库失败，ID {assessment_id}: {sync_db_err}")

        return {"status": "failure", "assessment_id": assessment_id, "error": error_msg}
    except Exception as task_exec_err:
        # 捕获 asyncio.run() 本身或其他同步代码可能抛出的错误
        logger.critical(f"{task_id_str} Celery 任务执行期间发生顶层错误，ID {assessment_id}: {task_exec_err}", exc_info=True)
        error_msg = f"任务执行错误: {type(task_exec_err).__name__} - {str(task_exec_err)}"
        # 同样尝试同步记录错误
        try:
             from src.data_handler import DataHandler
             sync_db_path = settings.DB_PATH_SQLITE
             sync_handler = DataHandler(db_path=sync_db_path)
             sync_handler.update_report_text(assessment_id, error_msg[:2000]) # 截断
             logger.info(f"{task_id_str} 已尝试同步记录顶层错误到数据库，ID {assessment_id}")
        except Exception as sync_db_err:
             logger.error(f"{task_id_str} 同步记录顶层错误到数据库失败，ID {assessment_id}: {sync_db_err}")

        return {"status": "failure", "assessment_id": assessment_id, "error": error_msg}

# ---- run_ai_analysis 函数体结束 ----
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\app\tasks\__init__.py

-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\ai_utils.py
# src/ai_utils.py
import os
import json
# 移除了 import sqlite3
from datetime import datetime
import sys
import logging

# --- 路径设置和模块导入 (保持不变) ---
SRC_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT_FROM_SRC = os.path.dirname(SRC_DIR)
if PROJECT_ROOT_FROM_SRC not in sys.path:
    sys.path.insert(0, PROJECT_ROOT_FROM_SRC)

try:
    # 移除了 from .data_handler import DataHandler 的导入，因为此文件不再直接使用它
    from .image_processor import ImageProcessor
    from .report_generator import ReportGenerator
    print("[ai_utils] 成功相对导入 ImageProcessor 和 ReportGenerator。")
except ImportError:
    try:
        # 移除了 from data_handler import DataHandler 的导入
        from image_processor import ImageProcessor
        from report_generator import ReportGenerator
        print("[ai_utils] 成功直接导入 ImageProcessor 和 ReportGenerator (后备)。")
    except ImportError as e:
        print(f"[ai_utils] CRITICAL ERROR: 无法导入必要的同级模块: {e}", file=sys.stderr)
        raise e

# --- calculate_score_and_interpret 函数 (保持不变) ---
def calculate_score_and_interpret(scale_type, scale_answers, task_logger=None):
    """
    根据量表答案计算分数，并根据量表类型提供基本解释。
    使用提供的 logger 实例进行日志记录。

    Args:
        scale_type (str): 表示量表类型的代码 (例如, 'SAS', 'SDS').
        scale_answers (dict): 问题答案字典 { 'q1': 'score_value', ... }.
        task_logger (logging.Logger, optional): 要使用的 Logger 实例. 默认为 None.

    Returns:
        tuple: (calculated_score, interpretation_string)
    """
    current_logger = task_logger or logging.getLogger(__name__)
    if task_logger is None:
        current_logger.warning("未向 calculate_score_and_interpret 提供 task_logger，使用默认日志记录器。")
        # 确保有基本的日志处理器
        if not logging.getLogger().hasHandlers():
            logging.basicConfig(level=logging.INFO)

    if not scale_answers:
        current_logger.info(f"量表类型 '{scale_type}' 的答案为空。")
        return 0, "无量表答案"

    calculated_score = 0
    try:
        # 确保值存在且可以转换为数字
        valid_scores = []
        for key, value in scale_answers.items():
            if value is not None:
                try:
                    # 尝试转换为浮点数，然后转整数（如果可能）
                    score_float = float(value)
                    valid_scores.append(int(score_float) if score_float.is_integer() else score_float)
                except (ValueError, TypeError):
                    current_logger.warning(f"无法将答案 '{key}':'{value}' 转换为数字，已忽略。")

        if not valid_scores:
             current_logger.warning(f"在类型 {scale_type} 的量表答案中未找到有效的数字分数: {scale_answers}")
             return 0, f"量表 '{scale_type}' 无有效得分项"
        calculated_score = sum(valid_scores)
        current_logger.debug(f"计算得到的分数: {calculated_score} (来自: {valid_scores})")
    except Exception as e:
        current_logger.error(f"从答案 {scale_answers} 计算分数时出错: {e}", exc_info=True)
        return "计算错误", f"分数计算出错: {e}"

    interpretation = f"量表 '{scale_type}' 总得分: {calculated_score}."
    current_logger.info(f"开始为量表 '{scale_type}' (得分: {calculated_score}) 生成解释。")

    try:
        # --- 量表解释逻辑 (保持不变) ---
        if scale_type == 'SAS': # Anxiety Self-Rating Scale
            standard_score = int(calculated_score * 1.25)
            interpretation = f"量表 '{scale_type}' 原始得分: {calculated_score}, 标准分: {standard_score}."
            if standard_score >= 70: interpretation += " (重度焦虑水平)"
            elif standard_score >= 60: interpretation += " (中度焦虑水平)"
            elif standard_score >= 50: interpretation += " (轻度焦虑水平)"
            else: interpretation += " (焦虑水平在正常范围)"
        elif scale_type == 'SDS': # Depression Self-Rating Scale
             standard_score = int(calculated_score * 1.25)
             interpretation = f"量表 '{scale_type}' 原始得分: {calculated_score}, 标准分: {standard_score}."
             if standard_score >= 73: interpretation += " (重度抑郁水平)" # 注意：Zung SDS 通常用 70, 60, 50 作为界限，这里按量表文件给出的界限
             elif standard_score >= 63: interpretation += " (中度抑郁水平)"
             elif standard_score >= 53: interpretation += " (轻度抑郁水平)"
             else: interpretation += " (抑郁水平在正常范围)"
        elif scale_type == 'ParentChild':
            if calculated_score >= 80: interpretation += " (亲子关系非常和谐)"
            elif calculated_score >= 60: interpretation += " (亲子关系良好)"
            else: interpretation += " (亲子关系可能存在挑战，建议关注)"
        elif scale_type == 'Personality':
            # 性格测试通常需要更复杂的解释，可能基于得分范围
            if calculated_score >= 101: interpretation += " (倾向：积极热情)"
            elif calculated_score >= 90: interpretation += " (倾向：领导人特质)"
            elif calculated_score >= 79: interpretation += " (倾向：感性)"
            elif calculated_score >= 60: interpretation += " (倾向：理性&淡定)"
            elif calculated_score >= 40: interpretation += " (倾向：双重&孤寂)"
            else: interpretation += " (倾向：现实&自我)"
            interpretation += " (具体解释需参考原始量表得分范围)"
        elif scale_type == 'InterpersonalRelationship':
            if calculated_score <= 8: interpretation += " (人际关系困扰较少)"
            elif calculated_score <= 14: interpretation += " (人际关系存在一定困扰)"
            else: interpretation += " (人际关系困扰较严重)"
        elif scale_type == 'EmotionalStability':
             if calculated_score <= 20: interpretation += " (情绪稳定，自信心强)"
             elif calculated_score <= 40: interpretation += " (情绪基本稳定，但可能较为深沉或消极)"
             else: interpretation += " (情绪不稳定，可能需要关注)"
        elif scale_type == 'HAMD24':
            if calculated_score >= 36: interpretation += " (重度抑郁)"
            elif calculated_score >= 21: interpretation += " (肯定有抑郁)"
            elif calculated_score >= 8: interpretation += " (可能有抑郁)"
            else: interpretation += " (无抑郁症状)"
        # EPQ85 需要分别计算 P, E, N, L 四个维度的得分，这里仅做标记
        elif scale_type == 'EPQ85':
             interpretation = f"艾森克人格问卷 (EPQ-85)，总得分无直接意义，需分析 P, E, N, L 各维度得分。"
             # 实际分析需要在 generate_report_content 中单独处理 EPQ85
        else:
            current_logger.warning(f"未找到量表类型 '{scale_type}' 的特定解释规则。")
            interpretation += " (无特定解释规则)"

    except Exception as e:
        current_logger.error(f"量表 {scale_type} 解释过程中出错: {e}", exc_info=True)
        interpretation += " (解释规则应用出错)"

    current_logger.info(f"量表 '{scale_type}' 解释完成: '{interpretation}'")
    return calculated_score, interpretation


# --- 重命名并重构核心函数 ---
def generate_report_content(submission_data: dict, config: dict, task_logger: logging.Logger) -> str:
    """
    根据传入的评估数据和配置，生成报告文本。不再直接操作数据库。

    Args:
        submission_data (dict): 从数据库异步加载的评估数据字典.
        config (dict): 应用程序配置字典 (来自 settings.model_dump()).
        task_logger (logging.Logger): 用于记录日志的 logger 实例.

    Returns:
        str: 生成的报告文本或错误信息字符串.
    """
    logger = task_logger
    submission_id = submission_data.get("id", "未知ID")
    logger.info(f"开始为评估 ID: {submission_id} 生成报告内容")

    # --- 提取数据 ---
    image_filename = submission_data.get('image_path') # 这是存储在 DB 中的相对路径或文件名
    image_full_path = None
    if image_filename:
        # 从配置中获取上传目录
        uploads_dir = config.get("UPLOADS_DIR")
        if uploads_dir and os.path.isdir(uploads_dir):
            image_full_path = os.path.join(uploads_dir, image_filename)
            logger.info(f"将使用的图片文件路径: {image_full_path}")
        elif not uploads_dir:
            logger.warning(f"配置中未找到 UPLOADS_DIR，无法定位图片文件: {image_filename}")
        else: # uploads_dir 存在但不是目录
             logger.warning(f"配置的 UPLOADS_DIR '{uploads_dir}' 不是有效目录，无法定位图片文件: {image_filename}")

    scale_type = submission_data.get('questionnaire_type')
    scale_answers_json = submission_data.get('questionnaire_data')
    basic_info = {k: submission_data.get(k) for k in [
        "subject_name", "gender", "id_card", "age", "occupation", "case_name",
        "case_type", "identity_type", "person_type", "marital_status",
        "children_info", "criminal_record", "health_status", "phone_number", "domicile"
    ]}
    # 确保 criminal_record 有默认值 0 (无)
    basic_info.setdefault('criminal_record', 0)
    # 确保 'name' 键存在，用于报告模板
    basic_info['name'] = basic_info.get('subject_name', '未知')

    logger.debug(f"用于报告生成的基础信息 (ID {submission_id}): {basic_info}")

    # --- 准备 AI 配置 ---
    ai_config = config.copy()
    if 'api_key' not in ai_config and 'DASHSCOPE_API_KEY' in ai_config:
        logger.info("复制 DASHSCOPE_API_KEY 到 'api_key' 以供 AI 处理器使用。")
        ai_config['api_key'] = ai_config['DASHSCOPE_API_KEY']
    elif 'api_key' not in ai_config:
        logger.error(f"CRITICAL: AI 处理器的 API Key 未在配置中找到! (ID: {submission_id})")
        return "错误：AI 服务配置不完整 (缺少 API Key)"

    # --- 处理图片 ---
    image_description = "未提供图片"
    if image_full_path:
        if os.path.exists(image_full_path):
            logger.info(f"开始处理图片: {image_full_path}")
            try:
                image_processor = ImageProcessor(ai_config)
                image_description = image_processor.process_image(image_full_path)
                logger.info(f"图片描述生成成功 (ID {submission_id})。描述片段: {image_description[:100]}...")
            except FileNotFoundError:
                 logger.error(f"图片文件在处理时未找到: {image_full_path}")
                 image_description = "图片文件未找到"
            except Exception as img_err:
                logger.error(f"图片处理失败 (ID {submission_id}): {img_err}", exc_info=True)
                image_description = f"图片处理错误: {img_err}"
        else:
            logger.warning(f"图片路径存在但文件在处理时未找到: {image_full_path}")
            image_description = "图片文件未找到"
    else:
        logger.info(f"评估 ID {submission_id} 未提供图片路径。")

    # --- 处理量表数据 ---
    scale_answers = None
    calculated_score = 0
    scale_interpretation = "无量表数据"
    if scale_type and scale_answers_json:
        logger.info(f"开始处理量表数据，类型: {scale_type} (ID {submission_id})")
        try:
            scale_answers = json.loads(scale_answers_json) # 期望是字典 {'q1': 'score', ...}
            if isinstance(scale_answers, dict):
                 # 特殊处理 EPQ85，因为它需要计算四个维度
                 if scale_type == 'EPQ85':
                      # TODO: 实现 EPQ85 的计分逻辑
                      # 这需要访问 EPQ85 的 JSON 文件来获取计分规则
                      # 假设有一个辅助函数 `calculate_epq85_scores(scale_answers)`
                      # 返回 {'P': score_p, 'E': score_e, 'N': score_n, 'L': score_l, 'interpretation': '...'}
                      # epq_results = calculate_epq85_scores(scale_answers, logger)
                      # calculated_score = epq_results # 或者只用某个主维度
                      # scale_interpretation = epq_results['interpretation']
                      logger.warning(f"EPQ85 量表计分逻辑尚未在此函数中完全实现 (ID: {submission_id})。")
                      calculated_score = "N/A" # 标记为不适用总分
                      scale_interpretation = "EPQ85 量表结果需单独分析各维度。"
                 else:
                     # 对于其他量表，使用通用计分函数
                     calculated_score, scale_interpretation = calculate_score_and_interpret(
                         scale_type, scale_answers, task_logger=logger
                     )
                 logger.info(f"量表处理完成: Score={calculated_score}, Interpretation='{scale_interpretation}' (ID {submission_id})")
            else:
                 logger.error(f"解析后的量表答案不是字典类型 (ID {submission_id}): {type(scale_answers)}")
                 scale_interpretation = "量表答案格式错误 (非字典)"
                 scale_answers = None # 重置为 None
        except json.JSONDecodeError as json_err:
            logger.error(f"量表答案 JSON 解析失败 (ID {submission_id}): {json_err}. JSON: {scale_answers_json[:200]}...")
            scale_interpretation = "量表答案格式错误 (JSON 解析失败)"
            scale_answers = None
        except Exception as scale_err:
            logger.error(f"量表数据处理失败 (ID {submission_id}): {scale_err}", exc_info=True)
            scale_interpretation = f"量表处理错误: {scale_err}"
            scale_answers = None
    elif scale_type:
         logger.warning(f"提供了量表类型 '{scale_type}' 但无答案数据 (ID {submission_id}).")
         scale_interpretation = f"量表 '{scale_type}' 未提供答案"
    else:
         logger.info(f"评估 ID {submission_id} 未提供量表类型.")

    # --- 调用 LLM 生成报告 ---
    logger.info(f"开始调用 LLM 生成报告 (ID {submission_id})")
    final_report_text = None
    try:
        report_generator = ReportGenerator(ai_config)
        final_report_text = report_generator.generate_report(
             description=image_description,
             questionnaire=scale_answers, # 传递解析后的字典或 None
             subject_info=basic_info,
             questionnaire_type=scale_type,
             score=calculated_score, # 可能是数字，也可能是 "N/A" (如 EPQ)
             scale_interpretation=scale_interpretation
         )
        if final_report_text is None:
             # ReportGenerator 应该返回字符串，即使是错误信息
             raise ValueError("报告生成器意外返回了 None")
        logger.info(f"LLM 报告生成成功 (ID {submission_id}, 长度: {len(final_report_text)})")

    except Exception as report_err:
        logger.error(f"LLM 报告生成失败 (ID {submission_id}): {report_err}", exc_info=True)
        # 返回具体的错误信息，而不是仅仅标记失败
        final_report_text = f"报告生成错误: {type(report_err).__name__} - {str(report_err)}"

    # --- 返回最终文本 ---
    # 注意：此函数不再负责数据库更新
    return final_report_text

# 移除旧的 process_data_and_generate_report_sync 函数定义（如果它还存在）
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\api_tester.py
# 文件路径: src/api_tester.py
import base64
import os
import sqlite3
import json
from openai import OpenAI
from src.utils import setup_logging
from src.image_processor import ImageProcessor
from src.report_generator import ReportGenerator

class APITester:
    def __init__(self, config):
        """初始化 API 测试模块"""
        self.logger = setup_logging()
        self.config = config
        
        # 统一客户端配置
        self.api_key = config["api_key"]
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
        )
        
        # 模型名称
        self.vision_model = "qwen-vl-max-latest"
        self.text_model = config["text_model"]  # 从配置中读取，例如 "qwen-plus"
        
        # 初始化数据库路径
        self.db_path = "psychology_analysis.db"
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        # 初始化测试用模块
        self.image_processor = ImageProcessor(config)
        self.report_generator = ReportGenerator(config)

    def setup_test_data(self):
        """为测试准备数据，插入测试用图片路径和量表数据"""
        self.logger.info("准备测试数据...")
        
        # 连接数据库
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # 清空现有数据（仅用于测试）
            cursor.execute("DELETE FROM analysis_data")
            
            # 插入测试数据
            test_image_path = os.path.join(self.project_root, "input", "images", "TestPic.jpg")
            subject_info = {"name": "测试用户", "age": 20, "gender": "男"}
            questionnaire_data = {"q1": "yes", "q2": "no", "q3": "sometimes"}
            
            cursor.execute('''
                INSERT INTO analysis_data (image_path, subject_name, age, gender, questionnaire_data)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                test_image_path,
                subject_info["name"],
                subject_info["age"],
                subject_info["gender"],
                json.dumps(questionnaire_data)
            ))
            
            # 插入只有量表数据的记录（无对应图片）
            cursor.execute('''
                INSERT INTO analysis_data (image_path, subject_name, age, gender, questionnaire_data)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                "no_image.jpg",  # 虚拟图片路径
                "无图片用户",
                25,
                "女",
                json.dumps({"q1": "no", "q2": "yes", "q3": "often"})
            ))
            
            conn.commit()
        self.logger.info("测试数据准备完成")

    def cleanup_test_data(self):
        """清理测试数据"""
        self.logger.info("清理测试数据...")
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("DELETE FROM analysis_data")
            conn.commit()
        self.logger.info("测试数据清理完成")

    def test_vision_api(self):
        """测试图像处理模型的 API 连通性"""
        self.logger.info("开始测试图像处理模型 API...")
        
        # 使用 input/images/TestPic.jpg 进行测试
        test_image_path = os.path.join(self.project_root, "input", "images", "TestPic.jpg")
        
        if not os.path.exists(test_image_path):
            self.logger.error(f"测试图片 {test_image_path} 不存在，请确保文件已放置在正确位置")
            return False
        
        # 将图片转为 base64 编码
        with open(test_image_path, "rb") as image_file:
            image_base64 = base64.b64encode(image_file.read()).decode("utf-8")

        # 构造消息内容
        messages = [
            {"role": "system", "content": [{"type": "text", "text": "You are a helpful assistant."}]},
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}},
                    {"type": "text", "text": "请简要描述这张图片的内容，用于API测试。"}
                ]
            }
        ]

        try:
            completion = self.client.chat.completions.create(
                model=self.vision_model,
                messages=messages,
            )
            description = completion.choices[0].message.content
            self.logger.info("图像模型 API 测试成功！")
            self.logger.info(f"测试图片描述: {description}")
            return True
        except Exception as e:
            self.logger.error(f"图像模型 API 测试失败: {str(e)}")
            return False

    def test_text_api(self):
        """测试文本生成模型的 API 连通性"""
        self.logger.info("开始测试文本生成模型 API...")
        
        # 构造测试消息
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "这是一个API连通性测试，请回复‘测试成功’。"}
        ]

        try:
            completion = self.client.chat.completions.create(
                model=self.text_model,
                messages=messages,
            )
            text_output = completion.choices[0].message.content
            self.logger.info("文本生成模型 API 测试成功！")
            self.logger.info(f"测试输出: {text_output}")
            return True
        except Exception as e:
            self.logger.error(f"文本生成模型 API 测试失败: {str(e)}")
            return False

    def test_report_with_questionnaire_only(self):
        """测试只有量表数据时是否能生成报告"""
        self.logger.info("开始测试只有量表数据时的报告生成...")
        
        # 从数据库加载只有量表数据的记录
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT subject_name, age, gender, questionnaire_data FROM analysis_data WHERE image_path = ?", ("no_image.jpg",))
            result = cursor.fetchone()
            if not result:
                self.logger.error("未找到只有量表数据的测试记录")
                return False
            
            subject_info = {"name": result[0], "age": result[1], "gender": result[2]}
            questionnaire_data = json.loads(result[3]) if result[3] else None
            
            if not questionnaire_data:
                self.logger.error("量表数据为空，无法生成报告")
                return False

        # 使用空描述生成报告
        description = "无图片输入，仅基于量表数据生成报告"
        try:
            report = self.report_generator.generate_report(description, questionnaire_data, subject_info)
            self.logger.info("仅使用量表数据生成报告成功！")
            self.logger.info(f"生成的报告片段: {report[:100]}...")  # 只打印前100个字符
            return True
        except Exception as e:
            self.logger.error(f"仅使用量表数据生成报告失败: {str(e)}")
            return False

    def test_full_pipeline(self):
        """测试完整流程：图像 + 量表数据"""
        self.logger.info("开始测试完整流程（图像 + 量表数据）...")
        
        # 使用 TestPic.jpg 进行测试
        test_image_path = os.path.join(self.project_root, "input", "images", "TestPic.jpg")
        
        if not os.path.exists(test_image_path):
            self.logger.error(f"测试图片 {test_image_path} 不存在，请确保文件已放置在正确位置")
            return False
        
        # 步骤1: 图像识别
        try:
            description = self.image_processor.process_image(test_image_path)
            self.logger.info("图像描述生成成功")
            self.logger.info(f"描述片段: {description[:100]}...")  # 只打印前100个字符
        except Exception as e:
            self.logger.error(f"图像描述生成失败: {str(e)}")
            return False
        
        # 步骤2: 从数据库加载量表数据
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT subject_name, age, gender, questionnaire_data FROM analysis_data WHERE image_path = ?", (test_image_path,))
            result = cursor.fetchone()
            if not result:
                self.logger.error(f"图片 {test_image_path} 无对应量表数据")
                return False
            
            subject_info = {"name": result[0], "age": result[1], "gender": result[2]}
            questionnaire_data = json.loads(result[3]) if result[3] else None
            
            if not questionnaire_data:
                self.logger.error("量表数据为空，无法生成报告")
                return False

        # 步骤3: 生成报告
        try:
            report = self.report_generator.generate_report(description, questionnaire_data, subject_info)
            self.logger.info("完整流程生成报告成功！")
            self.logger.info(f"生成的报告片段: {report[:100]}...")  # 只打印前100个字符
            return True
        except Exception as e:
            self.logger.error(f"完整流程生成报告失败: {str(e)}")
            return False

    def run_all_tests(self):
        """运行所有 API 测试"""
        self.logger.info("开始运行所有 API 测试...")
        
        # 准备测试数据
        self.setup_test_data()
        
        # 执行所有测试
        vision_result = self.test_vision_api()
        text_result = self.test_text_api()
        questionnaire_only_result = self.test_report_with_questionnaire_only()
        full_pipeline_result = self.test_full_pipeline()
        
        # 清理测试数据
        self.cleanup_test_data()
        
        # 汇总测试结果
        if all([vision_result, text_result, questionnaire_only_result, full_pipeline_result]):
            self.logger.info("所有 API 测试通过！")
            return True
        else:
            self.logger.error("部分或全部 API 测试未通过，请检查日志")
            self.logger.error(f"测试结果 - 图像API: {vision_result}, 文本API: {text_result}, 仅量表: {questionnaire_only_result}, 完整流程: {full_pipeline_result}")
            return False

if __name__ == "__main__":
    import yaml
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    config_path = os.path.join(project_root, "config", "config.yaml")
    with open(config_path, "r", encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    tester = APITester(config)
    tester.run_all_tests()
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\data_entry.py
# 文件路径: src/data_entry.py
import os
import sys

# 获取项目根目录（PsychologyAnalysis/）
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
# 将项目根目录添加到模块搜索路径
sys.path.append(project_root)

from src.data_handler import DataHandler

def main():
    # 初始化 DataHandler
    handler = DataHandler(db_path=os.path.join(project_root, "psychology_analysis.db"))

    # 定义测试数据
    data_entries = [
        {
            "image_path": os.path.join(project_root, "input\images\image1.jpg"),
            "subject_info": {"name": "张三", "age": 13, "gender": "男"},
            "questionnaire_data": {"q1": "yes", "q2": "no", "q3": "sometimes"}
        },
        {
            "image_path": os.path.join(project_root, "input\images\TestPic.jpg"),
            "subject_info": {"name": "李四", "age": 15, "gender": "女"},
            "questionnaire_data": {"q1": "no", "q2": "yes", "q3": "often"}
        },
        {
            "image_path": "no_image.jpg",  # 没有对应图片，仅量表数据
            "subject_info": {"name": "王五", "age": 20, "gender": "男"},
            "questionnaire_data": {"q1": "yes", "q2": "yes", "q3": "rarely"}
        }
    ]

    # 录入数据
    for entry in data_entries:
        image_path = entry["image_path"]
        subject_info = entry["subject_info"]
        questionnaire_data = entry["questionnaire_data"]

        # 检查图片文件是否存在（如果 image_path 不是虚拟路径）
        if image_path != "no_image.jpg" and not os.path.exists(image_path):
            print(f"警告: 图片文件 {image_path} 不存在，跳过录入")
            continue

        try:
            handler.save_data(image_path, subject_info, questionnaire_data)
            print(f"成功录入数据: 图片路径 {image_path}, 被测者 {subject_info['name']}")
        except Exception as e:
            print(f"录入数据失败: 图片路径 {image_path}, 错误: {str(e)}")

if __name__ == "__main__":
    main()
    from src.data_handler import check_db_content
    check_db_content(os.path.join(project_root, "psychology_analysis.db"))
    
    
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\data_handler.py
# 文件路径: src/data_handler.py
import sqlite3
import json
import os
from datetime import datetime

class DataHandler:
    def __init__(self, db_path="psychology_analysis.db"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """Initializes the database schema more robustly."""
        print(f"Initializing database schema at: {self.db_path}")
        required_columns_analysis = {
            # 列名: 列类型 (不包含约束和复杂默认值，这些在CREATE TABLE中处理)
            "image_path": "TEXT",
            "subject_name": "TEXT",
            "age": "INTEGER",
            "gender": "TEXT",
            "questionnaire_type": "TEXT",
            "questionnaire_data": "TEXT", # JSON
            "report_text": "TEXT",
            "updated_at": "TIMESTAMP", # 类型即可，默认值由触发器处理
            "id_card": "TEXT", # 类型即可，UNIQUE在CREATE TABLE中处理
            "occupation": "TEXT",
            "case_name": "TEXT",
            "case_type": "TEXT",
            "identity_type": "TEXT",
            "person_type": "TEXT",
            "marital_status": "TEXT",
            "children_info": "TEXT",
            "criminal_record": "INTEGER",
            "health_status": "TEXT",
            "phone_number": "TEXT",
            "domicile": "TEXT"
        }
        # questionnaire_questions 列
        required_columns_questions = {
            "scale_name": "TEXT"
        }


        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # --- Handle analysis_data Table ---
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='analysis_data';")
            table_exists = cursor.fetchone()

            if not table_exists:
                print("Table 'analysis_data' does not exist. Creating new table with full schema...")
                # Create table with all columns and constraints if it doesn't exist
                create_table_sql = """
                CREATE TABLE analysis_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    image_path TEXT,
                    subject_name TEXT,
                    age INTEGER,
                    gender TEXT,
                    questionnaire_type TEXT,
                    questionnaire_data TEXT,
                    report_text TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, -- Initial value
                    id_card TEXT , 
                    occupation TEXT,
                    case_name TEXT,
                    case_type TEXT,
                    identity_type TEXT,
                    person_type TEXT,
                    marital_status TEXT,
                    children_info TEXT,
                    criminal_record INTEGER DEFAULT 0,
                    health_status TEXT,
                    phone_number TEXT,
                    domicile TEXT
                );
                """
                cursor.execute(create_table_sql)
                print("Table 'analysis_data' created successfully.")
            else:
                print("Table 'analysis_data' exists. Checking for missing columns...")
                # If table exists, check and add missing columns without problematic constraints/defaults
                cursor.execute("PRAGMA table_info(analysis_data)")
                existing_columns = {info[1] for info in cursor.fetchall()}

                for col_name, col_type in required_columns_analysis.items():
                    if col_name not in existing_columns:
                        try:
                            # Add column with only the type, no complex defaults or UNIQUE constraints here
                            cursor.execute(f"ALTER TABLE analysis_data ADD COLUMN {col_name} {col_type}")
                            print(f"Added column '{col_name}' to analysis_data table.")
                        except sqlite3.OperationalError as e:
                            print(f"Warning: Could not add column '{col_name}': {e}")

            # --- Handle questionnaire_questions Table ---
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='questionnaire_questions';")
            q_table_exists = cursor.fetchone()
            if not q_table_exists:
                 print("Table 'questionnaire_questions' does not exist. Creating new table...")
                 cursor.execute('''CREATE TABLE questionnaire_questions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    questionnaire_type TEXT NOT NULL,
                    question_number INTEGER NOT NULL,
                    question_text TEXT NOT NULL,
                    options TEXT NOT NULL, -- JSON string
                    scale_name TEXT, -- User-friendly name
                    UNIQUE(questionnaire_type, question_number)
                )''')
                 print("Table 'questionnaire_questions' created successfully.")
            else:
                 print("Table 'questionnaire_questions' exists. Checking for missing columns...")
                 cursor.execute("PRAGMA table_info(questionnaire_questions)")
                 existing_q_columns = {info[1] for info in cursor.fetchall()}
                 for col_name, col_type in required_columns_questions.items():
                      if col_name not in existing_q_columns:
                           try:
                                cursor.execute(f"ALTER TABLE questionnaire_questions ADD COLUMN {col_name} {col_type}")
                                print(f"Added column '{col_name}' to questionnaire_questions table.")
                           except sqlite3.OperationalError as e:
                                print(f"Warning: Could not add column '{col_name}' to questionnaire_questions: {e}")


            # --- Ensure Triggers Exist ---
            # Trigger to update 'updated_at' timestamp (safe to run even if exists)
            try:
                 cursor.execute('''
                    CREATE TRIGGER IF NOT EXISTS update_analysis_data_updated_at
                    AFTER UPDATE ON analysis_data
                    FOR EACH ROW
                    WHEN OLD.updated_at = NEW.updated_at OR OLD.updated_at IS NULL -- Avoid infinite loops if trigger itself updates
                    BEGIN
                        UPDATE analysis_data SET updated_at = CURRENT_TIMESTAMP WHERE id = OLD.id;
                    END;
                ''')
                 print("Ensured 'updated_at' trigger exists.")
            except sqlite3.OperationalError as e:
                 print(f"Warning: Could not create/verify 'updated_at' trigger: {e}")


            conn.commit()
        print("Database schema initialization process completed.")
    def normalize_path(self, path):
        """Normalizes file path for consistency, returns None if path is None."""
        if path is None:
            return None
        return os.path.normpath(path).replace('\\', '/')

    def save_data(self, image_path, basic_info, scale_type, scale_answers_json):
        """Saves comprehensive data to the analysis_data table."""
        normalized_image_path = self.normalize_path(image_path) # Can be None

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            # Prepare column names and placeholders dynamically based on basic_info keys
            # Always include mandatory fields
            columns = ['image_path', 'questionnaire_type', 'questionnaire_data']
            placeholders = ['?', '?', '?']
            values = [normalized_image_path, scale_type, scale_answers_json]

            # Add fields from basic_info
            for key, value in basic_info.items():
                # Map form names to DB column names if necessary
                db_key = key # Assume direct mapping for now
                # Handle specific fields like subject_name, age, gender if they are part of basic_info
                if db_key == "name": db_key = "subject_name"

                # Ensure the key is a valid column name (check against required_columns keys)
                # This is a basic check; more robust validation might be needed
                valid_columns = { "subject_name", "age", "gender", "id_card", "occupation", "case_name", "case_type", "identity_type", "person_type", "marital_status", "children_info", "criminal_record", "health_status", "phone_number", "domicile"}
                if db_key in valid_columns:
                    columns.append(db_key)
                    placeholders.append('?')
                    # Special handling for criminal_record (assuming 1 for Yes, 0 for No from form)
                    if db_key == 'criminal_record':
                        values.append(1 if str(value).lower() in ['1', 'yes', 'true'] else 0)
                    else:
                        values.append(value)

            columns_str = ", ".join(columns)
            placeholders_str = ", ".join(placeholders)

            sql = f'''INSERT INTO analysis_data ({columns_str})
                      VALUES ({placeholders_str})'''

            try:
                cursor.execute(sql, values)
                submission_id = cursor.lastrowid
                conn.commit()
                print(f"Data saved successfully. Submission ID: {submission_id}")
                return submission_id
            except sqlite3.IntegrityError as e:
                 print(f"Error saving data: {e}. Possible duplicate entry (e.g., ID card)?")
                 # Depending on requirements, you might want to update instead of insert
                 # Or simply report the error
                 raise e # Re-raise the exception
            except Exception as e:
                 print(f"An unexpected error occurred during save: {e}")
                 raise e


    def load_data_by_id(self, submission_id):
        """Loads a submission's data by its ID, returning a dictionary."""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row # Return rows as dictionary-like objects
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM analysis_data WHERE id = ?", (submission_id,))
            result = cursor.fetchone()
            if result:
                return dict(result) # Convert Row object to a standard dictionary
            return None

    def update_report_text(self, submission_id, report_text):
        """Updates the report_text for a given submission ID."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("UPDATE analysis_data SET report_text = ? WHERE id = ?",
                           (report_text, submission_id))
            conn.commit()
            print(f"Report text updated for submission ID: {submission_id}")

    def load_questions_by_type(self, questionnaire_type_code):
        """Loads questions based on the questionnaire type code (e.g., 'SAS')."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''SELECT question_number, question_text, options
                            FROM questionnaire_questions
                            WHERE questionnaire_type = ?
                            ORDER BY question_number''', (questionnaire_type_code,))
            results = cursor.fetchall()
            questions = []
            if not results:
                 print(f"Warning: No questions found for type code '{questionnaire_type_code}'")
                 return None # Return None or empty list based on how you want to handle this
            for row in results:
                try:
                    options_list = json.loads(row[2])
                    # Ensure options have 'text' (or 'name') and 'score' keys
                    formatted_options = [
                        {"text": opt.get("text", opt.get("name", "N/A")), "score": opt.get("score", 0)}
                        for opt in options_list
                    ]
                    question = {
                        "number": row[0],
                        "text": row[1],
                        "options": formatted_options
                    }
                    questions.append(question)
                except json.JSONDecodeError:
                    print(f"Warning: Could not decode options for question {row[0]} of type {questionnaire_type_code}")
                except Exception as e:
                     print(f"Error processing question {row[0]} options: {e}")
            return questions


    def insert_question(self, questionnaire_type, question_number, question_text, options_json_str, scale_name=None):
        """Inserts a single question into the database."""
        # The options should already be a JSON string here if coming from import_questions
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            try:
                cursor.execute('''INSERT OR REPLACE INTO questionnaire_questions
                                (questionnaire_type, question_number, question_text, options, scale_name)
                                VALUES (?, ?, ?, ?, ?)''', (
                    questionnaire_type,
                    question_number,
                    question_text,
                    options_json_str, # Store as JSON string
                    scale_name if scale_name else questionnaire_type # Default scale_name to type if not provided
                ))
                conn.commit()
                # print(f"Inserted/Replaced question: {questionnaire_type} - Q{question_number}")
            except Exception as e:
                 print(f"Error inserting question {questionnaire_type}-Q{question_number}: {e}")

    def get_all_scale_types(self):
        """Retrieves distinct scale types (code and name) from the database."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            # Use COALESCE to provide the type code if name is NULL
            cursor.execute('''SELECT DISTINCT questionnaire_type, COALESCE(scale_name, questionnaire_type) as display_name
                            FROM questionnaire_questions
                            ORDER BY questionnaire_type''')
            results = cursor.fetchall()
            # Return as a list of dictionaries
            return [{"code": row[0], "name": row[1]} for row in results]


def check_db_content(db_path="psychology_analysis.db"):
    """Utility function to print the content of the analysis_data table."""
    print(f"\n--- Checking content of {db_path} ---")
    if not os.path.exists(db_path):
        print("Database file does not exist.")
        return

    try:
        with sqlite3.connect(db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            print("\n[analysis_data Table Content]")
            try:
                cursor.execute("SELECT * FROM analysis_data LIMIT 10") # Limit output for brevity
                rows = cursor.fetchall()
                if not rows:
                    print("Table is empty.")
                else:
                    # Print header
                    print(" | ".join(rows[0].keys()))
                    print("-" * (len(" | ".join(rows[0].keys())) + 10))
                    # Print rows
                    for row in rows:
                        print(" | ".join(map(str, row)))
            except sqlite3.OperationalError as e:
                print(f"Error querying analysis_data: {e}")


            print("\n[questionnaire_questions Table Content]")
            try:
                cursor.execute("SELECT DISTINCT questionnaire_type, scale_name FROM questionnaire_questions")
                scales = cursor.fetchall()
                if not scales:
                    print("Table is empty or contains no distinct scales.")
                else:
                    print("Available Scales (Type | Name):")
                    for scale in scales:
                        print(f"{scale['questionnaire_type']} | {scale['scale_name']}")

                # Optionally print a few questions per scale
                if scales:
                     print("\nSample Questions:")
                     for scale in scales[:2]: # Limit to first 2 scales for brevity
                         print(f"--- Scale: {scale['questionnaire_type']} ---")
                         cursor.execute("SELECT question_number, question_text FROM questionnaire_questions WHERE questionnaire_type = ? ORDER BY question_number LIMIT 3", (scale['questionnaire_type'],))
                         questions = cursor.fetchall()
                         for q in questions:
                             print(f"  Q{q['question_number']}: {q['question_text'][:50]}...") # Truncate long text

            except sqlite3.OperationalError as e:
                print(f"Error querying questionnaire_questions: {e}")

    except sqlite3.Error as e:
        print(f"An error occurred connecting to or reading the database: {e}")
    print("--- End of content check ---\n")


# Example usage (can be run directly to check DB)
if __name__ == "__main__":
     # Assume db is in the project root relative to this file's location (src/)
     project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
     db_file_path = os.path.join(project_root, "psychology_analysis.db")
     # Initialize schema first (important if DB or tables don't exist)
     print("Initializing DataHandler to ensure schema exists...")
     try:
         handler = DataHandler(db_path=db_file_path)
         print("DataHandler initialized.")
         # Now check content
         check_db_content(db_file_path)
         print("\nAvailable scale types:")
         print(handler.get_all_scale_types())
     except Exception as e:
          print(f"Error during DataHandler initialization or check: {e}")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\image_processor.py
# src/image_processor.py
import os
import base64
from openai import OpenAI
import logging # Use logging

# Get the logger instance setup in app.py or utils.py
logger = logging.getLogger("PsychologyAnalysis")

class ImageProcessor:
    def __init__(self, config):
        """Initializes ImageProcessor with configuration and explicit safe headers."""
        self.api_key = config.get("api_key")
        if not self.api_key:
             logger.warning("API Key not found in config for ImageProcessor.")
             # Attempt to get from environment as a fallback
             self.api_key = os.environ.get('DASHSCOPE_API_KEY')
             if not self.api_key:
                  raise ValueError("API Key missing for ImageProcessor.")

        self.model = config.get("vision_model", "qwen-vl-plus") # Use config
        self.base_url = config.get("base_url", "https://dashscope.aliyuncs.com/compatible-mode/v1")

        # --- Explicitly set safe default headers ---
        safe_headers = {
            "User-Agent": "MyPsychologyApp-ImageProcessor/1.0", # Example ASCII User-Agent
            "Accept": "application/json",
            # Add other standard headers if needed, but keep values ASCII safe
            # Avoid adding headers derived from potentially non-ASCII user input here
        }
        # --- End of safe headers ---

        try:
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                default_headers=safe_headers # <--- Add explicit safe headers
            )
            logger.info(f"ImageProcessor OpenAI client initialized. Base URL: {self.base_url}. Using explicit safe headers.")
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client in ImageProcessor: {e}", exc_info=True)
            raise

    def process_image(self, image_path):
        """Processes an image using the configured vision model."""
        logger.info(f"Processing image: {image_path}")
        try:
            # 将图片转为 base64 编码
            with open(image_path, "rb") as image_file:
                image_base64 = base64.b64encode(image_file.read()).decode("utf-8")
        except FileNotFoundError:
             logger.error(f"Image file not found: {image_path}")
             raise FileNotFoundError(f"图片文件未找到: {image_path}")
        except Exception as e:
             logger.error(f"Error reading or encoding image {image_path}: {e}", exc_info=True)
             raise Exception(f"读取或编码图片时出错: {e}") from e

        # 构造消息内容
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": "You are a helpful assistant focused on image description."}] # System prompt
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"} # Assume JPEG, adjust if needed
                    },
                    # Keep the prompt focused on description
                    {"type": "text", "text": "请详细描述这张图片的内容，包括物体、人物（如有）、场景氛围、颜色和构图等。"}
                ]
            }
        ]

        try:
            # 调用 API
            logger.debug(f"Calling vision model '{self.model}' for image {os.path.basename(image_path)}")
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
            )
            description = completion.choices[0].message.content
            logger.info(f"Image description received successfully for {os.path.basename(image_path)}.")
            return description
        except Exception as e:
            # Log the specific error during the API call
            logger.error(f"Error calling vision API for {os.path.basename(image_path)}: {type(e).__name__} - {e}", exc_info=True)
            # Re-raise a user-friendly exception (the original traceback still points here)
            raise Exception(f"图像识别失败: {str(e)}") from e
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\import_questions.py
# 文件路径: src/import_questions.py
import os
import json
import sqlite3
# Correct import assuming data_handler.py is in the same directory (src)
try:
    from data_handler import DataHandler
except ImportError:
    print("Error: data_handler.py not found in the same directory.")
    # Fallback for running directly from PsychologyAnalysis root
    try:
        from data_handler import DataHandler
    except ImportError:
         print("Error: Could not import DataHandler. Make sure you run this script correctly.")
         exit(1)


def import_questions_from_json():
    """Loads scale questions from JSON files in input/questionnaires/ into the SQLite DB."""
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    db_path = os.path.join(project_root, "psychology_analysis.db")
    handler = DataHandler(db_path=db_path)
    questionnaire_dir = os.path.join(project_root, "input", "questionnaires")

    print(f"Scanning for JSON questionnaires in: {questionnaire_dir}")

    json_files = [f for f in os.listdir(questionnaire_dir) if f.endswith('.json')]
    if not json_files:
        print(f"No .json files found in {questionnaire_dir}")
        return

    imported_count = 0
    error_count = 0

    # Define mapping from filename (or title) to scale type code and name
    # Prioritize title if available, otherwise use filename mapping
    scale_mapping = {
        "1测你性格最真实的一面.json": {"code": "Personality", "name": "测你性格最真实的一面"},
        "2亲子关系问卷量表.json": {"code": "ParentChild", "name": "亲子关系问卷量表"},
        "3焦虑症自评量表 (SAS).json": {"code": "SAS", "name": "焦虑症自评量表 (SAS)"},
        "4标准量表：抑郁症自测量表 (SDS).json": {"code": "SDS", "name": "抑郁症自测量表 (SDS)"},
        "5人际关系综合诊断量表.json": {"code": "InterpersonalRelationship", "name": "人际关系综合诊断量表"},
        "6情绪稳定性测验量表.json": {"code": "EmotionalStability", "name": "情绪稳定性测验量表"},
        "7汉密尔顿抑郁量表HAMD24.json": {"code": "HAMD24", "name": "汉密尔顿抑郁量表 (HAMD-24)"},
        "8艾森克人格问卷EPQ85成人版.json": {"code": "EPQ85", "name": "艾森克人格问卷 (EPQ-85成人版)"}
        # Add more mappings as needed
    }
    title_mapping = {
        "测你性格最真实的一面": {"code": "Personality", "name": "测你性格最真实的一面"},
        "亲子关系问卷量表": {"code": "ParentChild", "name": "亲子关系问卷量表"},
        "焦虑症自评量表 (SAS)": {"code": "SAS", "name": "焦虑症自评量表 (SAS)"},
        "标准量表：抑郁症自测量表 (SDS)": {"code": "SDS", "name": "抑郁症自测量表 (SDS)"},
        "人际关系综合诊断量表.json": {"code": "InterpersonalRelationship", "name": "人际关系综合诊断量表"},
        "情绪稳定性测验量表.json": {"code": "EmotionalStability", "name": "情绪稳定性测验量表"},
        "汉密尔顿抑郁量表HAMD24.json": {"code": "HAMD24", "name": "汉密尔顿抑郁量表 (HAMD-24)"},
        "艾森克人格问卷EPQ85成人版.json": {"code": "EPQ85", "name": "艾森克人格问卷 (EPQ-85成人版)"}
    }


    for json_file in json_files:
        file_path = os.path.join(questionnaire_dir, json_file)
        print(f"\nProcessing file: {json_file}")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Determine scale type code and name
            title = data.get("title")
            scale_info = None
            if title and title in title_mapping:
                scale_info = title_mapping[title]
            elif json_file in scale_mapping:
                 scale_info = scale_mapping[json_file]
            else:
                # Fallback: use filename without extension as code and title as name
                scale_code = os.path.splitext(json_file)[0]
                scale_name = title if title else scale_code
                scale_info = {"code": scale_code, "name": scale_name}
                print(f"  Warning: No specific mapping found for '{json_file}' or title '{title}'. Using fallback code='{scale_code}', name='{scale_name}'.")

            questionnaire_type = scale_info["code"]
            scale_display_name = scale_info["name"]

            print(f"  Identified as: Code='{questionnaire_type}', Name='{scale_display_name}'")

            # Clear old questions for this type before inserting new ones (optional but recommended)
            with sqlite3.connect(handler.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("DELETE FROM questionnaire_questions WHERE questionnaire_type = ?", (questionnaire_type,))
                conn.commit()
            print(f"  Cleared existing questions for type '{questionnaire_type}'.")


            # Insert questions
            questions_in_file = data.get("questions", [])
            if not questions_in_file:
                 print(f"  Warning: No 'questions' array found in {json_file}")
                 error_count += 1
                 continue

            for question in questions_in_file:
                try:
                    q_num = question["number"]
                    q_text = question["text"]
                    # Prepare options as JSON string, ensuring 'text' and 'score' keys exist
                    options_list = [
                        {"text": opt.get("text", "N/A"), "score": opt.get("score", 0)}
                        for opt in question.get("options", [])
                    ]
                    options_json = json.dumps(options_list, ensure_ascii=False)

                    # Use the enhanced insert_question method
                    handler.insert_question(
                        questionnaire_type,
                        q_num,
                        q_text,
                        options_json, # Pass JSON string directly
                        scale_display_name # Pass the scale name
                        )
                    imported_count += 1
                except KeyError as ke:
                     print(f"    Error processing question in {json_file}: Missing key {ke}")
                     error_count += 1
                except Exception as e_inner:
                     print(f"    Error processing question {question.get('number', 'N/A')} in {json_file}: {e_inner}")
                     error_count += 1

        except json.JSONDecodeError as jde:
            print(f"  Error decoding JSON from {json_file}: {jde}")
            error_count += 1
        except Exception as e_outer:
            print(f"  Error processing file {json_file}: {e_outer}")
            error_count += 1

    print(f"\nImport finished. Successfully imported {imported_count} questions.")
    if error_count > 0:
        print(f"Encountered {error_count} errors during import.")

if __name__ == "__main__":
    import_questions_from_json()
    # Optional: Check DB content after import
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    db_file_path = os.path.join(project_root, "psychology_analysis.db")
    try:
        from data_handler import check_db_content
        check_db_content(db_file_path)
    except ImportError:
         print("\nRun `python src/data_handler.py` to check database content.")
    except Exception as e:
        print(f"\nError checking DB content after import: {e}")
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\report_generator.py
# src/report_generator.py
from openai import OpenAI
import json
import os
import logging # Use logging

# Get the logger instance setup in app.py or utils.py
logger = logging.getLogger("PsychologyAnalysis")

class ReportGenerator:
    # --- 确保 __init__ 方法定义正确，包含 self ---
    def __init__(self, config):
        """Initializes ReportGenerator with configuration and explicit safe headers."""
        # --- 使用 self. 引用实例变量 ---
        self.config = config # Store config if needed elsewhere
        self.api_key = config.get("api_key")
        if not self.api_key:
             logger.warning("API Key not found in config for ReportGenerator.")
             self.api_key = os.environ.get('DASHSCOPE_API_KEY')
             if not self.api_key:
                  raise ValueError("API Key missing for ReportGenerator.")

        self.model = config.get("text_model", "qwen-plus")
        self.base_url = config.get("base_url", "https://dashscope.aliyuncs.com/compatible-mode/v1")

        # --- Explicitly set safe default headers ---
        safe_headers = {
            "User-Agent": "MyPsychologyApp-ReportGenerator/1.0", # Example ASCII User-Agent
            "Accept": "application/json",
        }
        # --- End of safe headers ---

        try:
             # --- 使用 self. 引用实例变量 ---
             self.client = OpenAI(
                 api_key=self.api_key,
                 base_url=self.base_url,
                 default_headers=safe_headers # <--- Add explicit safe headers
             )
             logger.info(f"ReportGenerator OpenAI client initialized. Base URL: {self.base_url}. Using explicit safe headers.")
        except Exception as e:
             logger.error(f"Failed to initialize OpenAI client in ReportGenerator: {e}", exc_info=True)
             raise

        # --- Default prompt template definition (moved inside init for clarity) ---
        self.default_prompt_template = """
请根据以下信息生成一份详细的心理分析报告，服务于警务工作场景。

**I. 被测者基础信息:**
姓名: {subject_info[name]}
性别: {subject_info[gender]}
身份证号: {subject_info[id_card]}
年龄: {subject_info[age]}
职业: {subject_info[occupation]}
案件名称: {subject_info[case_name]}
案件类型: {subject_info[case_type]}
人员身份: {subject_info[identity_type]}
人员类型: {subject_info[person_type]}
婚姻状况: {subject_info[marital_status]}
子女情况: {subject_info[children_info]}
有无犯罪前科: {criminal_record_text}
健康情况: {subject_info[health_status]}
手机号: {subject_info[phone_number]}
归属地: {subject_info[domicile]}

**II. 绘画分析 (基于AI对图片的描述):**
{description}

**III. 量表分析:**
量表类型: {questionnaire_type}
量表得分: {score}
量表答案详情 (JSON):
{questionnaire}
初步解释: {scale_interpretation}

**IV. 综合心理状态分析与建议:**
请结合以上所有信息（基础信息、绘画分析、量表结果），进行深入的心理状态评估，提取关键人格特征，并针对警务工作场景（如未成年人犯罪预防、在押人员管理、上访户调解、民辅警关怀等，根据人员类型判断侧重点）给出具体的风险评估、干预建议或沟通策略。分析需专业、客观、有条理。报告应直接开始分析内容，无需重复引言。

--- 分析报告正文 ---
"""
        # --- 使用 self. 引用实例变量 ---
        # Use template from config if provided and is a string, otherwise use the default
        config_template = config.get("REPORT_PROMPT_TEMPLATE")
        if isinstance(config_template, str) and config_template.strip():
             self.prompt_template = config_template
             logger.debug("Using prompt template from config.")
        else:
             if config_template is not None: # Log if it existed but wasn't valid
                  logger.warning("REPORT_PROMPT_TEMPLATE in config is not a valid string. Using default.")
             else:
                  logger.debug("REPORT_PROMPT_TEMPLATE not found in config. Using default.")
             self.prompt_template = self.default_prompt_template


    # 确保 generate_report 方法也正确包含 self 和所有需要的参数
    def generate_report(self, description, questionnaire, subject_info, questionnaire_type, score, scale_interpretation):
        """Generates the report by formatting the prompt and calling the LLM API."""
        logger.info(f"Generating report for subject: {subject_info.get('name', 'N/A')}")

        # --- 在这里计算 criminal_record_text ---
        criminal_record_text = '是' if subject_info.get('criminal_record', 0) == 1 else '否'
        # ------------------------------------

        # Safely format questionnaire data (which should be a dictionary passed from ai_utils)
        questionnaire_str = "N/A"
        if questionnaire and isinstance(questionnaire, dict): # Check if it's a dict
            try:
                # Dump the dictionary to a pretty JSON string for the prompt
                questionnaire_str = json.dumps(questionnaire, ensure_ascii=False, indent=2)
            except Exception as json_err:
                 logger.warning(f"Could not dump questionnaire dict to JSON: {json_err}. Using raw dict string.")
                 questionnaire_str = str(questionnaire) # Fallback
        elif isinstance(questionnaire, str): # If it's already a string (e.g., JSON string)
            questionnaire_str = questionnaire
        elif questionnaire:
            logger.warning(f"Unexpected type for questionnaire data: {type(questionnaire)}. Using raw string.")
            questionnaire_str = str(questionnaire)


        # --- 构建 prompt_context，包含所有模板需要的键 ---
        prompt_context = {
            'description': description if description else "无",
            'questionnaire': questionnaire_str, # Use formatted string
            'subject_info': subject_info if subject_info else {}, # Ensure it's a dict
            'questionnaire_type': questionnaire_type if questionnaire_type else "未知",
            'score': score if score is not None else "N/A",
            'scale_interpretation': scale_interpretation if scale_interpretation else "无",
            'criminal_record_text': criminal_record_text # <--- 添加计算出的文本
        }
        # ---------------------------------------------

        # Ensure all required keys for the template exist in subject_info context
        default_keys = ["name", "gender", "id_card", "age", "occupation", "case_name", "case_type", "identity_type", "person_type", "marital_status", "children_info", "criminal_record", "health_status", "phone_number", "domicile"]
        # Ensure subject_info itself is a dict before iterating
        if isinstance(prompt_context['subject_info'], dict):
             for key in default_keys:
                 prompt_context['subject_info'].setdefault(key, '未提供') # Set default if key missing
        else: # If subject_info is somehow not a dict, create a default one
             logger.warning(f"subject_info was not a dictionary (type: {type(prompt_context['subject_info'])}). Creating default context.")
             prompt_context['subject_info'] = {key: '未提供' for key in default_keys}


        try:
            # --- 使用 self.prompt_template 和构建好的 context 格式化 ---
            final_prompt = self.prompt_template.format(**prompt_context)
            logger.debug(f"Formatted Prompt (first 500 chars): {final_prompt[:500]}...")
        except KeyError as e:
             logger.error(f"Prompt template formatting error: Missing key {e}. Context keys available: {list(prompt_context.keys())}", exc_info=True)
             # Check if the missing key is expected in subject_info
             if str(e).strip("'") in default_keys:
                  logger.error(f"Missing key '{e}' likely expected within subject_info dictionary: {prompt_context.get('subject_info')}")
             raise KeyError(f"Prompt template formatting error: Missing key {e}") from e
        except Exception as e_fmt:
             logger.error(f"Prompt template formatting error: {e_fmt}", exc_info=True)
             raise Exception(f"Prompt template formatting error: {e_fmt}") from e_fmt


        messages = [
            # Refined system prompt
            {"role": "system", "content": "你是一位专业的心理分析师。请根据用户提供的多维度信息（基础信息、绘画描述、量表结果与解释），结合心理学知识和警务场景，生成一份结构清晰、分析深入、建议具体的综合心理评估报告。"},
            {"role": "user", "content": final_prompt}
        ]

        try:
            logger.debug(f"Calling text model '{self.model}'...")
             # --- 使用 self.client 和 self.model 调用 API ---
            completion = self.client.chat.completions.create(
                model=self.model, # 使用 self.model
                messages=messages,
            )
            report_content = completion.choices[0].message.content
            logger.info("Report content received successfully.")
            return report_content
        except Exception as e:
            logger.error(f"Error calling text generation API: {type(e).__name__} - {e}", exc_info=True)
            # Re-raise the exception so ai_utils can catch it
            raise Exception(f"调用大模型 API 时出错 - {str(e)}") from e
-----
FILE: C:\Users\ymh33\Desktop\Project\PythonProject\QingtingzheDemoProject\PsychologyAnalysis\src\utils.py
# src/utils.py
import logging
import os
import sys
from logging.handlers import RotatingFileHandler # (可选) 使用轮转日志

# --------------------------------------------------------------------------
# 重要: 这个文件现在属于 'src' 包，
# 它将被 'app/main.py' 导入。
# 'app/main.py' 已经将项目根目录添加到了 sys.path,
# 所以这里理论上可以直接访问 'app' 包，但最好避免循环导入。
# 因此，日志配置的参数（如 level, dir）应该由调用者传入。
# --------------------------------------------------------------------------

# 获取项目根目录 (PsychologyAnalysis/)
# __file__ 指向当前文件 (utils.py)
# os.path.dirname(__file__) 指向 src 目录
# os.path.dirname(os.path.dirname(__file__)) 指向 PsychologyAnalysis 目录
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

def setup_logging(log_level_str: str = "INFO", log_dir_name: str = "logs", logger_name: str = "QingtingzheApp"):
    """
    配置应用程序的日志记录。

    Args:
        log_level_str (str): 日志级别字符串 (e.g., "DEBUG", "INFO", "WARNING").
        log_dir_name (str): 相对于项目根目录的日志文件夹名称.
        logger_name (str): 要配置的日志记录器的名称.
    """
    # --- 1. 获取日志级别 ---
    log_level = getattr(logging, log_level_str.upper(), logging.INFO)
    print(f"[Logging Setup] Setting log level to: {logging.getLevelName(log_level)} ({log_level_str})")

    # --- 2. 计算日志文件路径 ---
    log_directory = os.path.join(PROJECT_ROOT, log_dir_name)
    try:
        os.makedirs(log_directory, exist_ok=True)
        print(f"[Logging Setup] Ensured log directory exists: {log_directory}")
    except OSError as e:
        print(f"[Logging Setup] Error creating log directory {log_directory}: {e}", file=sys.stderr)
        # 如果目录创建失败，可能无法写入文件日志，但控制台日志仍应工作
        log_directory = None # 标记目录不可用

    log_file_path = os.path.join(log_directory, "app.log") if log_directory else None
    print(f"[Logging Setup] Log file path set to: {log_file_path}")


    # --- 3. 获取或创建 Logger 实例 ---
    # 使用传入的 logger_name，而不是固定的 "PsychologyAnalysis"
    # 这样可以更容易地区分来自不同模块的日志（如果需要的话）
    # 但对于简单应用，使用根 logger 或一个统一的 app logger 也可以
    logger = logging.getLogger(logger_name)
    logger.setLevel(log_level) # 设置 Logger 的基础级别

    # --- 4. 清除旧的 Handlers (防止重复添加) ---
    # 如果多次调用 setup_logging，这可以防止日志重复输出
    if logger.hasHandlers():
        print("[Logging Setup] Clearing existing handlers for logger:", logger_name)
        logger.handlers.clear()

    # --- 5. 创建 Formatter ---
    log_format = "%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s"
    formatter = logging.Formatter(log_format)

    # --- 6. 创建并添加 Handlers ---

    # a) 控制台 Handler (总是添加)
    console_handler = logging.StreamHandler(sys.stdout) # 输出到标准输出
    console_handler.setLevel(log_level) # 控制台 Handler 也遵循设定的级别
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    print(f"[Logging Setup] Added Console Handler (Level: {logging.getLevelName(console_handler.level)})")

    # b) 文件 Handler (如果路径有效)
    if log_file_path:
        try:
            # 可选：使用 RotatingFileHandler 实现日志轮转
            # maxBytes=10MB, backupCount=5 (保留5个旧日志文件)
            file_handler = RotatingFileHandler(log_file_path, maxBytes=10*1024*1024, backupCount=5, encoding='utf-8')
            # 或者使用你原来的 FileHandler:
            # file_handler = logging.FileHandler(log_file_path, encoding='utf-8')

            file_handler.setLevel(log_level) # 文件 Handler 也遵循设定的级别
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
            print(f"[Logging Setup] Added Rotating File Handler (Level: {logging.getLevelName(file_handler.level)})")
        except Exception as e:
            print(f"[Logging Setup] Failed to create/add file handler for {log_file_path}: {e}", file=sys.stderr)
            logger.error(f"Failed to set up file logging to {log_file_path}: {e}")

    # --- 7. (可选) 配置特定库的日志级别 ---
    # 减少某些库的冗余输出
    logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING) # httpx 日志可能很多
    # logging.getLogger("sqlalchemy.engine").setLevel(logging.INFO) # 查看 SQL

    print(f"[Logging Setup] Configuration for logger '{logger_name}' complete.")
    # setup_logging 不再返回 logger 实例，因为它配置的是指定名称的 logger
    # 在其他模块中，通过 logging.getLogger(logger_name) 获取即可
    # 或者如果配置的是根 logger (logging.basicConfig)，则直接使用 logging.info() 等


# --- 使用方式说明 ---
# 在你的 app/main.py 中:
#
# import logging
# from app.core.config import settings
# from src.utils import setup_logging
#
# # 在创建 FastAPI app 实例之前或之后调用
# setup_logging(log_level_str=settings.LOG_LEVEL,
#               log_dir_name=os.path.basename(settings.LOGS_DIR), # 从完整路径获取目录名
#               logger_name=settings.APP_NAME) # 使用 App 名称作为 Logger 名称
#
# # 获取 logger 实例以在 main.py 中使用
# logger = logging.getLogger(settings.APP_NAME)
# logger.info("FastAPI application starting...")
#
# # 在其他模块 (e.g., app/routers/some_router.py or src/data_handler.py) 中:
# import logging
# from app.core.config import settings # 如果需要配置中的 logger 名称
#
# # 获取在 main.py 中配置好的同名 logger
# logger = logging.getLogger(settings.APP_NAME)
# # 或者，如果决定所有模块都用同一个名字:
# # logger = logging.getLogger("QingtingzheApp") # 使用 setup_logging 时传入的固定名字
#
# logger.info("This log message comes from another module.")
-----
